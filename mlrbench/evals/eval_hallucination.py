import os
import os.path as osp
import numpy as np
import json
import re
from mlrbench.utils.utils import *
from mlrbench.lmm.lmm import *


HALLUCINATION_RUBRIC = """
You are an expert AI researcher, tasked with identifying hallucinations in a research paper generated by an AI agent.
Your primary goal is to first determine IF the paper contains any hallucinations, and then to report them if they exist.
You will be given a research paper, the task description that guided its creation, and potentially the source code.

## Hallucination Types
Focus on identifying the following types of hallucinations:
- **Nonexistent Citations**: References to academic papers that cannot be found or do not exist.
- **Hallucinated Methodology**: The technical approach is reported incorrectly (e.g., claiming the method uses RL training when it actually does not).
- **Mathematical Errors**: Incorrect equations, flawed proofs, or the improper application of mathematical concepts.
- **Faked Experimental Results**: The fabrication of data, metrics, or outcomes for experiments that were never actually performed.

## Output Format
Please provide your findings in the following JSON format. Your output must be a single, complete, and valid JSON object.

First, set the `has_hallucination` flag to `true` or `false`.
- If `has_hallucination` is `true`, you MUST provide a non-empty list in the `hallucinations` field detailing each issue.
- If `has_hallucination` is `false`, the `hallucinations` list MUST be empty.

```json
{
    "has_hallucination": <true | false>,
    "hallucinations": [
        {
            "type": "<Nonexistent Citations | Hallucinated Methodology | Mathematical Errors | Faked Experimental Results>",
            "description": "<Detailed explanation of the hallucination and why it is a hallucination>",
            "evidence": "<Specific quote from the paper or snippet from the code that demonstrates the hallucination>"
        }
    ],
    "overall_assessment": "<A brief summary of your findings. State clearly whether hallucinations were found and how severe they are. If none, state that the paper appears to be free of hallucinations.>",
    "confidence": <1-5>
}
```

IMPORTANT: The `has_hallucination` field is critical. Your output must be only the JSON object.
"""


def overall_review(
    paper_path,
    client,
    task_file,
    code_path=None,
    review_form=HALLUCINATION_RUBRIC,
    max_retries=3,
):
    paper, img_list = load_multimodal_content(paper_path)
    task = read_text(task_file)
    if code_path:
        code_content = read_combine_files(code_path)
    else:
        code_content = ""
    base_prompt = review_form

    base_prompt += f"""
## Task Description

```
{task}
```

## Paper to Be Reviewed
Note: The paper is generated by AI and may contain some errors. Please check the paper carefully and provide your review.
    
```json
{paper}
```

## Code of the Paper

```json
{code_content}
```
"""

    base_prompt += """
Please now perform the hallucination detection based on the rubric, task description, paper, and code provided above.
"""
    for attempt in range(max_retries):
        try:
            response, token_usage = client.generate(prompt=base_prompt, media=img_list)
            review = extract_json_between_markers(response)
            # Validate JSON structure
            if not isinstance(review, dict):
                raise ValueError("Response is not a dictionary")
            
            required_keys = ["has_hallucination", "hallucinations", "overall_assessment", "confidence"]
            if not all(key in review for key in required_keys):
                raise ValueError(f"Missing required keys. Found: {list(review.keys())}")

            if not isinstance(review["has_hallucination"], bool):
                raise ValueError("The 'has_hallucination' field must be a boolean.")

            if not isinstance(review["hallucinations"], list):
                raise ValueError("The 'hallucinations' field must be a list.")
            
            return review, token_usage
            
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt == max_retries - 1:
                print("All retry attempts failed")
                return None
            continue
    
    return None


if __name__ == "__main__":
    evaluator_name = "claude-3-7-sonnet-20250219"
    # evaluator_name = "google/gemini-2.5-pro-preview"
    evaluator_folder = format_model_name(evaluator_name)
    reviewer = create_lmm_client(model_name=evaluator_name, max_tokens=8192*2, judge_mode=True)
    agent_name = "o4-mini"
    print(f"Reviewing {agent_name} paper using {evaluator_name}...")
    tasklist = get_tasklist(osp.join("ai_scientist_papers", agent_name))
    for task_name in tasklist:
        task_path = osp.join("ai_scientist_papers", agent_name, task_name)
        task_file = osp.join("claude_experiments", task_name, "task.md")
        # paper_path = osp.join(task_path, "results", f"paper.md")
        paper_path = osp.join(task_path, f"{task_name}.pdf")
        code_path = osp.join(task_path, "experiments")
        output_path = osp.join(task_path, f"review_hallucination_{evaluator_folder}.json")
        # if os.path.exists(output_path):
        #     print(f"Review file {output_path} already exists!")
        #     continue
        review, token_usage = overall_review(paper_path=paper_path, 
                                            client=reviewer, 
                                            task_file=task_file,
                                            code_path=code_path
                                            )
        save_json(review, output_path)