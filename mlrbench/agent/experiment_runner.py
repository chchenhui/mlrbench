import os
import os.path as osp
import logging
import subprocess
import time


claude_prompt = """
    Based on task.md, idea.md, related_work.md and proposal.md in this folder, please design an experimental plan to test the hypothesis that the proposed method is effective. Then write code to implement the experimental plan, and run the experiments in a fully automated manner.
    The experimental process should be fully automated and include the following steps:
    1. Create a folder named 'claude_code' inside this folder to save the code and results.
    2. Write some Python scripts to run the experiment automatically. IMPORTANT: The script must be thoroughly debugged and tested until it can run successfully without any errors. The script should handle all necessary data processing, model training, and evaluation steps automatically. NOTE: If the environment has available GPUs, the script should utilize GPU acceleration where possible.
       - The scripts should be able to run all baseline methods automatically.
       - The scripts should be able to save the results in a structured format (e.g., CSV or JSON) for easy analysis.
       - The scripts should generate figures to visualize the results. Figures should be generated for the main results, including but not limited to:
            - Training and validation loss curves
            - Performance metrics (e.g., accuracy, F1 score) over time
            - Comparison of the proposed method with baseline methods
            - Any other relevant figures that help to understand the performance of the proposed method
        - Make sure that the figures are properly labeled and include legends, titles, and axis labels. There should be data points in the figures, and the figures should be easy to read and interpret.
    3. Write a README.md file to explain how to run the experiment.
    4. Run the experiment automatically, visualize the results, and save the results. Make sure the generated figure files are not empty.
    5. Save the experiment execution process in log.txt.
    6. Analyze and summarize the experiment results in results.md after all experiments (including all baselines) have been successfully completed. 
        - Tables should be generated for the main results, including but not limited to:
            - Summary of the experimental setup (e.g., hyperparameters, dataset splits)
            - Comparison of the proposed method with baseline methods
            - Any other relevant tables that help to understand the performance of the proposed method
    7. Finally, create a folder named 'results' under this folder and move results.md, log.txt, and all figures generated by the experiment into 'results'.

    IMPORTANT: 
    - Do not use synthetic results or generate any fake data. The results should be based on real experiments. If the experimental dataset is too large, please use a smaller subset of the dataset for testing purposes.
    - If the experiment is about large language models (LLMs), please confirm the models used are LLMs and not simple multi-layer neural networks such as LSTM.
    - You can download the datasets from Hugging Face datasets or other open-sourced datasets. Please do not use any closed-sourced datasets.
    - If the experiment requires using open-sourced large language models (LLMs) such as Meta Llama-3.1-8B, Llama-3.2-1B, Mistral Ministral-8B or Qwen Qwen3-0.6B and multimodal LLMs like Qwen2-VL-3B, please download the models from the Hugging Face model hub. Due to limited computing resources, please use the models no larger than 8B.
    - If the experiment requires using closed-sourced large language models (LLMs) such as OpenAI's GPT-4o-mini, o4-mini or Claude-3.7-sonnet, please directly use the API provided by the model provider. API keys have already been provided in the environment variables. Please use the API key to access the model and run the experiment.
    - Only save essential files that are necessary for the experiment. Do not create or save any unnecessary files, temporary files, or intermediate files. Keep the output minimal and focused on the core experiment requirements.
    - No need to generate any paper or academic document. Focus only on the experimental implementation and results.
    - Please remove checkpoints or datasets larger than 1MB after the experiment is completed.

    Remember to analyze and summarize the experiment results, create a folder named 'results' under this folder and move results.md, log.txt, and all figures generated by the experiment into 'results'.
    - Figures and tables generated by the experiment should be included in the results.md with clear explanations of what they show. Please make sure the generated figures are not repeated. Please do not use the same figures multiple times in results.md.
    - Make sure paths to the figures in results.md are correct and point to the right locations.
    - The results.md should also include a discussion of the results, including any insights gained from the experiment and how they relate to the hypothesis.
    - The results.md should include a summary of the main findings and conclusions drawn from the experiment.
    - The results.md should also include any limitations of the experiment and suggestions for future work.
"""


def run_claude_experiment(
    task_path,
    prompt=claude_prompt,
    wait_seconds=10,  
    check_interval=0.5 
):
    claude_command = [
        "claude", "-p", prompt,
        "--output-format", "json",
        "--verbose", "--dangerously-skip-permissions"
    ]

    original_cwd = os.getcwd()
    try:
        os.chdir(task_path)
        if os.path.exists("claude_code"):
            logging.info("claude_code already exists, skipping command execution.")
            return 
        with open("claude_output.json", "w") as fout:
            result = subprocess.run(claude_command, stdout=fout, stderr=subprocess.PIPE, text=True)
        if result.returncode != 0:
            logging.error(f"Command failed with return code {result.returncode}")
            logging.error(f"STDERR:\n{result.stderr}")
            raise RuntimeError(f"Command failed with return code {result.returncode}")
        else:
            logging.info("Command executed successfully.")
            result_file = osp.join("results", "results.md")
            waited = 0
            while not os.path.exists(result_file) and waited < wait_seconds:
                time.sleep(check_interval)
                waited += check_interval
            if not os.path.exists(result_file):
                raise NotImplementedError(f"Failed to generate results.md for {task_path} after waiting {wait_seconds} seconds...")
            else:
                logging.info(f"Successfully generated results.md for {task_path} after waiting {waited:.1f} seconds.")
    finally:
        os.chdir(original_cwd)
    

def run_experiment(
    model_name,
    task_path,
):
    # if "claude" in model_name:
    #     logging.info(f"Conducting experiments using Claude Code...")
    #     run_claude_experiment(task_path=task_path)
    # else:
    #     raise NotImplementedError(f"Model {model_name} is not supported for experiment running.")
    logging.info(f"Conducting experiments using Claude Code...")
    run_claude_experiment(task_path=task_path)