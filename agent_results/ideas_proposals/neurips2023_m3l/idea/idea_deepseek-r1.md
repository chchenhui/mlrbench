**Title:** Understanding and Harnessing the Edge of Stability in Deep Learning Optimization  

**Motivation:** Modern deep learning relies on large learning rates to accelerate training, but classical convergence theory fails to explain how optimization succeeds beyond the "stable" regime (where step sizes ensure monotonic loss decrease). The empirical "Edge of Stability" (EoS) phenomenon—where loss oscillates but still decreases—challenges existing analyses. Bridging this gap is critical for designing efficient, theory-guided optimization strategies, especially for costly large-scale models.  

**Main Idea:** This work proposes a theoretical framework to characterize gradient descent (GD) dynamics at EoS by integrating stochastic gradient noise and Hessian spectral properties. We model training trajectories via *modified gradient flows* that account for non-monotonic loss behavior, using stochastic differential equations to capture noise-driven escapes from unstable regions. The analysis will derive sufficient conditions for convergence under large learning rates by linking gradient variance, Hessian eigenvalue distributions, and step size choices. Experiments on vision transformers and LLMs will validate the theory, measuring how noise magnitude and learning rate interact to stabilize EoS training. Outcomes include convergence guarantees for EoS regimes and practical guidelines for tuning optimizers to exploit instability for faster convergence. The results will advance principled, efficient training of large models and inspire noise-aware optimization algorithms.