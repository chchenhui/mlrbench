**Title:** Adaptive Conformal Prediction for Calibrated LLM Uncertainty Across Domains

**Motivation:** Large Language Models (LLMs) often provide outputs without reliable uncertainty estimates, posing risks in critical applications. Conformal Prediction (CP) offers distribution-free uncertainty guarantees but standard methods assume a fixed data distribution, which rarely holds for LLMs deployed across diverse, shifting domains or topics.

**Main Idea:** We propose developing Adaptive Conformal Prediction (ACP) methods specifically for black-box LLMs. The core idea is to dynamically adjust the conformal calibration based on recent performance or domain indicators, without needing model retraining. We will design non-conformity scores suitable for text generation (e.g., semantic dissimilarity evaluated by an auxiliary model) and develop online algorithms that update the uncertainty quantile (used for prediction sets/intervals) based on recent conformal scores or context features signaling a domain shift. This aims to maintain target coverage rates (e.g., 90% confidence) even when the underlying data distribution changes, providing more robust and trustworthy uncertainty quantification for deployed LLMs.