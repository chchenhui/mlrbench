**Title:** Dynamic Cross-Modal Watermarking for Robust Identification of AI-Generated Content  

**Motivation:**  
The rapid proliferation of multi-modal generative models (e.g., Sora, Latte) enables creation of hyper-realistic, multi-modal content (text, images, video, audio), raising risks of misinformation, deepfake abuse, and intellectual property disputes. Existing watermarking techniques are often modality-specific, fragile to transformations (e.g., compression, translation), and ineffective for composite outputs (e.g., video with edited audio). A robust, cross-modal solution is critical to ensure accountability, trace AI-generated content in real-world scenarios, and support regulatory compliance (e.g., EU AI Act).  

**Main Idea:**  
We propose a dynamic watermarking framework that embeds **multi-modal alignment signatures** into latent representations of foundation models. Unlike static watermarks, our method leverages contrastive learning to bind watermarks across modalities (e.g., linking text prompts to generated video frames), ensuring persistence even after content manipulation. The watermark is injected during generation via a differentiable, modality-agnostic module trained to resist common attacks (e.g., cropping, modality translation, partial re-editing). Detection uses a lightweight, open-source verifier that checks alignment consistency across modalities without requiring access to the original model. We will evaluate robustness against adversarial attacks, measure trade-offs in generation quality, and benchmark compatibility with diverse MFMs (e.g., Stable Diffusion, Llava). Success will enable reliable, cross-modal content provenance verification, empowering platforms to enforce safety policies and mitigate harms from malicious AI-generated material.