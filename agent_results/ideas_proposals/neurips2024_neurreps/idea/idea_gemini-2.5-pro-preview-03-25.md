**Title:** Equivariant Recurrent Models for Neural Manifold Dynamics

**Motivation:** While low-dimensional manifolds are known to capture population activity in brain regions like motor cortex, the principles governing their temporal evolution during behavior remain unclear. Understanding these dynamics is crucial for insights into neural computation and for building better brain-computer interfaces. Existing models often lack explicit geometric priors reflecting how these manifolds transform over time.

**Main Idea:** We propose developing Equivariant Recurrent Neural Networks (E-RNNs) to model the dynamics of neural representations. By analyzing neural recordings (e.g., from motor tasks), we will first identify the relevant geometric transformations (e.g., rotations, scaling) governing transitions between neural states on the manifold. We will then design E-RNN architectures that explicitly incorporate these identified symmetries. Training these E-RNNs on time-series neural data aims to predict future neural states more accurately and robustly than standard RNNs by leveraging geometric inductive biases. Success would yield models that not only predict neural activity but also provide interpretable insights into the underlying geometric principles of cortical dynamics.