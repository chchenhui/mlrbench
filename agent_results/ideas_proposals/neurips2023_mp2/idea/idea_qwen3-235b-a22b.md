1. **Title**: *Pluralistic Value Alignment in AI through Deliberative Aggregation*

2. **Motivation**:  
Current AI alignment methods often amplify monolithic values, risking societal polarization and exclusion. Many systems silently entrench dominant cultural or corporate priorities, neglecting ethical pluralism. This research addresses how AI can equitably embody *competing* human values, such as autonomy vs. collective welfare or justice vs. mercy, resolving the workshop’s concern about “missing pluralistic values” and amplifying diverse voices.

3. **Main Idea**:  
We propose a “deliberative aggregation” framework that synthesizes divergent moral preferences through structured dialogue-inspired computing. First, we would map value conflicts using moral philosophy taxonomies (e.g., deontological vs. consequentialist frameworks). Then, in a novel application of multi-agent reinforcement learning (MARL), multiple “value agents” representing stakeholder perspectives (e.g., disabled communities, labor groups, ethicists) would negotiate optimal compromises within AI decision-making policies. This approach replaces RLHF’s single aggregate reward with a dynamic equilibrium between contesting priorities. Initial experiments would focus on autonomous vehicles (balancing speed vs. safety) and public health AI (privacy vs. surveillance trade-offs). Expected outcomes include transparency in value trade-offs, reduced bias amplification, and a novel benchmark for pluralistic alignment. By simulating ethical deliberation, this work could redefine how AI systems mediate value conflicts rather than encode static rules, directly addressing workshop themes around cross-disciplinary ethical integration.