**Title:** Physics-Informed Concept Bottleneck Models for Interpretable PDE Solving

**Motivation:** Deep learning models like PINNs or Fourier Neural Operators accelerate solving PDEs but often act as black boxes. This lack of transparency hinders trust and makes it difficult to verify if the model learns correct physical principles or relies on spurious correlations, limiting their reliable use in scientific discovery.

**Main Idea:** We propose integrating Concept Bottleneck Models (CBMs) into neural PDE solvers. The model will first predict intermediate, human-understandable physical concepts (e.g., Reynolds number range, flow regime, presence of specific structures like vortices or shock waves) relevant to the PDE directly from the input conditions or low-fidelity data. The final PDE solution prediction will then be conditioned *only* on these predicted concepts. This forces the model to reason via physically meaningful intermediate variables, making the prediction process inherently interpretable. Explainability techniques can then directly probe the concept layer. Expected outcomes include more trustworthy PDE solvers where a scientist can verify the model's physical reasoning and potentially gain insights by analyzing the learned concept representations.