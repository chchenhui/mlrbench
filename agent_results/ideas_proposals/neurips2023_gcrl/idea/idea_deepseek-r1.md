**Title:** Disentangled Representation Learning through Mutual Information Constraints in Goal-Conditioned RL  

**Motivation:** Goal-conditioned RL (GCRL) relies heavily on learned representations to generalize across diverse goals. However, existing methods often fail to explicitly disentangle environment dynamics, goal associations, and agent-specific factors, leading to suboptimal sample efficiency and limited transfer. This work addresses the gap between GCRL and representation learning by promoting structured, interpretable latent spaces that enhance adaptability in complex domains like robotics and molecular design.  

**Main Idea:** We propose a GCRL framework that integrates mutual information constraints to disentangle latent factors. The agent learns a state encoder decomposed into independent modules, each capturing distinct factors (e.g., object positions, environmental context). A variational objective maximizes reward while minimizing mutual information between modules, enforcing disentanglement. For training, goals are sampled from perturbed or abstracted subsets of factors (e.g., varying molecular substructures while retaining core properties). Benchmarks will include environments with known ground-truth factors (e.g., simulated robot manipulation) and molecular generation tasks. Expected outcomes include improved sample efficiency in reaching unseen goals and better zero-shot transfer across domains. Successful disentanglement could enable applications where interpretable control is critical, such as drug discovery or precision robotics.