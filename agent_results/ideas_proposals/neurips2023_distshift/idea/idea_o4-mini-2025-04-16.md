Title: Domain-Aware Adapter Tuning for Robust Foundation Model Adaptation

Motivation: Fine-tuning large pretrained models on specialized datasets often improves in-domain performance but erodes their inherent robustness to distribution shifts. This trade-off poses challenges in high-stakes applications—e.g., medical imaging across hospitals—where out-of-distribution (OOD) reliability is critical.

Main Idea: We propose inserting lightweight “domain-aware” adapter modules into a frozen foundation model. Each adapter is parameterized by a learnable domain embedding that interpolates between (1) the original pretraining data manifold and (2) the target downstream domain. During adaptation, we jointly optimize adapter weights and a gating network that dynamically balances these manifold contributions based on input features. To preserve robustness, we add a contrastive loss that encourages the model to distinguish between pretraining-like and downstream-like samples, thus maintaining sensitivity to OOD variations. At inference, the gating network predicts domain weights per example, automatically adjusting the balance to mitigate distribution shifts. We will validate on WILDS benchmarks and specialized biomedical datasets, aiming to show that our method retains OOD accuracy while delivering strong in-domain gains.