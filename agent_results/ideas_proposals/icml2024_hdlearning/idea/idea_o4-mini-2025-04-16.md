Title: Spectral Dynamics as a Lens for Implicit Regularization in Wide Deep Networks

Motivation:  
Modern deep networks exhibit implicit regularization through optimizer-induced spectral compression, yet we lack a predictive, analyzable framework. Bridging random matrix theory with high-dimensional learning dynamics can clarify how architecture and optimization jointly shape generalization, reducing reliance on costly empirical search.

Main Idea:  
We introduce a dynamical-systems model tracking the evolving eigenvalue distribution of hidden‚Äêlayer activations and weight Hessians under SGD and its variants. Using tools from free probability and Stieltjes transforms, we derive deterministic differential equations approximating spectral flow in the infinite-width limit. We will validate this framework by: (1) comparing predicted spectral trajectories against empirical measurements on ResNet and Transformer blocks; (2) correlating spectral concentration metrics with out-of-sample accuracy; and (3) extending the model to adaptive optimizers (Adam, RMSProp) to quantify their unique regularization signatures. Expected outcomes include closed-form scaling laws linking learning rate, batch size, and depth to spectral bias, and a practical diagnostic for hyperparameter tuning. This work can inform architecture design and optimizer choice by offering principled, high-dimensional insights into implicit bias and generalization.