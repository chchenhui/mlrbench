Title: RL-PoS: A Prior-Optimality Spectrum Benchmark for Reincarnating RL

Motivation:  
Despite growing interest in reusing past computation to accelerate reinforcement learning, the field lacks a standardized evaluation framework that captures varying qualities and types of priors (policies, models, datasets). Without such benchmarks, it is hard to compare methods fairly or understand their robustness to suboptimal prior work.

Main Idea:  
We introduce RL-PoS, a modular benchmark suite that (1) generates priors at graded performance levels by training policies, dynamics models, and offline datasets under controlled budgets; (2) defines three core tasks (continuous control, navigation, manipulation) in MuJoCo and MiniGrid where priors span a spectrum from poor to near-optimal; and (3) provides a Python API for plugging in reincarnation algorithms and measuring key metrics: sample‚Äêefficiency speed-up, transfer regret, and robustness under prior misalignment. Baseline implementations of popular fine-tuning, model-based warm-starting, and meta-RL approaches will be included. By systematically varying prior quality and type, RL-PoS enables the community to benchmark new reincarnation methods, diagnose failure modes when priors are suboptimal, and drive best practices for democratizing large-scale RL research.