**Title:** Co-Designed Analog Stochastic Sampling for Energy-Based Models  

**Motivation:** Energy-based models (EBMs) offer flexible representations for generative tasks and uncertainty estimation but suffer from computationally expensive Markov Chain Monte Carlo (MCMC) sampling during training and inference. Emerging analog hardware provides a promising avenue to accelerate sampling via inherently stochastic physical processes, yet its noise and device variability remain underexploited in ML co-design.  

**Main Idea:** This research proposes to co-design EBMs with analog stochastic hardware (e.g., resistor networks, photonic circuits) where device-level noise is leveraged as a resource for efficient sampling. The methodology includes: (1) characterizing analog hardware noise distributions and mapping them to EBM sampling dynamics, (2) developing training algorithms that optimize EBMs to align with hardware-imposed constraints (e.g., limited bit-depth, spatial connectivity), and (3) hybrid digital-analog training frameworks to stabilize learning. The outcome would enable orders-of-magnitude faster, energy-efficient sampling compared to digital MCMC, making large-scale EBMs practical. Impact includes sustainable generative AI systems and broader adoption of EBMs in low-power settings, while also guiding hardware designers toward ML-relevant noise and connectivity specifications.