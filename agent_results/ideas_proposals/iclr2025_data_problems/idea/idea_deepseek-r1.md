**Title:** Dynamical Stability in Synthetic Data Training: Preventing Model Collapse through Adaptive Diversity Preservation  

**Motivation:** As synthetic data becomes integral to training foundation models (FMs), model collapse—progressive loss of task performance and diversity in generations—poses a critical threat. Traditional approaches often fail to address the dynamic interplay between synthetic data quality and training stability, risking irreversible degradation in multi-generational FM training.  

**Main Idea:** This research proposes a framework combining dynamical systems theory and adaptive data curation to prevent collapse. First, we model training as a dynamical process, identifying key thresholds where synthetic data diversity loss triggers collapse (via spectral analysis of gradient descent dynamics). Second, we integrate reinforcement learning (RL) to adaptively adjust the synthetic-real data blending ratio during training, guided by real-time diversity metrics (e.g., entropy of feature embeddings). The method includes a "collision detection" module that flags incipient collapse and triggers rebalancing. Experiments on text and multimodal FMs will benchmark collapse mitigation, while theoretical work links data diversity to convergence guarantees. Expected outcomes: (1) principled guidelines for synthetic data usage ratios, (2) open-source tools for collapse monitoring, and (3) scalable training protocols enabling sustainable synthetic data adoption without performance decay. Impact: Enables safer reliance on synthetic data in resource-constrained domains like healthcare or low-resource languages.