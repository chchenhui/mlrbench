**Title:** Co-Steer: Real-Time Bidirectional Alignment via Interactive Value Negotiation

**Motivation:** Static, pre-deployment AI alignment fails to capture the dynamic nature of human needs and evolving contexts. Effective bidirectional alignment requires mechanisms for continuous adjustment during interaction. Current systems often lack interfaces for users to transparently negotiate and steer AI behavior based on shifting goals or conflicting values (e.g., helpfulness vs. harmlessness), limiting both AI adaptability (AI aligned to Human) and user agency (Human aligned to AI understanding/capabilities).

**Main Idea:** We propose "Co-Steer," an interactive system enabling real-time alignment negotiation between humans and AI. The system features a novel user interface allowing multi-dimensional feedback beyond simple preferences, such as adjusting sliders representing competing values (e.g., creativity vs. factuality), providing targeted natural language critiques, or selecting/editing specific parts of AI responses. This continuous, nuanced feedback dynamically updates the AI's internal alignment model or policy, potentially using online RLHF variants or contextual preference learning. We will evaluate Co-Steer via user studies measuring task performance, perceived alignment, user control, and cognitive load compared to static alignment baselines. Expected outcomes include a validated interactive paradigm for dynamic alignment, improved user trust and collaboration, and more contextually appropriate AI behavior.