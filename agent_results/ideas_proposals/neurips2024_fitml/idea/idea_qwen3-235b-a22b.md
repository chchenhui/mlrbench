**Title:** Dynamic Sparse Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models  

**Motivation:**  
Fine-tuning large language models (LLMs) is computationally expensive, limiting their deployment on resource-constrained devices. Existing methods like low-rank adaptation (LoRA) reduce parameters but lack flexibility in adapting to task-specific sparsity patterns. This work addresses the inefficiency of fixed-rank and dense fine-tuning by proposing a dynamic framework that combines low-rank and sparse representations, enabling adaptive parameter allocation during training.  

**Main Idea:**  
We propose **DynaLoRA**, a hybrid fine-tuning method that dynamically adjusts both the rank and sparsity of adapter layers in LLMs. Instead of fixing the rank *a priori*, DynaLoRA uses a differentiable sparsity controller to identify and prune redundant low-rank parameters during training, guided by task-specific gradients. This integrates the efficiency of low-rank matrices with the adaptability of sparse structures. Theoretically, we analyze the optimization landscape under dynamic sparsity constraints and derive generalization bounds for transfer learning. Empirically, we evaluate DynaLoRA on NLP tasks (e.g., GLUE benchmarks) and few-shot learning scenarios, comparing it to LoRA, dense fine-tuning, and structured pruning baselines. We expect DynaLoRA to reduce trainable parameters by 40â€“60% while maintaining >95% of full-model accuracy, with latency improvements for edge deployment. This could advance efficient fine-tuning theory and enable practical LLM adaptation for low-resource settings.