**Title:** Generative Control Policies via Conditional Diffusion Models

**Motivation:** Designing controllers for complex dynamical systems, especially under uncertainty or with high-dimensional state/action spaces, remains challenging. Traditional methods often struggle with exploration and generalization, while standard reinforcement learning can be sample inefficient or yield deterministic policies lacking robustness.

**Main Idea:** We propose formulating the control policy as a conditional diffusion model. Given a state (and potentially a goal), the policy model learns to reverse a diffusion process that gradually transforms noise into optimal actions. This generative approach allows sampling diverse, high-quality actions, facilitating better exploration and inherent stochasticity for robustness. Training leverages insights from both diffusion models and optimal control/RL, potentially using dynamic programming principles or policy gradients adapted for diffusion processes. Expected outcomes include more sample-efficient learning of robust, stochastic control policies capable of handling complex, high-dimensional systems compared to standard RL algorithms.