Title: Adaptive Mini-Batch Fairness Estimation under Compute Constraints

Motivation: Real‐world ML systems, especially on‐edge or low‐power devices, often cannot afford exhaustive group‐fairness evaluations due to limited memory and computation. Without accurate fairness monitoring, models may perpetuate bias, harming sensitive subpopulations. New methods are needed to certify and correct fairness guarantees with minimal overhead.

Main Idea: We propose a two‐phase adaptive subsampling framework that maintains tight fairness bounds by (1) stratified mini‐batch sampling focusing on protected and underrepresented groups, and (2) online variance reduction via control variates. At each training iteration, the algorithm estimates group‐wise demographic parity and equality‐of‐opportunity metrics with statistically guaranteed error bounds, dynamically adjusting batch sizes to respect a user‐specified compute budget. A theoretical analysis derives sample‐complexity bounds for target fairness tolerance ε and confidence δ, revealing a trade‐off curve between compute cost and certification precision. Empirical evaluation on tabular and vision fairness benchmarks demonstrates up to 5× reduction in evaluation time for comparable fairness guarantees, enabling trustworthy deployment in resource‐constrained environments.