Title: Windowed Forward-Forward Streaming Learning for Edge Video Analytics

Motivation:  
Real-time video analytics on resource-constrained edge devices is bottlenecked by global backpropagation’s high latency, memory footprint, and synchronization overhead. A local, forward-only learning scheme can enable continual adaptation to nonstationary video streams without centralized coordination or expensive backward passes.

Main Idea:  
We propose a sliding-window Forward-Forward (FF) framework that breaks incoming video streams into short temporal clips. For each clip, positive samples are patches exhibiting coherent motion and negative samples are temporally shuffled or spatially distorted patches. Each convolutional layer learns independently by maximizing the difference between its FF goodness scores on positive versus negative patches. Layers process windows asynchronously, updating local weights whenever their goodness margin falls below a threshold, thereby adapting to scene changes in real time. Minimal inter-layer signalling (just goodness summaries) ensures gradual consistency without full backprop. We will implement this on a commodity edge GPU and evaluate on UAV surveillance and mobile AR benchmarks, measuring latency, model drift, and energy. Expected outcomes include sub-10 ms updates, 50% memory savings, and robust adaptation to lighting or viewpoint shifts—paving the way for scalable, biologically inspired streaming learning at the edge.