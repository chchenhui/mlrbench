Title: TouchBERT â€“ Self-Supervised Transformer for Spatio-Temporal Tactile Representation Learning

Motivation: Modern high-resolution tactile sensors generate rich spatio-temporal data, yet labeled datasets remain scarce. Without scalable representation learning, robots struggle to generalize across objects, materials, and manipulation tasks. A dedicated self-supervised backbone for touch can unlock data efficiency and robustness in downstream applications like object recognition, slip detection, and adaptive grasping.

Main Idea: We propose TouchBERT, a transformer-based architecture pre-trained on large-scale unlabeled tactile sequences using a masked patch prediction objective. Each input sequence is tokenized into spatio-temporal patches of tactile pressure maps (and optional proprioceptive cues). Randomly masked tokens are reconstructed by the model, forcing it to capture both local contact geometry and global temporal dynamics. After pre-training on 500K+ contact trials across varied objects and motions, TouchBERT is fine-tuned on small labeled sets for tasks such as material classification, slip onset detection, and force estimation. We expect significant gains in sample efficiency, cross-domain generalization, and seamless multimodal fusion with vision. By open-sourcing the pre-trained weights and code, TouchBERT lowers the barrier to entry and accelerates the development of touch-driven robotic behaviors in unstructured environments.