**Title:** Spatiotemporal Contrastive Learning for Touch-Vision Representation Alignment  

**Motivation:** Touch sensing in robotics suffers from sparse, temporally dynamic data and a scarcity of labeled datasets. Integrating touch with vision can overcome these limitations, enabling robots to associate tactile interactions with visual context—critical for tasks like manipulation in unstructured environments or teleoperation.  

**Main Idea:** Develop a self-supervised framework leveraging contrastive learning to align tactile sequences (from high-resolution sensors) with synchronized visual data. The model uses a spatiotemporal encoder for touch, combining 3D convolutions to capture local texture changes and transformers to model long-range temporal dependencies. Vision inputs are processed via standard CNNs. Contrastive loss encourages alignment between touch embeddings and corresponding video clips of the interaction. For instance, a robot gripping an object would align tactile frames with video of the grip, while contrasting mismatched pairs. This approach bypasses tactile data labeling, exploits unlabeled real-world interactions, and bridges touch’s local 3D→2D sensing with global visual context. Expected outcomes include improved cross-modal retrieval accuracy and generalization in tactile-only tasks (e.g., material recognition). Impact: Enables scalable touch representation learning, accelerating deployment in agriculture, prosthetics, and AR/VR with minimal supervision.