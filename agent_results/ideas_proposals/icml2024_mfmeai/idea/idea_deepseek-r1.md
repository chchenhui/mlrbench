**Title:** Bridging the Semantic-Action Gap: Hierarchical Multimodal Foundation Models for Low-Level Control in Embodied AI  

**Motivation:** While multi-modal foundation models (MFMs) excel at high-level reasoning (e.g., task planning, scene understanding), their application to low-level control (e.g., precise motor actions, real-time adjustments) remains underexplored. This gap limits embodied agents’ ability to execute nuanced physical interactions in dynamic environments, such as manipulating fragile objects or navigating cluttered spaces.  

**Main Idea:** We propose a hierarchical MFM framework that synergizes high-level semantic understanding with low-level control. The system comprises two modules: (1) a *semantic planner* (MFM-based) that converts multimodal inputs (e.g., vision, language) into intermediate, interpretable representations (e.g., object affordances, spatial constraints), and (2) a *low-level controller* trained via reinforcement learning to map these representations to precise motor commands (e.g., force-sensitive grasping). To bridge the hierarchy, we introduce a cross-modal alignment loss that encourages the planner’s outputs to be actionable by the controller, using paired trajectory data. We will validate the approach in simulation benchmarks (e.g., MetaWorld, Habitat) requiring both contextual reasoning (e.g., "pour water into the cracked cup") and millimeter-scale precision. Expected outcomes include a unified training protocol for MFM-based control and a simulation-to-real transfer strategy, enabling robots to perform complex tasks where semantic intent directly informs low-level execution. This could democratize deployment of MFM-powered agents in real-world settings like healthcare or logistics.