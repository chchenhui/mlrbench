Title: Peer-to-Peer Mixture-of-Experts for Continual Decentralized Learning

Motivation:  
Current monolithic deep models are costly to retrain, prone to catastrophic forgetting, and seldom reused once deprecated. In decentralized or federated settings, sharing a single giant model exacerbates communication overhead and privacy risks. A modular, collaborative framework that allows each participant to contribute and leverage specialized experts can reduce training cost, improve adaptability, and foster continual learning without central coordination.

Main Idea:  
We propose a peer-to-peer Mixture-of-Experts (P2P-MoE) system in which each client trains or upcycles a small expert module on its local data (or via parameter-efficient fine-tuning of a pre-trained model). A lightweight, shared router network is maintained via gossip aggregation: clients exchange only sparse routing statistics and expert signatures, not full weights. At inference, the router dynamically selects a sparse subset of experts across peers to form a composite prediction. Periodic expert merging clusters and merges redundant experts using representation similarity, pruning redundancies and bounding communication. This approach enables continuous specialization, privacy-preserving collaboration, and efficient model reuse, while mitigating forgetting and lowering resource demands for large-scale decentralized deployments.