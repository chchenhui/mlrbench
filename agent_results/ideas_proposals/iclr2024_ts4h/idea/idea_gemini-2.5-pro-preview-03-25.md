**Title:** Continuous-Time Transformer Embeddings for Irregularly Sampled Health Data

**Motivation:** Standard sequence models like Transformers struggle with irregularly sampled time series common in healthcare (e.g., EHR visits, sporadic wearable readings), as they implicitly assume fixed time intervals. This hinders the application of powerful foundation models. Bridging this gap requires architectures that naturally handle arbitrary time gaps.

**Main Idea:** We propose adapting Transformer architectures by replacing standard positional embeddings with learned continuous-time embeddings. These embeddings will be generated by a neural network (e.g., MLP) that explicitly takes the continuous time difference (Î”t) between observations as input, alongside the observation index. This allows the self-attention mechanism to weigh information based on true temporal proximity, rather than just sequence order. The model could be pre-trained on large-scale, unlabeled health time series and fine-tuned for tasks like disease forecasting or patient subtyping. This approach directly addresses the irregular measurement challenge, potentially improving model robustness and performance on real-world clinical data compared to interpolation or fixed-interval methods.