Title: Adaptive Meta-Probing for Cross-Domain Representational Alignment

Motivation:  
Existing alignment metrics often rely on fixed, hand-designed probes that fail to capture the diverse transformations and non-linearities present when comparing representations from different systems or modalities. A dynamic, task-adaptive probing framework would yield more robust and generalizable measures of alignment, illuminating true shared structure and guiding targeted interventions.

Main Idea:  
We propose a bi-level optimization framework in which a meta-probe network learns, for two given representation sets (e.g., a convolutional vision model and a primate V4 recording), the optimal pair of probe functions f and g that maximize a chosen similarity objective (e.g., mutual information or canonical correlation). The inner loop adapts f, g via self-supervised or weakly supervised signals; the outer loop regularizes the probe complexity to avoid overfitting. By training across multiple tasks and domains, the meta-probe generalizes to unseen representational pairs, yielding an adaptive alignment metric. We will evaluate this method on vision–neuroscience, language–behavior, and multi-modal benchmarks. Expected outcomes include (1) a unified, data-driven alignment score, (2) insights into cross-system shared computations, and (3) a tool for systematically increasing or decreasing alignment through targeted regularization.