**Title:** Structured Knowledge Integration in Probabilistic Generative Models via Relational Constraints  

**Motivation:** Probabilistic generative models often struggle to exploit domain-specific structural priors (e.g., relational dependencies in molecules or social networks), leading to suboptimal accuracy and generalization. Enabling explicit encoding of such knowledge is critical for improving sample quality and inference efficiency in structured data applications like drug discovery or recommender systems.  

**Main Idea:** Propose a framework that integrates relational constraints (e.g., sparsity, symmetry, or hierarchy) into probabilistic generative models via graph-structured latent variables. The model combines variational autoencoders (VAEs) with graph neural networks (GNNs), where domain knowledge is encoded as differentiable constraints on latent interactions. For instance, in molecular generation, atomic bonding rules are enforced via GNN message-passing layers, while a constrained optimization objective ensures adherence to stoichiometry. The approach leverages Hamiltonian Monte Carlo for efficient sampling under constraints and quantifies uncertainty in structured outputs. Expected outcomes include improved sample validity and inference speed on graph/time-series datasets, with applications in scientific domains requiring rigorous adherence to domain rules.