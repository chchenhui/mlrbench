# Curriculum-Enhanced Generative RL: A Pathway to Perpetual Agent Learning

## Motivation
Traditional reinforcement learning systems plateau once mastering predefined tasks, failing to capture the perpetual adaptation capabilities of natural intelligence. As large generative models increasingly interact with real-world environments, we need learning frameworks that continuously evolve rather than stagnate. Current approaches lack mechanisms to systematically challenge themselves with novel problems of appropriate difficulty, limiting their ability to develop increasingly general capabilities and robust out-of-distribution performance.

## Main Idea
This research proposes a self-generating curriculum framework where a generative model serves dual roles as both problem generator and problem solver. The system contains three key components: (1) a difficulty estimator that evaluates the complexity of generated tasks relative to the agent's current capabilities; (2) a diversity promoter that ensures generated tasks explore novel skill combinations; and (3) a curriculum scheduler that selects tasks at the "frontier of learnability" - challenging enough to drive learning but achievable with agent growth. Unlike static curricula, our approach continuously evolves the problem space based on agent performance, generating increasingly complex tasks that require compositional reasoning and skill transfer. By maintaining a dynamic archive of past tasks and solutions, the system can revisit and recombine concepts, creating a never-ending stream of novel challenges that drive perpetual capability expansion while avoiding catastrophic forgetting of foundational skills.