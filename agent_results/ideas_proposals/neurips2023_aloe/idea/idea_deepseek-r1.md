**Title**: Emergent Complexity Metrics for Real-Time Assessment of Open-Ended Learning in Generative Agents  

**Motivation**: Current measures of open-endedness (e.g., task diversity or reward baselines) fail to quantify how learning systems *continuously* generate novel, impactful capabilities. Without principled metrics aligned with emergent behaviors, we cannot diagnose stagnation or guide interventions in OEL systems like large language models (LLMs) interacting with dynamic environments.  

**Main Idea**: Propose metrics combining *computational mechanics* (to quantify state complexity and causal architecture) and *quality-diversity algorithms* (to measure solution space coverage). Use unsupervised methods (e.g., contrastive learning) to embed agent trajectories or LLM outputs into a latent space, where local entropy and divergence across time steps capture emergent complexity. These metrics would auto-discover phase transitions in capability growth and flag plateaus. Validate in simulated OEL benchmarks (e.g., evolving game-playing agents) and real-world generative agents (e.g., chatbots generating training data). Success would enable adaptive curricula and environment redesign to sustain open-ended learning, accelerating generalizable AI while mitigating unguided exploration risks.