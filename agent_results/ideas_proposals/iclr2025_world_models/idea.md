**Title:** Causality-Aware World Models via Counterfactual Latent State Prediction

**Motivation:** Existing world models excel at predicting future observations based on correlations but often lack genuine causal understanding. This limits their ability to generalize to novel situations or anticipate the results of specific interventions, crucial for robust decision-making in complex domains like robotics or healthcare.

**Main Idea:** We propose training world models not only to predict future latent states autoregressively but also to predict counterfactual latent states resulting from hypothetical interventions. During training, alongside standard sequence prediction, the model receives perturbed actions or states (representing interventions) and must predict the resulting deviation from the original predicted trajectory. This encourages the latent space to implicitly encode causal relationships. We will leverage architectures combining Transformers/SSMs for temporal dynamics with mechanisms (e.g., attention modulation, graphical structures) sensitive to intervention signals. Evaluation will focus on zero-shot generalization to unseen interventions in simulated environments and analyzing the learned latent space for explicit causal structure representations. This aims for more robust, generalizable, and interpretable world models.