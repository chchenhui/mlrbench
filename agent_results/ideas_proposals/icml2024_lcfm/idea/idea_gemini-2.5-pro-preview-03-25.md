**Title:** Attention-Guided Dynamic KV Cache Compression for Efficient Long-Context Inference

**Motivation:** Autoregressive Long-Context Foundation Models (LCFMs) suffer from excessive memory consumption due to the large Key-Value (KV) cache required during inference. This limits sequence length and hinders deployment on resource-constrained hardware. Uniform cache compression techniques risk discarding important long-range information critical for LCFM performance.

**Main Idea:** We propose a dynamic KV cache compression method where compression strength (e.g., quantization bits, eviction rate) is adaptively determined based on historical attention patterns. Tokens (or blocks of tokens) in the KV cache that consistently receive low attention scores from subsequent query tokens are compressed more aggressively (e.g., heavily quantized or pruned). Conversely, frequently attended tokens retain higher fidelity. This approach prioritizes memory allocation for contextually salient information, aiming to significantly reduce the KV cache footprint with minimal degradation in perplexity and performance on tasks requiring long-range dependency understanding, thereby improving the practical efficiency of LCFMs.