# Title: Graph Neural Surrogate-Driven GFlowNets for Black-Box Discrete Sampling

## Introduction

Sampling and optimization in discrete space are fundamental problems with wide-ranging applications across various domains, including physics, combinatorial optimization, compiler optimization, and modern machine learning models. Despite their importance, these problems are notoriously difficult, particularly when dealing with black-box objectives that have long-range and high-order correlations, such as language model posteriors or protein design. Existing methods often struggle with the lack of gradient information and the computational expense of function evaluations, leading to slow and inefficient sampling/optimization processes.

This research proposal presents an innovative approach to accelerate black-box discrete sampling and optimization. The proposed method couples a trainable graph neural network (GNN) surrogate with a GFlowNet sampler. The GNN is initialized on a small seed set of true objective evaluations to learn an approximate energy landscape and supply pseudo-gradients. The GFlowNet then generates diverse proposals guided by the surrogate’s learned structure. Periodically, selected proposals are evaluated on the true objective to fine-tune the GNN via active learning and recalibrate GFlowNet rewards. By alternating surrogate updates and flow sampling with importance-weighted corrections, the method drastically reduces true-objective queries while efficiently exploring high-order discrete spaces.

This proposal aims to address the challenges of black-box discrete sampling/optimization by leveraging the strengths of both GNNs and GFlowNets. The proposed framework offers significant potential for applications in language modeling, protein engineering, and combinatorial design, where improved sampling/optimization methods can lead to substantial advancements.

## Methodology

### Research Design

The proposed methodology comprises an iterative framework that alternates between updating the GNN surrogate and sampling with the GFlowNet. The framework can be outlined as follows:

1. **Initialization**:
   - Initialize a small seed set of true objective evaluations.
   - Train a GNN surrogate on this seed set to approximate the energy landscape of the objective function.

2. **Sampling with GFlowNet**:
   - Use the GFlowNet sampler to generate diverse proposals guided by the surrogate’s learned structure.
   - Evaluate the proposals on the true objective function to obtain rewards.

3. **Active Learning and Surrogate Refinement**:
   - Select a subset of proposals with high uncertainty (e.g., based on the variance of the GNN surrogate predictions) and evaluate them on the true objective.
   - Use the true objective evaluations to fine-tune the GNN surrogate via active learning.
   - Recalibrate the GFlowNet rewards based on the new true objective evaluations.

4. **Iteration**:
   - Repeat steps 2 and 3 until convergence or a predefined number of iterations.

### Detailed Steps

#### Step 1: Initialization

1. **Seed Set Generation**:
   - Generate a small seed set of true objective evaluations, \( \mathcal{S} \), by evaluating a predefined set of samples on the true objective function \( f \).
   - Let \( \mathcal{S} = \{ s_i \}_{i=1}^{N_s} \), where \( s_i \) are the samples and \( N_s \) is the size of the seed set.

2. **GNN Surrogate Training**:
   - Train a GNN surrogate, \( G \), on the seed set \( \mathcal{S} \) to approximate the energy landscape of the objective function \( f \).
   - Use a loss function that measures the discrepancy between the true objective evaluations and the surrogate predictions:
     \[
     \mathcal{L}_{\text{GNN}} = \sum_{i=1}^{N_s} \left( f(s_i) - G(s_i) \right)^2
     \]

#### Step 2: Sampling with GFlowNet

1. **Proposal Generation**:
   - Use the GFlowNet sampler to generate diverse proposals guided by the surrogate’s learned structure.
   - Let \( \mathcal{P} \) be the set of proposals generated by the GFlowNet.

2. **Reward Evaluation**:
   - Evaluate the proposals in \( \mathcal{P} \) on the true objective function \( f \) to obtain rewards.
   - Let \( r_i = f(p_i) \) be the reward for proposal \( p_i \).

#### Step 3: Active Learning and Surrogate Refinement

1. **Uncertainty-Based Sampling**:
   - Select a subset of proposals with high uncertainty based on the variance of the GNN surrogate predictions.
   - Let \( \mathcal{U} \) be the set of uncertain proposals, where uncertainty is quantified by the variance of the surrogate predictions:
     \[
     \text{Uncertainty}(p_i) = \text{Var}(G(p_i))
     \]
   - Select proposals \( \mathcal{U} \) such that:
     \[
     \mathcal{U} = \arg\max_{p_i \in \mathcal{P}} \text{Uncertainty}(p_i)
     \]

2. **Fine-Tuning GNN Surrogate**:
   - Use the true objective evaluations of the selected uncertain proposals to fine-tune the GNN surrogate.
   - Update the GNN parameters \( \theta \) using the following loss function:
     \[
     \mathcal{L}_{\text{update}} = \sum_{p_i \in \mathcal{U}} \left( f(p_i) - G(p_i) \right)^2
     \]
   - Optimize \( \theta \) using a gradient-based optimization method, such as stochastic gradient descent (SGD).

3. **Recalibrate GFlowNet Rewards**:
   - Recalibrate the GFlowNet rewards based on the new true objective evaluations.
   - Update the GFlowNet parameters \( \phi \) using the following loss function:
     \[
     \mathcal{L}_{\text{recalibrate}} = \sum_{p_i \in \mathcal{U}} \left( r_i - R(p_i) \right)^2
     \]
   - Optimize \( \phi \) using a gradient-based optimization method, such as SGD.

#### Step 4: Iteration

- Repeat steps 2 and 3 until convergence or a predefined number of iterations.

### Evaluation Metrics

To evaluate the performance of the proposed method, the following metrics will be used:

1. **Sampling Efficiency**:
   - Measure the number of true objective evaluations required to achieve a certain level of performance.
   - Lower values indicate more efficient sampling.

2. **Solution Quality**:
   - Evaluate the quality of the sampled solutions by comparing them to known optimal solutions or by measuring the objective function value.
   - Higher values indicate better solution quality.

3. **Computational Efficiency**:
   - Measure the computational time required to generate a set of proposals and evaluate them on the true objective.
   - Lower values indicate more efficient computation.

4. **Convergence**:
   - Monitor the convergence of the GNN surrogate and GFlowNet parameters over iterations.
   - Faster convergence indicates better performance.

## Expected Outcomes & Impact

The proposed research aims to develop a novel framework for black-box discrete sampling and optimization that leverages the strengths of GNNs and GFlowNets. The expected outcomes of this research include:

1. **Improved Sampling Efficiency**:
   - The proposed method aims to significantly reduce the number of true objective evaluations required to achieve high-quality solutions.
   - This improvement will be demonstrated through experiments on benchmark datasets and real-world applications.

2. **Enhanced Solution Quality**:
   - The proposed method aims to generate solutions with higher quality by efficiently exploring high-order discrete spaces.
   - This improvement will be demonstrated through experiments on benchmark datasets and real-world applications.

3. **Computational Efficiency**:
   - The proposed method aims to reduce the computational overhead of sampling and optimization in discrete spaces.
   - This improvement will be demonstrated through experiments on benchmark datasets and real-world applications.

4. **Broad Applicability**:
   - The proposed method is designed to be broadly applicable to various domains, including language modeling, protein engineering, and combinatorial design.
   - This broad applicability will be demonstrated through experiments on diverse datasets and real-world applications.

The impact of this research is expected to be significant, as it addresses a critical challenge in black-box discrete sampling and optimization. The proposed framework has the potential to accelerate the development of advanced machine learning models, improve the efficiency of combinatorial optimization problems, and enable the discovery of novel solutions in various scientific and engineering domains.