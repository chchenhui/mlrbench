**Title:** Latent Transition MCMC for Efficient Black-Box Discrete Optimization

**Motivation:** Optimizing black-box objective functions over large, high-dimensional discrete spaces is a critical challenge in areas like compiler optimization and materials discovery. Existing methods like random search are inefficient, while gradient-based or embedding methods often require objective function gradients or struggle with complex dependencies and non-smooth landscapes.

**Main Idea:** We propose Latent Transition MCMC (LT-MCMC), a novel approach that learns a continuous latent representation of probable *transitions* or *moves* between discrete states, rather than embedding the states themselves. Using a generative model (e.g., a conditional VAE trained on heuristic or random transitions), we map discrete neighborhood exploration to a smooth latent space. MCMC algorithms (e.g., Metropolis-Adjusted Langevin Algorithm) can then efficiently propose promising transitions within this latent space, potentially guided by a cheap surrogate model of the black-box objective. These latent proposals are decoded back into discrete state changes, evaluated using the true black-box function within a Metropolis-Hastings framework. LT-MCMC aims to accelerate exploration by focusing sampling effort on effective neighbourhood operators learned implicitly within the latent transition space, improving optimization performance on complex, black-box discrete problems.