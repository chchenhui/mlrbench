**Title:** Modeling and Mitigating Feedback Loaps in Human-Algorithmic Systems via Multi-Agent Reinforcement Learning  

**Motivation:** Feedback loops between human behavior and algorithmic decision-making can amplify societal harms such as polarization, misinformation spread, and inequality. For example, recommendation systems trained on user engagement data may promote extremist content to maximize clicks, shaping user preferences that further bias the algorithm. Quantifying these dynamics and designing systems resilient to harmful loops is critical for ethical AI.  

**Main Idea:** This research proposes a multi-agent reinforcement learning (MARL) framework to model bidirectional interactions between adaptive users and algorithmic decision-makers. Agents represent users with diverse, time-varying preferences (modeled via bounded rationality) and algorithms optimizing platform-specific objectives (e.g., engagement). The system is simulated as a dynamical game, capturing long-term societal outcomes like polarization metrics. To mitigate harmful loops, we introduce a hybrid optimization strategy: algorithms are trained with regularization terms penalizing metrics of disparity in user preference distributions, while users adapt via heuristic rules (e.g., confirmation bias). The framework will be validated on synthetic and real-world recommendation tasks, comparing societal outcomes under baseline and regularized policies. Expected outcomes include tools to diagnose feedback-driven risks and a taxonomy of mitigation strategies balancing platform goals and societal welfare.