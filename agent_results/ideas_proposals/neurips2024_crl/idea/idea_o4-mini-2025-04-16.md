Title: CausalDiff – Integrating Latent Structural Causal Models into Diffusion Generation

Motivation: Modern diffusion models excel at high-fidelity image synthesis but treat latent factors as undifferentiated noise, ignoring underlying cause–effect structure. This leads to entangled representations, brittleness under distribution shift or interventions, and limited interpretability—critical barriers for scientific applications and fairness-sensitive domains.

Main Idea: We propose CausalDiff, a diffusion-based generative framework that jointly learns a latent Structural Causal Model (SCM) and the reverse-diffusion process. A graph neural network parameterizes a learnable adjacency matrix and non-linear causal mechanisms in latent space. Training optimizes a variational ELBO augmented with intervention-aware contrastive losses: by synthetically intervening on latent dimensions, the model disentangles cause and effect and infers arrow directions. During sampling, ancestral sampling on the learned SCM generates causally coherent latent roots, which then drive the diffusion decoder to reconstruct images. We evaluate CausalDiff on synthetic benchmarks with known SCMs (dSprites, 3DShapes) and real-world image sets paired with simple interventions. Expected outcomes include improved latent disentanglement, robustness to out-of-distribution interventions, and enhanced transparency. This approach paves the way for interpretable, reliable generative AI in scientific modeling, simulation, and fairness-critical applications.