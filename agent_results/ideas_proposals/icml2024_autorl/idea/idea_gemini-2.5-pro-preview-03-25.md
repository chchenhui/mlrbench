**Title:** LLM-Guided AutoRL: Translating Task Descriptions into Effective RL Configurations

**Motivation:** Applying reinforcement learning effectively remains challenging due to the complex process of algorithm selection, hyperparameter tuning, and reward shaping, often requiring significant expertise. This brittleness limits RL's accessibility for novel problems. Large Language Models (LLMs) possess strong capabilities in understanding natural language and contextual information.

**Main Idea:** We propose using an LLM to bridge the gap between high-level task descriptions and low-level RL configurations. The system will take a natural language description of an RL problem (including environment characteristics, objectives, and constraints) as input. The LLM, potentially fine-tuned on a curated dataset mapping task descriptions to successful RL setups (algorithms, hyperparameters, reward function sketches), will generate candidate configurations. We hypothesize this approach can significantly reduce the manual effort required, outperforming default settings and approaching expert-level performance, thereby democratizing RL application. Evaluation will involve testing generated configurations across diverse RL benchmarks.