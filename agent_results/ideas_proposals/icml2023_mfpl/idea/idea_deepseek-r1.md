**Title:** Preference-Driven Imitation Learning for Robotic Manipulation via Active Human Feedback  

**Motivation:** Traditional imitation learning (IL) in robotics relies on high-quality expert demonstrations, which are costly and time-consuming to collect. Preference-based learning offers a scalable alternative, as humans can more easily compare trajectories than provide precise demonstrations. This research addresses the challenge of training robots to perform complex manipulation tasks with minimal expert data, enabling adaptation to dynamic environments and diverse user needs.  

**Main Idea:** We propose a hybrid framework combining preference-based learning with active imitation learning. Robots initially learn from a small set of suboptimal demonstrations, then iteratively query humans for trajectory preferences (e.g., "Which grasp is more stable?") to refine their policy. A reward model trained on these preferences guides reinforcement learning (RL) to optimize task performance. Key innovations include: 1) A diversity-driven active sampling strategy to select maximally informative trajectory pairs for human feedback, and 2) Integration of preference-derived rewards with offline RL to handle partial/noisy preferences. Experiments on real-world robotic arms (e.g., pick-and-place, tool use) will validate the approach against standard IL and RL baselines. Expected outcomes include reduced dependence on expert data and improved generalization to unseen tasks. This could democratize robotic deployment in settings like healthcare or logistics, where explicit reward engineering is impractical.