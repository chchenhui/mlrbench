**Title**: Formal Verification-Guided Reinforcement Learning for Correct Code Generation  

**Motivation**: Large language models (LLMs) for code generation often produce syntactically valid but functionally incorrect outputs, especially for low-resource languages lacking training data. Post-hoc formal verification methods catch errors reactively but do not inherently improve the modelâ€™s reasoning. Proactively integrating verification into the training process can align model outputs with correctness guarantees, addressing safety and reliability gaps critical for real-world deployment.  

**Main Idea**: We propose a reinforcement learning (RL) framework where LLMs are optimized to generate code that satisfies formal specifications. During training, generated code is verified using tools like SMT solvers and static analyzers, with verification outcomes converted into rewards. The model learns to prioritize code that meets functional and safety constraints encoded in specifications. For low-resource languages, we synthesize specifications from codebases via automated invariant detection or human annotations. Experiments on benchmarks for under-resourced languages (e.g., Raku, Nim) will evaluate correctness rates against baselines. Expected outcomes include improved alignment between generated code and formal properties, reduced post-hoc repair needs, and a scalable methodology for trustworthy AI-driven code synthesis. This bridges probabilistic LLMs and formal methods, enabling safer code generation in resource-constrained settings.