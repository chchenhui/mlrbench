**Title:** RobustSafeGen: Integrated Robustness and Formal Safety Verification for Generative AI in Healthcare  

**Motivation:** Deploying generative AI in healthcare requires rigorous safety and robustness guarantees, as errors can have life-threatening consequences. Current methods often address robustness (e.g., adversarial attacks) or safety (e.g., output constraints) in isolation, leaving gaps in real-world reliability.  

**Main Idea:** We propose a framework combining adversarial training with formal verification to ensure generative models adhere to predefined safety rules while resisting input perturbations. The methodology involves: (1) Training models on medical data with adversarial examples to improve robustness; (2) Embedding domain-specific safety constraints (e.g., drug interaction checks) into the architecture via constrained optimization; (3) Applying formal verification post-training to mathematically validate that generated outputs (e.g., treatment suggestions) never violate safety properties. Expected outcomes include models with quantifiable robustness metrics (e.g., attack success rate reduction by 30%) and formal safety certificates. Impact: This approach bridges the gap between empirical robustness and provable safety, enabling trustworthy deployment of generative AI in high-stakes healthcare applications like diagnosis or drug discovery.