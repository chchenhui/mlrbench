Title: Explainable Chain-of-Thought Automated Scoring with Saliency Attribution

Motivation: Large language models have dramatically improved automated essay scoring accuracy but remain “black boxes,” limiting educator and stakeholder trust. Delivering clear, rubric-aligned explanations alongside numeric scores is essential to ensure fairness, accountability, and wide adoption in high-stakes assessments.

Main Idea: We introduce a two-phase framework:  
1. Chain-of-Thought Prompting: Fine-tune a foundation model on rubric-annotated essays using chain-of-thought (CoT) prompts, so that for each response it generates a stepwise rationale linked to scoring dimensions (e.g., coherence, grammar, argument strength).  
2. Saliency Attribution: Apply feature-attribution techniques (e.g., Integrated Gradients) over the CoT outputs to quantify each rationale step’s contribution to the final score. The system then produces (a) a numeric score, (b) a human-readable rationale mapped to rubric criteria, and (c) highlighted evidence in the student’s text.  

We will evaluate on public essay datasets and through educator feedback loops. Expected outcomes include enhanced transparency, reduced bias, and strengthened stakeholder confidence—paving the way for trustworthy, AI-driven educational assessments.