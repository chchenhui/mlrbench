Title: CogMatTest – A Cognitive-Skill–Centered Adaptive Benchmark for Mathematical Reasoning

Motivation:  
Current math benchmarks for LLMs obscure the underlying cognitive competencies—pattern recognition, symbolic manipulation, abstraction, proof planning—that drive performance. Without fine‐grained assessment, we cannot pinpoint strengths or guide targeted model improvements. An adaptive, interpretable evaluation is needed to measure and compare both AI models and human reasoners across core mathematical skills.

Main Idea:  
We introduce CogMatTest, an adaptive benchmarking suite built on a cognitive‐skill taxonomy of mathematical reasoning. Key components:  
1. Skill Taxonomy & Item Bank – Curated and procedurally generated problems annotated by skills (e.g., inference, planning, generalization) and calibrated for difficulty via item response theory (IRT).  
2. Adaptive Test Engine – Dynamically selects items based on real‐time performance, constructing a multidimensional profile of a subject’s proficiency in each cognitive skill.  
3. Comparative Evaluation – Apply CogMatTest to LLMs (e.g., GPT-4, PaLM) and human cohorts to reveal detailed skill gaps.  

Expected outcomes include (1) interpretable AI skill profiles, (2) insights for targeted model architecture or training tweaks, and (3) guidelines for AI–human collaborative tools in education and theorem proving.