# Adaptive Mathematical Reasoning Assessment via Procedural Problem Generation

## Introduction

### Background  
Mathematical reasoning—the ability to abstract, generalize, and derive logical conclusions—is a cornerstone of human intelligence. Recent advancements in large language models (LLMs) have demonstrated remarkable proficiency on benchmarks like MATH and GSM8K, where models solve pre-curated problems with high accuracy. However, these static benchmarks face two critical limitations: **data contamination** (training sets overlapping with test sets) and **limited diagnostic utility** (focusing on final-answer accuracy rather than reasoning processes). For instance, studies show that LLMs often exploit shallow pattern-matching rather than deep comprehension, particularly when exposed to overfitting during training (Sharma et al., 2023). This raises a pivotal question: *How can we design evaluations that robustly measure genuine mathematical reasoning capabilities in LLMs?*

### Research Objectives  
This proposal addresses this challenge by developing an **adaptive assessment framework** for mathematical reasoning, grounded in procedural content generation (PCG). The system will:  
1. Generate math problems dynamically using parameterized templates, ensuring diversity and contamination resistance.  
2. Adapt problem difficulty and type in real-time based on model performance, enabling fine-grained diagnosis of strengths and weaknesses.  
3. Evaluate reasoning quality beyond final answers, incorporating metrics for step validity and logical coherence.  

### Significance  
This work directly responds to the workshop’s theme by probing the boundaries of LLM mathematical comprehension. By moving beyond static leaderboards, our framework will:  
- **Mitigate data leakage risks** through procedurally generated, never-before-seen problems.  
- **Enable personalized assessment**, akin to human tutoring systems, to identify gaps in reasoning strategies.  
- **Inform educational applications**, such as adaptive tutoring systems for resource-constrained environments.  

Our approach builds on recent work like Mathador-LM (Kurtic et al., 2024), which introduced dynamic benchmarks, and ReasonEval (Xia et al., 2024), which emphasized reasoning-step validity. However, we uniquely integrate **adaptive difficulty adjustment** and **multi-dimensional evaluation** to create a holistic diagnostic tool.

---

## Methodology  

### 1. Procedural Content Generation (PCG) Framework  

#### Template Design and Parameterization  
We will curate a library of problem templates spanning domains such as algebra, geometry, and combinatorics. Each template defines:  
- **Structural constraints** (e.g., "Solve for $x$ in $ax^2 + bx + c = 0$").  
- **Parameter ranges** (e.g., $a \in [1, 10]$, $b \in [-5, 5]$).  
- **Reasoning skills targeted** (e.g., factoring, substitution).  

Problems are generated by sampling parameters within constraints and applying **transformations** to vary presentation (e.g., changing variable names, embedding in word problems). For example, a quadratic equation template might generate:  
$$
\text{Solve } 3x^2 - 7x + 2 = 0 \quad \text{or} \quad \text{Find roots of } 2y^2 + 5y - 3 = 0.
$$  

#### Diversity and Contamination Resistance  
To avoid unintended overlaps with training data, all templates and parameters will be manually curated and validated against public datasets using semantic hashing (Reid et al., 2023).  

---

### 2. Adaptive Difficulty Adjustment Mechanism  

#### Performance Tracking  
The system will monitor model performance using a **sliding window** of recent responses. For each problem type (e.g., linear equations), we compute:  
- **Accuracy**: $A = \frac{\text{Correct Answers}}{\text{Total Attempts}}$.  
- **Step Consistency**: A score measuring alignment between intermediate steps and reference solutions (adapted from ReasonEval; Xia et al., 2024).  

#### Difficulty Scaling  
Difficulty is parameterized via a scalar $d \in [0, 1]$, where $d=0$ represents minimal complexity (e.g., single-step arithmetic) and $d=1$ maximal complexity (e.g., multi-concept proofs). Adjustments follow:  
$$
d_{\text{new}} = 
\begin{cases} 
\min(d + \Delta, 1) & \text{if } A > \tau_{\text{high}} \\
\max(d - \Delta, 0) & \text{if } A < \tau_{\text{low}} \\
d & \text{otherwise}
\end{cases}
$$  
Here, $\Delta$ is a step size (e.g., 0.1), and $\tau_{\text{high}}, \tau_{\text{low}}$ are thresholds (e.g., 0.8 and 0.5).  

#### Skill-Specific Adaptation  
If a model excels in algebra ($A_{\text{algebra}} > 0.9$) but struggles in geometry ($A_{\text{geometry}} < 0.4$), the system will:  
1. Increase algebra problem complexity (e.g., introducing polynomial identities).  
2. Simplify geometry problems (e.g., decomposing multi-step proofs into subtasks).  

---

### 3. Experimental Design and Evaluation Metrics  

#### Baseline Models  
We will evaluate:  
- **State-of-the-art LLMs**: Llama-3-8B, Mistral-7B, and GPT-4.  
- **Specialized models**: MathPrompter (Imani et al., 2023) and TATA (Xu et al., 2025).  

#### Comparative Benchmarks  
- **Static benchmarks**: MATH, GSM8K.  
- **Dynamic benchmarks**: Mathador-LM (Kurtic et al., 2024).  

#### Evaluation Metrics  
1. **Final-Answer Accuracy**: Standard benchmark metric.  
2. **Step Validity**: Proportion of reasoning steps matching reference solutions (ReasonEval; Xia et al., 2024).  
3. **Generalization**: Performance on out-of-distribution (OOD) problems generated by unseen templates.  
4. **Adaptability Score**: How effectively models improve on reattempted problem types.  

#### Ablation Studies  
- **Template diversity**: Measure performance variance when restricting template subsets.  
- **Adaptation strategies**: Compare threshold-based scaling (above) to reinforcement learning approaches.  

#### Human Validation  
A panel of mathematicians will rate:  
- **Problem quality**: Clarity and educational relevance.  
- **Solution correctness**: For edge cases where automated metrics are ambiguous.  

---

## Expected Outcomes & Impact  

### 1. Robust, Adaptive Assessment System  
- **Open-source PCG toolkit**: A library of 1,000+ parameterized templates and APIs for dynamic problem generation.  
- **Contamination-resistant benchmarks**: Datasets like **MathBench-PCG**, immune to training-data overlap.  

### 2. Diagnostic Insights into LLM Reasoning  
- **Skill-specific profiles**: Heatmaps highlighting model strengths (e.g., GPT-4’s algebra prowess) and gaps (e.g., Llama-3’s struggles with geometric proofs).  
- **Step-level analysis**: Identification of common failure modes, such as sign errors in equation manipulation.  

### 3. Educational Applications  
- **Adaptive tutoring systems**: Deploying the PCG engine to generate personalized math exercises for K-12 learners.  
- **Teacher dashboards**: Real-time analytics of student reasoning patterns, inspired by TATA’s personalized learning framework (Xu et al., 2025).  

### 4. Theoretical and Practical Impact  
- **Advancing evaluation science**: Shifting from "accuracy-only" metrics to multi-dimensional reasoning assessment.  
- **Guiding model development**: Informing training strategies (e.g., curricula focusing on weak reasoning skills).  

---

## Conclusion  

This proposal introduces a paradigm shift in evaluating mathematical reasoning in AI. By combining procedural content generation, adaptive difficulty scaling, and multi-faceted evaluation metrics, our framework will provide deeper insights into LLM capabilities than ever before. The resulting tools will not only advance AI research but also democratize access to personalized math education. As LLMs increasingly permeate scientific and educational domains, such rigorous assessment mechanisms are critical to ensuring their reliability and growth.  

---

**References**  
- Kurtic et al. (2024). *Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models*. arXiv:2406.12572.  
- Xia et al. (2024). *Evaluating Mathematical Reasoning Beyond Accuracy*. arXiv:2404.05692.  
- Xu et al. (2025). *Teaching LLMs According to Their Aptitude: Adaptive Reasoning for Mathematical Problem Solving*. arXiv:2502.12022.  
- Imani et al. (2023). *MathPrompter: Mathematical Reasoning using Large Language Models*. arXiv:2303.05398.  

This proposal directly aligns with the workshop’s goal of exploring AI’s role in mathematical reasoning while addressing pressing challenges in evaluation robustness and interpretability.