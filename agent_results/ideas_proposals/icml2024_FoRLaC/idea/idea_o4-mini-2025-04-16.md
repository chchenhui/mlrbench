Title: Lyapunov-Regularized Optimistic Exploration for Nonlinear Control Systems

Motivation:  
Real-world control tasks require both rapid learning and provable stability. Traditional RL exploration can destabilize nonlinear systems, while control theory offers Lyapunov-based stability guarantees but lacks efficient data-driven exploration. Bridging these will enable safe, sample-efficient learning in high-stakes domains like autonomous vehicles and industrial automation.

Main Idea:  
We propose a model-based RL framework that interleaves optimistic exploration with Lyapunov stability constraints. First, fit a Gaussian process dynamics model and derive uncertainty bounds. Second, learn a candidate Lyapunov function Vθ via temporal-difference on nominal data, enforcing Vθ(xₜ₊₁)–Vθ(xₜ)≤–α||xₜ||² in expectation. Third, at each decision step solve a constrained MPC: maximize an optimism-driven exploration bonus (uncertainty reduction) subject to the Lyapunov decrease condition. We prove that under mild excitation assumptions this yields Õ(√T) regret while maintaining input-to-state stability. Experiments on robotic and supply-chain benchmarks will validate faster convergence, guaranteed safety, and robustness to modeling errors.