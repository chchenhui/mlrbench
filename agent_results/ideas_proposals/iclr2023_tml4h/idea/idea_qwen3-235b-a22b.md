1. **Title**: Causal Uncertainty Quantification via Disentangled Representation Learning for Robust Medical Diagnosis  

2. **Motivation**: In healthcare, ML models often fail to generalize to real-world data due to distribution shifts, spurious correlations, and unmeasured confounders. While current methods address uncertainty through probabilistic modeling, they frequently overlook causal relationships critical for interpretable decision-making. Without explicit causal disentanglement, uncertainty estimates become unreliable when input data deviate from training distributions. This is particularly risky in high-stakes applications like cancer detection, where uncertainty should reflect the causal relevance of features (e.g., tumor characteristics) rather than irrelevant biases (e.g., scanner artifacts). Developing causally grounded uncertainty quantification would improve both model reliability and clinical trust, bridging the gap between ML deployment and practical usability in healthcare.  

3. **Main Idea**: Propose a causally disentangled Bayesian neural network framework that explicitly separates causal factors (e.g., pathology-related features) from non-causal confounders in medical imaging data. The model will use causal discovery techniques to learn a sparse causal graph from heterogeneous datasets (e.g., multi-center MRI scans). Conditional on the causal structure, a hybrid architecture will encode disentangled latent representations and propagate aleatoric/epistemic uncertainty through causal pathways. For example, in prostate cancer grading from histology images, the model will identify causal pathways (e.g., gland morphology) and downweight non-causal artifacts (e.g., staining variations). Validation will use synthetic counterfactual benchmarks and clinical datasets with varying acquisition protocols. Expected outcomes include robust uncertainty metrics aligning with human expert intuitions about diagnostic reliability, validated through uncertainty calibration metrics and clinician feedback. This work could transform uncertainty estimation into a causal-aware framework, reducing overconfidence in spurious predictions and enabling safer deployment in heterogeneous clinical environments.