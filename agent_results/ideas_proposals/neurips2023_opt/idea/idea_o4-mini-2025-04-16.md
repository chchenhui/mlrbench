Title: Capacity-Aware Extrapolated Hyperparameter Scheduling

Motivation:
As model sizes climb into the billions of parameters, exhaustive hyperparameter search becomes prohibitively expensive in compute, time, and energy. Yet small-scale models can be tuned cheaply. If we can learn how optimal hyperparameters (learning rate, batch size, momentum) systematically vary with model capacity, we can predict near-optimal settings for very large networks—dramatically cutting tuning costs and environmental impact.

Main Idea:
We propose to train a diverse suite of small‐to‐medium models (varying width, depth, and architecture) under automated hyperparameter sweeps to identify their optimal settings. We then fit smooth scaling functions (e.g., via Bayesian regression or neural surrogates) that map model capacity metrics (parameter count, FLOPs) to each hyperparameter. At deployment, a target large model’s capacity is plugged into these learned functions to generate its initial hyperparameter schedule. We validate this pipeline on transformer families ranging from 10M to 1B parameters, showing that extrapolated schedules achieve within 2% of exhaustive search performance while cutting tuning compute by over 80%. This approach generalizes across optimizers and architectures, enabling efficient fine-tuning and environmentally sustainable large-scale training.