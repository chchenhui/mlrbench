**Title:** Predictive Coding-Driven Active Inference for Data-Efficient Reinforcement Learning

**Motivation:** Reinforcement learning (RL) agents often require extensive interaction with the environment to learn effective policies. Biological systems, however, learn remarkably efficiently. Predictive coding and active inference theories suggest the brain minimizes surprise by refining internal models and acting purposefully. Integrating these principles into RL could drastically improve sample efficiency.

**Main Idea:** We propose an RL framework where the agent embodies active inference principles implemented via hierarchical predictive coding networks. The agent learns a world model by minimizing prediction errors (free energy) on sensory inputs. Crucially, actions are selected not just to maximize reward but to minimize *expected* free energy â€“ actively seeking information-rich states that reduce uncertainty about the world model or task goals. This involves generating predictions about the consequences of potential actions and choosing those leading to the least surprising outcomes consistent with the agent's objectives. We will evaluate this approach on sparse-reward or complex exploration tasks, comparing its sample efficiency and final performance against standard model-based and model-free RL algorithms. The expected outcome is a more data-efficient RL agent with intrinsically motivated exploration stemming from neuro-inspired principles.