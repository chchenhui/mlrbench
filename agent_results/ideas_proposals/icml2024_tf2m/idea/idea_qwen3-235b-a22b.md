1. **Title**: Information-Theoretic Probing of Transformer Architectures: Bridging Compression and Emergent Capabilities  

2. **Motivation**: Despite transformers' dominance in foundation models (FMs), their architectural design principles remain poorly understood. This gap hinders progress in efficiency, interpretability, and mitigating emergent risks (e.g., bias amplification). A theoretical framework explaining how transformers achieve superior compression and in-context learning could enable principled improvements in model design and responsible deployment.  

3. **Main Idea**: This work proposes an information-theoretic framework to quantify how transformers encode, compress, and propagate information across layers. By analyzing mutual information between inputs, attention mechanisms, and hidden representations, we aim to identify architectural inductive biases that drive capabilities like long-range dependency modeling and few-shot learning. Key steps include: (1) developing estimators for information flow in attention heads and feedforward layers, (2) comparing these patterns across transformer variants (e.g., dense vs. Mixture-of-Experts), and (3) correlating information bottlenecks with emergent behaviors (e.g., memorization vs. generalization). The resulting insights will guide the design of lightweight architectures with provable efficiency gains and reduced reliance on massive-scale training. Expected outcomes include a diagnostic toolkit for auditing FM capabilities and a new class of "information-aware" models that balance performance with transparency. This work bridges the theory-practice divide, directly addressing the workshopâ€™s goals of advancing principled foundations for FMs.  

(Word count: 199)