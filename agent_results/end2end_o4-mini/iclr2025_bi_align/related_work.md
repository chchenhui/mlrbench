1. **Title**: The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI Collaboration (arXiv:2502.01493)
   - **Authors**: Aung Pyae
   - **Summary**: This paper introduces the "Human-AI Handshake Model," emphasizing a bidirectional, adaptive framework for human-AI collaboration. It highlights five key attributes: information exchange, mutual learning, validation, feedback, and mutual capability augmentation, aiming to foster balanced interactions and dynamic partnerships between humans and AI systems.
   - **Year**: 2025

2. **Title**: Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration (arXiv:2304.14130)
   - **Authors**: Ioannis Papantonis, Vaishak Belle
   - **Summary**: This study examines how combining model explanations with uncertainty estimates affects user reliance, understanding, and trust in AI systems. It also explores the impact of users' self-confidence on their interactions with AI, providing insights into enhancing human-AI collaboration through transparency and uncertainty communication.
   - **Year**: 2023

3. **Title**: Overconfident and Unconfident AI Hinder Human-AI Collaboration (arXiv:2402.07632)
   - **Authors**: Jingshu Li, Yitian Yang, Renwen Zhang, Yi-chieh Lee
   - **Summary**: This research investigates the effects of uncalibrated AI confidence on user trust and collaboration outcomes. It finds that both overconfident and underconfident AI can lead to misuse or disuse, respectively, and emphasizes the importance of confidence calibration and trust calibration support in enhancing human-AI collaboration.
   - **Year**: 2024

4. **Title**: Human Uncertainty in Concept-Based AI Systems (arXiv:2303.12872)
   - **Authors**: Katherine M. Collins, Matthew Barker, Mateo Espinosa Zarlenga, Naveen Raman, Umang Bhatt, Mateja Jamnik, Ilia Sucholutsky, Adrian Weller, Krishnamurthy Dvijotham
   - **Summary**: This paper studies human uncertainty in concept-based AI systems, highlighting the challenges of integrating uncertain human feedback into AI models. It introduces datasets and tools to facilitate research on building interactive, uncertainty-aware systems that can effectively handle human uncertainty in collaborative tasks.
   - **Year**: 2023

5. **Title**: AI Alignment (Wikipedia)
   - **Authors**: Wikipedia contributors
   - **Summary**: This article provides an overview of AI alignment, discussing the challenges and approaches in aligning AI systems with human values and goals. It highlights issues such as AI deception and the importance of ensuring that AI systems act in accordance with intended objectives.
   - **Year**: 2025

**Key Challenges:**

1. **Uncalibrated AI Confidence**: AI systems often exhibit overconfidence or underconfidence, leading to misuse or disuse by human collaborators. Ensuring accurate confidence calibration is crucial for effective human-AI collaboration.

2. **Human Uncertainty**: Integrating uncertain human feedback into AI systems poses challenges, as human inputs can be inconsistent or ambiguous, affecting the reliability of the AI's learning process.

3. **Dynamic Human Preferences**: Human preferences and values can evolve over time, making it difficult for static AI alignment techniques to maintain effective collaboration without continuous adaptation.

4. **Transparency and Trust**: Achieving transparency in AI decision-making processes is essential to build user trust. However, providing explanations that are both comprehensive and understandable remains a significant challenge.

5. **Ethical Considerations**: Ensuring that AI systems align with ethical principles and societal norms requires ongoing evaluation and adaptation, particularly as AI systems take on more complex decision-making roles. 