2025-08-03 15:17:17,021 - INFO - Starting experiment...
2025-08-03 15:17:17,021 - INFO - Loading dataset: super_glue (boolq)
2025-08-03 15:17:29,929 - INFO - Initializing Dense Model (T5-small)
2025-08-03 15:17:34,571 - INFO - Initializing Standard MoE Model
2025-08-03 15:17:35,089 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:17:46,181 - INFO - Use pytorch device_name: cuda
2025-08-03 15:17:46,944 - INFO - Initializing PRo-MoE Model
2025-08-03 15:17:47,439 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:17:49,618 - INFO - Use pytorch device_name: cuda
2025-08-03 15:17:50,117 - INFO - --- Training dense_model ---
2025-08-03 15:17:51,997 - ERROR - An error occurred during the experiment: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 311, in main
    results, logs = run_experiment()
                    ^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 228, in run_experiment
    dense_results, dense_logs = train_and_evaluate(dense_model, train_dataset, eval_dataset, "dense_model")
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 191, in train_and_evaluate
    trainer.train()
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 2116, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer_callback.py", line 371, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer_callback.py", line 415, in call_event
    result = getattr(callback, event)(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/integrations/integration_utils.py", line 768, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/integrations/integration_utils.py", line 741, in setup
    self._wandb.init(
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1620, in init
    wandb._sentry.reraise(e)
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/analytics/sentry.py", line 157, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 1548, in init
    wi.maybe_login(init_settings)
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/sdk/wandb_init.py", line 191, in maybe_login
    wandb_login._login(
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/sdk/wandb_login.py", line 315, in _login
    key, key_status = wlogin.prompt_api_key(referrer=referrer)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/wandb/sdk/wandb_login.py", line 243, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
2025-08-03 15:18:26,642 - INFO - Starting experiment...
2025-08-03 15:18:26,643 - INFO - Loading dataset: super_glue (boolq)
2025-08-03 15:18:33,346 - INFO - Initializing Dense Model (T5-small)
2025-08-03 15:18:34,430 - INFO - Initializing Standard MoE Model
2025-08-03 15:18:34,934 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:18:37,136 - INFO - Use pytorch device_name: cuda
2025-08-03 15:18:37,873 - INFO - Initializing PRo-MoE Model
2025-08-03 15:18:38,374 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:18:40,585 - INFO - Use pytorch device_name: cuda
2025-08-03 15:18:41,082 - INFO - --- Training dense_model ---
2025-08-03 15:18:49,769 - INFO - --- Evaluating dense_model ---
2025-08-03 15:18:50,331 - INFO - --- Training standard_moe ---
2025-08-03 15:18:51,392 - ERROR - An error occurred during the experiment: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 97, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 109, in forward
    moe_output = self.moe_layer(hidden_states + task_bias)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 68, in forward
    output = torch.einsum('be,beio->bio', routing_weights, expert_outputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/functional.py", line 422, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 0 and no ellipsis was given
Traceback (most recent call last):
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 312, in main
    results, logs = run_experiment()
                    ^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 239, in run_experiment
    moe_results, moe_logs = train_and_evaluate(standard_moe_model, standard_moe_train_dataset, standard_moe_eval_dataset, "standard_moe")
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 192, in train_and_evaluate
    trainer.train()
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 194, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 213, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 127, in parallel_apply
    output.reraise()
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/_utils.py", line 750, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 97, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 109, in forward
    moe_output = self.moe_layer(hidden_states + task_bias)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 68, in forward
    output = torch.einsum('be,beio->bio', routing_weights, expert_outputs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/functional.py", line 422, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): the number of subscripts in the equation (2) does not match the number of dimensions (3) for operand 0 and no ellipsis was given

2025-08-03 15:19:40,901 - INFO - Starting experiment...
2025-08-03 15:19:40,901 - INFO - Loading dataset: super_glue (boolq)
2025-08-03 15:19:47,316 - INFO - Initializing Dense Model (T5-small)
2025-08-03 15:19:48,334 - INFO - Initializing Standard MoE Model
2025-08-03 15:19:48,840 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:19:51,195 - INFO - Use pytorch device_name: cuda
2025-08-03 15:19:52,123 - INFO - Initializing PRo-MoE Model
2025-08-03 15:19:52,658 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:19:54,745 - INFO - Use pytorch device_name: cuda
2025-08-03 15:19:55,271 - INFO - --- Training dense_model ---
2025-08-03 15:20:04,444 - INFO - --- Evaluating dense_model ---
2025-08-03 15:20:05,016 - INFO - --- Training standard_moe ---
2025-08-03 15:20:06,304 - ERROR - An error occurred during the experiment: Caught AttributeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 97, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 120, in forward
    moe_output = self.moe_layer(hidden_states, task_bias=task_bias)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 65, in forward
    logits += task_bias.unsqueeze(0).unsqueeze(0) # Broadcast to (1, 1, num_experts)
              ^^^^^^^^^^^^^^^^^^^
AttributeError: 'int' object has no attribute 'unsqueeze'
Traceback (most recent call last):
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 323, in main
    results, logs = run_experiment()
                    ^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 250, in run_experiment
    moe_results, moe_logs = train_and_evaluate(standard_moe_model, standard_moe_train_dataset, standard_moe_eval_dataset, "standard_moe")
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 203, in train_and_evaluate
    trainer.train()
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 2203, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 3138, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/transformers/trainer.py", line 3161, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 194, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 213, in parallel_apply
    return parallel_apply(
           ^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 127, in parallel_apply
    output.reraise()
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/_utils.py", line 750, in reraise
    raise exception
AttributeError: Caught AttributeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py", line 97, in _worker
    output = module(*input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 120, in forward
    moe_output = self.moe_layer(hidden_states, task_bias=task_bias)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 65, in forward
    logits += task_bias.unsqueeze(0).unsqueeze(0) # Broadcast to (1, 1, num_experts)
              ^^^^^^^^^^^^^^^^^^^
AttributeError: 'int' object has no attribute 'unsqueeze'

2025-08-03 15:20:50,234 - INFO - Starting experiment...
2025-08-03 15:20:50,234 - INFO - Loading dataset: super_glue (boolq)
2025-08-03 15:20:56,554 - INFO - Initializing Dense Model (T5-small)
2025-08-03 15:20:57,659 - INFO - Initializing Standard MoE Model
2025-08-03 15:20:58,236 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:21:00,449 - INFO - Use pytorch device_name: cuda
2025-08-03 15:21:01,202 - INFO - Initializing PRo-MoE Model
2025-08-03 15:21:01,707 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:21:03,872 - INFO - Use pytorch device_name: cuda
2025-08-03 15:21:04,386 - INFO - --- Training dense_model ---
2025-08-03 15:21:13,326 - INFO - --- Evaluating dense_model ---
2025-08-03 15:21:13,894 - INFO - --- Training standard_moe ---
2025-08-03 15:21:19,563 - INFO - --- Evaluating standard_moe ---
2025-08-03 15:21:20,198 - INFO - --- Training pro_moe ---
2025-08-03 15:21:25,883 - INFO - --- Evaluating pro_moe ---
2025-08-03 15:21:26,558 - INFO - Saving raw results...
2025-08-03 15:21:26,559 - INFO - Generating plots...
2025-08-03 15:21:26,939 - INFO - Generating results.md...
2025-08-03 15:21:26,940 - ERROR - An error occurred during the experiment: 'eval_steps'
Traceback (most recent call last):
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 340, in main
    generate_results_md(results)
  File "/home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_scope/gemini/run_experiment.py", line 290, in generate_results_md
    md_content += f"| {name.replace('_', ' ').title()} | {res['eval_loss']:.4f} | {res['epoch']} | {res['eval_steps']} |\n"
                                                                                                    ~~~^^^^^^^^^^^^^^
KeyError: 'eval_steps'
2025-08-03 15:22:26,884 - INFO - Starting experiment...
2025-08-03 15:22:26,884 - INFO - Loading dataset: super_glue (boolq)
2025-08-03 15:22:33,174 - INFO - Initializing Dense Model (T5-small)
2025-08-03 15:22:34,144 - INFO - Initializing Standard MoE Model
2025-08-03 15:22:34,643 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:22:36,825 - INFO - Use pytorch device_name: cuda
2025-08-03 15:22:37,426 - INFO - Initializing PRo-MoE Model
2025-08-03 15:22:37,941 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-08-03 15:22:40,216 - INFO - Use pytorch device_name: cuda
2025-08-03 15:22:40,744 - INFO - --- Training dense_model ---
2025-08-03 15:22:49,562 - INFO - --- Evaluating dense_model ---
2025-08-03 15:22:50,138 - INFO - --- Training standard_moe ---
2025-08-03 15:22:55,674 - INFO - --- Evaluating standard_moe ---
2025-08-03 15:22:56,291 - INFO - --- Training pro_moe ---
2025-08-03 15:23:01,644 - INFO - --- Evaluating pro_moe ---
2025-08-03 15:23:02,291 - INFO - Saving raw results...
2025-08-03 15:23:02,293 - INFO - Generating plots...
2025-08-03 15:23:02,698 - INFO - Generating results.md...
2025-08-03 15:23:02,699 - INFO - Experiment finished successfully!
