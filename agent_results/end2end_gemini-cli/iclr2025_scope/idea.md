**Title:** Proactive Routing in Mixture-of-Experts for Zero-Shot Task Adaptation

**Motivation:** Mixture-of-Experts (MoE) models achieve high efficiency by activating only a subset of parameters per input. However, their routing function is typically trained on a pre-defined data distribution, limiting its adaptability to new, unseen tasks at inference time. This leads to suboptimal expert selection and performance degradation on out-of-distribution or novel downstream tasks without costly re-training or fine-tuning of the routing network.

**Main Idea:** We propose a "Proactive Router," a lightweight meta-network that predicts optimal routing policies for unseen tasks in a zero-shot manner. This meta-router is trained on a diverse set of tasks, learning to map from a task description (e.g., a natural language prompt or a few examples) to parameters for the main modelâ€™s MoE gating network. At inference, given a new task, the meta-router generates a task-specific routing configuration on-the-fly. This allows the MoE model to dynamically specialize its expert utilization for the task at hand without any gradient updates, enabling rapid and efficient adaptation to a continuous stream of new computational demands.