**Title:** Detecting Neural Backdoors via Permutation-Equivariant Weight Analysis

**Motivation:** Backdoor attacks, which embed hidden malicious behavior in neural networks, pose a significant security risk. Current detection methods often require access to training data or knowledge of the attack pattern. This research aims to develop a data-free method to identify backdoored models by directly analyzing their parameters, based on the premise that backdoor mechanisms leave detectable artifacts in the weight space.

**Main Idea:** We propose using a Graph Neural Network (GNN) to classify a model as clean or backdoored by processing its computational graph. In this setup, neurons are nodes and weights are edge attributes. This GNN architecture is naturally permutation-equivariant, allowing it to learn canonical "fingerprints" of backdoor attacks that are independent of neuron orderingâ€”a key symmetry in neural networks. We will construct a large dataset of models (a "model zoo") containing both clean and backdoored instances to train the GNN detector. The expected outcome is a robust tool that flags malicious models by inspecting their weights alone, generalizing across different architectures and attack types without needing access to any external data.