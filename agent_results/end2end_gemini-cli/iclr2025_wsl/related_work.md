1. **Title**: ProP: Efficient Backdoor Detection via Propagation Perturbation for Overparametrized Models (arXiv:2411.07036)
   - **Authors**: Tao Ren, Qiongxiu Li
   - **Summary**: This paper introduces ProP, a novel backdoor detection method that leverages statistical output distributions to identify backdoored models without relying on exhaustive optimization strategies. ProP operates with minimal assumptions, requiring no prior knowledge of triggers or malicious samples, making it highly applicable to real-world scenarios.
   - **Year**: 2024

2. **Title**: Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features (arXiv:2306.16697)
   - **Authors**: Mingli Zhu, Shaokui Wei, Hongyuan Zha, Baoyuan Wu
   - **Summary**: The authors propose a backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer. This approach aims to purify poisoned samples by filtering trigger information while maintaining benign information, requiring only a limited clean dataset for training.
   - **Year**: 2023

3. **Title**: UMD: Unsupervised Model Detection for X2X Backdoor Attacks (arXiv:2305.18651)
   - **Authors**: Zhen Xiang, Zidi Xiong, Bo Li
   - **Summary**: This paper presents UMD, the first unsupervised model detection method that effectively detects X2X backdoor attacks via joint inference of adversarial (source, target) class pairs. The method defines a novel transferability statistic to measure and select a subset of putative backdoor class pairs based on a proposed clustering approach.
   - **Year**: 2023

4. **Title**: Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection (arXiv:2307.05422)
   - **Authors**: Hao Fu, Prashanth Krishnamurthy, Siddharth Garg, Farshad Khorrami
   - **Summary**: The authors propose a data-efficient detection method for deep neural networks against backdoor attacks under a black-box scenario. The approach introduces five metrics to measure the effects of triggers and benign features on determining the backdoored network output, utilizing synthetic samples generated by injecting partial contents into clean validation samples.
   - **Year**: 2023

5. **Title**: ReVeil: Unconstrained Concealed Backdoor Attack on Deep Neural Networks using Machine Unlearning
   - **Authors**: Manaar Alam, Hithem Lamri, Michail Maniatakos
   - **Summary**: This paper introduces ReVeil, a concealed backdoor attack targeting the data collection phase of the DNN training pipeline, requiring no model access or auxiliary data. ReVeil maintains low pre-deployment attack success rates and restores high success rates post-deployment through machine unlearning.
   - **Year**: 2025

6. **Title**: Cross-Attention Graph Neural Networks for Inferring Gene Regulatory Networks with Skewed Degree Distribution (arXiv:2412.16220)
   - **Authors**: Jiaqi Xiong, Nan Yin, Yifan Sun, Haoyang Li, Yingxu Wang, Duo Ai, Fang Pan, Shiyang Liang
   - **Summary**: The authors propose the Cross-Attention Complex Dual Graph Embedding Model (XATGRN) to infer gene regulatory networks from gene expression data, addressing the challenge of skewed degree distribution. This model introduces a cross-attention mechanism to effectively capture complex regulatory relationships.
   - **Year**: 2024

7. **Title**: Eliminating Backdoors in Neural Code Models via Trigger Inversion (arXiv:2408.04683)
   - **Authors**: Weisong Sun, Yuchen Chen, Chunrong Fang, Yebo Feng, Yuan Xiao, An Guo, Quanjun Zhang, Yang Liu, Baowen Xu, Zhenyu Chen
   - **Summary**: This paper addresses the vulnerability of neural code models to backdoor attacks by proposing a method to eliminate backdoors via trigger inversion. The approach focuses on identifying and neutralizing adversary-crafted triggers to restore model integrity.
   - **Year**: 2024

8. **Title**: TrojanInterpret: A Detecting Backdoors Method in DNN Based on Neural Network Interpretation Methods
   - **Authors**: [Authors not specified]
   - **Summary**: The authors present a new method for backdoor detection in neural networks based on neural network interpretation techniques. The approach utilizes the difference between distributions of saliency values of neural networks with and without backdoors to detect malicious alterations.
   - **Year**: 2024

9. **Title**: Is It Possible to Backdoor Face Forgery Detection with Natural Triggers? (arXiv:2401.00414)
   - **Authors**: Xiaoxuan Han, Songlin Yang, Wei Wang, Ziwen He, Jing Dong
   - **Summary**: This paper investigates the feasibility of backdooring face forgery detection models using natural triggers. The authors propose an analysis-by-synthesis backdoor attack that embeds natural triggers in the latent space, challenging existing detection methods.
   - **Year**: 2023

10. **Title**: Hidden Backdoors in Human-Centric Language Models (arXiv:2105.00164)
    - **Authors**: Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, Jialiang Lu
    - **Summary**: The authors create covert and natural triggers for textual backdoor attacks, termed "hidden backdoors," which can fool both modern language models and human inspection. The paper explores embedding triggers through homograph replacement and subtle differences between generated and natural text.
    - **Year**: 2021

**Key Challenges:**

1. **Data-Free Detection**: Developing methods to detect backdoors without access to training data or knowledge of attack patterns remains a significant challenge.

2. **Generalization Across Architectures**: Ensuring that detection methods are robust and effective across various neural network architectures and attack types is difficult.

3. **Permutation Equivariance**: Designing models that can learn canonical "fingerprints" of backdoor attacks independent of neuron ordering requires advanced architectural considerations.

4. **Scalability**: Creating large datasets of models containing both clean and backdoored instances to train detection models is resource-intensive.

5. **Interpretability**: Developing interpretable models that can provide insights into the presence and nature of backdoors in neural networks is an ongoing challenge. 