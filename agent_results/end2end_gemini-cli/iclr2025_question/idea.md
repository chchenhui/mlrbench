**Title:** Disentangled Uncertainty Estimation for Hallucination-Aware Generation

**Motivation:** Current methods for detecting hallucinations in LLMs often struggle to differentiate between factual inaccuracies (undesirable) and creative generation (desirable). This can lead to overly cautious models that lose their generative power. A method that can distinguish between a model's "lack of knowledge" and its "creative freedom" is needed to selectively mitigate harmful hallucinations while preserving creativity.

**Main Idea:** We propose a framework to disentangle epistemic (model) and aleatoric (data) uncertainty in LLMs. The core idea is to train an LLM with an auxiliary objective that encourages it to separate these uncertainties. We will construct a training dataset containing both closed-domain factual QA (low aleatoric uncertainty) and open-ended creative tasks (high aleatoric uncertainty). By explicitly training the model to recognize these contexts, we hypothesize it can learn to associate factual hallucinations with high epistemic uncertainty, while attributing creative variance to high aleatoric uncertainty. This disentangled signal can then be used to selectively flag potential factual errors for human review or trigger a safer response (e.g., "I don't know"), without penalizing creative outputs.