**Title:** Spurious-Correlation-Aware Adapters (SCA-Adapters): Efficiently Robustifying Foundation Models

**Motivation:** Fine-tuning large foundation models (LMMs, LLMs) for robustness against spurious correlations is computationally prohibitive. Existing methods often require full model retraining or large, group-annotated datasets, limiting their applicability. There is a critical need for a parameter-efficient method to instill robustness in pre-trained models without catastrophic forgetting or excessive computational cost, making robust AI more scalable and accessible.

**Main Idea:** We propose SCA-Adapters, a novel parameter-efficient fine-tuning (PEFT) technique. During fine-tuning, we insert two lightweight adapter modules: a "task" adapter and a "spurious" adapter. The spurious adapter is explicitly trained to solve the task using only automatically-identified shortcut features (e.g., backgrounds in images, stylistic tokens in text). Concurrently, the task adapter's gradients are projected to be orthogonal to the spurious adapter's gradients. This nullifies updates in the direction of the shortcut, forcing the model to learn core, invariant features for the main task. This approach efficiently robustifies the model by only tuning the small adapters, providing a practical solution for deploying reliable foundation models.