2025-07-31 12:25:52.753297: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-07-31 12:25:52.772085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1753964752.795149 3107807 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1753964752.801548 3107807 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1753964752.818730 3107807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753964752.818768 3107807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753964752.818771 3107807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1753964752.818774 3107807 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-07-31 12:25:52.823818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Starting the full experiment pipeline...
Downloading and preparing data...
Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]Generating train split:  21%|██        | 27000/127085 [00:00<00:00, 266180.45 examples/s]Generating train split:  48%|████▊     | 61000/127085 [00:00<00:00, 303208.51 examples/s]Generating train split:  77%|███████▋  | 98000/127085 [00:00<00:00, 325526.56 examples/s]Generating train split: 100%|██████████| 127085/127085 [00:00<00:00, 323569.27 examples/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Data preparation complete.
Using device: cuda
Starting Bi-Align model training...
Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]                                                                       0%|          | 0/79 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  1%|▏         | 1/79 [00:06<08:51,  6.81s/it]  3%|▎         | 2/79 [00:07<03:52,  3.02s/it]  4%|▍         | 3/79 [00:07<02:16,  1.80s/it]  5%|▌         | 4/79 [00:07<01:33,  1.24s/it]  6%|▋         | 5/79 [00:08<01:10,  1.05it/s]  8%|▊         | 6/79 [00:08<00:53,  1.35it/s]  9%|▉         | 7/79 [00:09<00:44,  1.63it/s] 10%|█         | 8/79 [00:09<00:38,  1.84it/s] 11%|█▏        | 9/79 [00:09<00:33,  2.06it/s] 13%|█▎        | 10/79 [00:10<00:35,  1.97it/s] 14%|█▍        | 11/79 [00:10<00:32,  2.12it/s] 15%|█▌        | 12/79 [00:11<00:30,  2.23it/s] 16%|█▋        | 13/79 [00:11<00:27,  2.38it/s] 18%|█▊        | 14/79 [00:11<00:26,  2.47it/s] 19%|█▉        | 15/79 [00:12<00:24,  2.66it/s] 20%|██        | 16/79 [00:12<00:23,  2.74it/s] 22%|██▏       | 17/79 [00:12<00:23,  2.69it/s] 23%|██▎       | 18/79 [00:13<00:22,  2.70it/s] 24%|██▍       | 19/79 [00:13<00:22,  2.67it/s] 25%|██▌       | 20/79 [00:14<00:22,  2.68it/s] 27%|██▋       | 21/79 [00:14<00:22,  2.61it/s] 28%|██▊       | 22/79 [00:14<00:21,  2.65it/s] 29%|██▉       | 23/79 [00:15<00:20,  2.73it/s] 30%|███       | 24/79 [00:15<00:20,  2.68it/s] 32%|███▏      | 25/79 [00:15<00:20,  2.63it/s] 33%|███▎      | 26/79 [00:16<00:19,  2.70it/s] 34%|███▍      | 27/79 [00:16<00:19,  2.68it/s] 35%|███▌      | 28/79 [00:17<00:19,  2.64it/s] 37%|███▋      | 29/79 [00:17<00:18,  2.69it/s] 38%|███▊      | 30/79 [00:17<00:17,  2.72it/s] 39%|███▉      | 31/79 [00:18<00:22,  2.17it/s] 41%|████      | 32/79 [00:18<00:19,  2.36it/s] 42%|████▏     | 33/79 [00:19<00:18,  2.50it/s] 43%|████▎     | 34/79 [00:19<00:17,  2.55it/s] 44%|████▍     | 35/79 [00:19<00:18,  2.36it/s] 46%|████▌     | 36/79 [00:20<00:17,  2.50it/s] 47%|████▋     | 37/79 [00:20<00:16,  2.54it/s] 48%|████▊     | 38/79 [00:21<00:16,  2.49it/s] 49%|████▉     | 39/79 [00:21<00:15,  2.63it/s] 51%|█████     | 40/79 [00:21<00:14,  2.66it/s] 52%|█████▏    | 41/79 [00:22<00:15,  2.51it/s] 53%|█████▎    | 42/79 [00:22<00:14,  2.62it/s] 54%|█████▍    | 43/79 [00:22<00:13,  2.71it/s] 56%|█████▌    | 44/79 [00:23<00:12,  2.73it/s] 57%|█████▋    | 45/79 [00:23<00:12,  2.75it/s] 58%|█████▊    | 46/79 [00:24<00:12,  2.72it/s] 59%|█████▉    | 47/79 [00:24<00:11,  2.78it/s] 61%|██████    | 48/79 [00:24<00:11,  2.78it/s] 62%|██████▏   | 49/79 [00:25<00:11,  2.63it/s] 63%|██████▎   | 50/79 [00:25<00:11,  2.63it/s] 65%|██████▍   | 51/79 [00:25<00:11,  2.53it/s] 66%|██████▌   | 52/79 [00:26<00:10,  2.62it/s] 67%|██████▋   | 53/79 [00:26<00:09,  2.64it/s] 68%|██████▊   | 54/79 [00:27<00:09,  2.69it/s] 70%|██████▉   | 55/79 [00:27<00:08,  2.69it/s] 71%|███████   | 56/79 [00:27<00:08,  2.70it/s] 72%|███████▏  | 57/79 [00:28<00:08,  2.73it/s] 73%|███████▎  | 58/79 [00:28<00:07,  2.82it/s] 75%|███████▍  | 59/79 [00:28<00:07,  2.79it/s] 76%|███████▌  | 60/79 [00:29<00:09,  2.05it/s] 77%|███████▋  | 61/79 [00:30<00:08,  2.12it/s] 78%|███████▊  | 62/79 [00:30<00:07,  2.29it/s] 80%|███████▉  | 63/79 [00:30<00:06,  2.35it/s] 81%|████████  | 64/79 [00:31<00:06,  2.47it/s] 82%|████████▏ | 65/79 [00:31<00:05,  2.58it/s] 84%|████████▎ | 66/79 [00:31<00:04,  2.68it/s] 85%|████████▍ | 67/79 [00:32<00:04,  2.76it/s] 86%|████████▌ | 68/79 [00:32<00:04,  2.75it/s] 87%|████████▋ | 69/79 [00:32<00:03,  2.59it/s] 89%|████████▊ | 70/79 [00:33<00:03,  2.55it/s] 90%|████████▉ | 71/79 [00:33<00:03,  2.62it/s] 91%|█████████ | 72/79 [00:34<00:02,  2.63it/s] 92%|█████████▏| 73/79 [00:34<00:02,  2.59it/s] 94%|█████████▎| 74/79 [00:34<00:01,  2.69it/s] 95%|█████████▍| 75/79 [00:35<00:01,  2.70it/s] 96%|█████████▌| 76/79 [00:35<00:01,  2.71it/s] 97%|█████████▋| 77/79 [00:35<00:00,  2.66it/s] 99%|█████████▊| 78/79 [00:36<00:00,  2.76it/s]100%|██████████| 79/79 [00:36<00:00,  2.93it/s]                                               100%|██████████| 79/79 [00:36<00:00,  2.93it/s]100%|██████████| 79/79 [00:36<00:00,  2.16it/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
{'train_runtime': 36.6299, 'train_samples_per_second': 273.001, 'train_steps_per_second': 2.157, 'train_loss': 0.1805631299562092, 'epoch': 1.0}
Epoch 1/4 complete.
  0%|          | 0/79 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  1%|▏         | 1/79 [00:01<01:39,  1.28s/it]  3%|▎         | 2/79 [00:01<00:58,  1.31it/s]  4%|▍         | 3/79 [00:02<00:44,  1.72it/s]  5%|▌         | 4/79 [00:02<00:37,  2.01it/s]  6%|▋         | 5/79 [00:02<00:33,  2.21it/s]  8%|▊         | 6/79 [00:03<00:33,  2.18it/s]  9%|▉         | 7/79 [00:03<00:30,  2.34it/s] 10%|█         | 8/79 [00:03<00:28,  2.45it/s] 11%|█▏        | 9/79 [00:04<00:27,  2.51it/s] 13%|█▎        | 10/79 [00:04<00:26,  2.57it/s] 14%|█▍        | 11/79 [00:05<00:26,  2.61it/s] 15%|█▌        | 12/79 [00:05<00:24,  2.74it/s] 16%|█▋        | 13/79 [00:05<00:23,  2.79it/s] 18%|█▊        | 14/79 [00:06<00:23,  2.74it/s] 19%|█▉        | 15/79 [00:06<00:23,  2.68it/s] 20%|██        | 16/79 [00:06<00:23,  2.73it/s] 22%|██▏       | 17/79 [00:07<00:24,  2.52it/s] 23%|██▎       | 18/79 [00:07<00:24,  2.47it/s] 24%|██▍       | 19/79 [00:08<00:25,  2.32it/s] 25%|██▌       | 20/79 [00:08<00:24,  2.40it/s] 27%|██▋       | 21/79 [00:08<00:22,  2.56it/s] 28%|██▊       | 22/79 [00:09<00:22,  2.56it/s] 29%|██▉       | 23/79 [00:09<00:21,  2.63it/s] 30%|███       | 24/79 [00:10<00:20,  2.69it/s] 32%|███▏      | 25/79 [00:10<00:19,  2.77it/s] 33%|███▎      | 26/79 [00:10<00:19,  2.79it/s] 34%|███▍      | 27/79 [00:11<00:18,  2.86it/s] 35%|███▌      | 28/79 [00:11<00:21,  2.33it/s] 37%|███▋      | 29/79 [00:12<00:20,  2.46it/s] 38%|███▊      | 30/79 [00:12<00:19,  2.51it/s] 39%|███▉      | 31/79 [00:12<00:18,  2.58it/s] 41%|████      | 32/79 [00:13<00:17,  2.61it/s] 42%|████▏     | 33/79 [00:13<00:18,  2.51it/s] 43%|████▎     | 34/79 [00:14<00:18,  2.39it/s] 44%|████▍     | 35/79 [00:14<00:17,  2.49it/s] 46%|████▌     | 36/79 [00:14<00:16,  2.55it/s] 47%|████▋     | 37/79 [00:15<00:16,  2.62it/s] 48%|████▊     | 38/79 [00:15<00:16,  2.53it/s] 49%|████▉     | 39/79 [00:15<00:15,  2.65it/s] 51%|█████     | 40/79 [00:16<00:14,  2.69it/s] 52%|█████▏    | 41/79 [00:16<00:14,  2.65it/s] 53%|█████▎    | 42/79 [00:17<00:14,  2.61it/s] 54%|█████▍    | 43/79 [00:17<00:13,  2.65it/s] 56%|█████▌    | 44/79 [00:17<00:13,  2.59it/s] 57%|█████▋    | 45/79 [00:18<00:12,  2.63it/s] 58%|█████▊    | 46/79 [00:18<00:12,  2.70it/s] 59%|█████▉    | 47/79 [00:18<00:11,  2.71it/s] 61%|██████    | 48/79 [00:19<00:11,  2.62it/s] 62%|██████▏   | 49/79 [00:19<00:11,  2.64it/s] 63%|██████▎   | 50/79 [00:20<00:10,  2.68it/s] 65%|██████▍   | 51/79 [00:20<00:10,  2.73it/s] 66%|██████▌   | 52/79 [00:20<00:09,  2.74it/s] 67%|██████▋   | 53/79 [00:21<00:09,  2.61it/s] 68%|██████▊   | 54/79 [00:21<00:11,  2.15it/s] 70%|██████▉   | 55/79 [00:22<00:10,  2.27it/s] 71%|███████   | 56/79 [00:22<00:09,  2.38it/s] 72%|███████▏  | 57/79 [00:23<00:09,  2.37it/s] 73%|███████▎  | 58/79 [00:23<00:08,  2.43it/s] 75%|███████▍  | 59/79 [00:23<00:08,  2.49it/s] 76%|███████▌  | 60/79 [00:24<00:07,  2.44it/s] 77%|███████▋  | 61/79 [00:24<00:07,  2.55it/s] 78%|███████▊  | 62/79 [00:24<00:06,  2.60it/s] 80%|███████▉  | 63/79 [00:25<00:06,  2.63it/s] 81%|████████  | 64/79 [00:25<00:05,  2.69it/s] 82%|████████▏ | 65/79 [00:26<00:05,  2.67it/s] 84%|████████▎ | 66/79 [00:26<00:04,  2.68it/s] 85%|████████▍ | 67/79 [00:26<00:04,  2.63it/s] 86%|████████▌ | 68/79 [00:27<00:04,  2.67it/s] 87%|████████▋ | 69/79 [00:27<00:03,  2.72it/s] 89%|████████▊ | 70/79 [00:27<00:03,  2.70it/s] 90%|████████▉ | 71/79 [00:28<00:03,  2.59it/s] 91%|█████████ | 72/79 [00:28<00:02,  2.65it/s] 92%|█████████▏| 73/79 [00:29<00:02,  2.72it/s] 94%|█████████▎| 74/79 [00:29<00:01,  2.69it/s] 95%|█████████▍| 75/79 [00:29<00:01,  2.73it/s] 96%|█████████▌| 76/79 [00:30<00:01,  2.19it/s] 97%|█████████▋| 77/79 [00:30<00:00,  2.32it/s] 99%|█████████▊| 78/79 [00:31<00:00,  2.57it/s]100%|██████████| 79/79 [00:31<00:00,  2.77it/s]                                               100%|██████████| 79/79 [00:31<00:00,  2.77it/s]100%|██████████| 79/79 [00:31<00:00,  2.51it/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
{'train_runtime': 31.4193, 'train_samples_per_second': 318.276, 'train_steps_per_second': 2.514, 'train_loss': 0.10874149467371687, 'epoch': 1.0}
Epoch 2/4 complete.
  0%|          | 0/79 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  1%|▏         | 1/79 [00:01<01:26,  1.10s/it]  3%|▎         | 2/79 [00:01<00:52,  1.48it/s]  4%|▍         | 3/79 [00:01<00:39,  1.91it/s]  5%|▌         | 4/79 [00:02<00:34,  2.20it/s]  6%|▋         | 5/79 [00:02<00:31,  2.35it/s]  8%|▊         | 6/79 [00:03<00:36,  1.98it/s]  9%|▉         | 7/79 [00:03<00:32,  2.22it/s] 10%|█         | 8/79 [00:03<00:30,  2.35it/s] 11%|█▏        | 9/79 [00:04<00:28,  2.44it/s] 13%|█▎        | 10/79 [00:04<00:27,  2.50it/s] 14%|█▍        | 11/79 [00:05<00:25,  2.63it/s] 15%|█▌        | 12/79 [00:05<00:25,  2.67it/s] 16%|█▋        | 13/79 [00:05<00:23,  2.76it/s] 18%|█▊        | 14/79 [00:06<00:23,  2.73it/s] 19%|█▉        | 15/79 [00:06<00:23,  2.67it/s] 20%|██        | 16/79 [00:06<00:23,  2.74it/s] 22%|██▏       | 17/79 [00:07<00:24,  2.56it/s] 23%|██▎       | 18/79 [00:07<00:24,  2.53it/s] 24%|██▍       | 19/79 [00:08<00:25,  2.39it/s] 25%|██▌       | 20/79 [00:08<00:24,  2.41it/s] 27%|██▋       | 21/79 [00:08<00:23,  2.50it/s] 28%|██▊       | 22/79 [00:09<00:23,  2.48it/s] 29%|██▉       | 23/79 [00:09<00:22,  2.53it/s] 30%|███       | 24/79 [00:10<00:21,  2.55it/s] 32%|███▏      | 25/79 [00:10<00:20,  2.67it/s] 33%|███▎      | 26/79 [00:10<00:19,  2.72it/s] 34%|███▍      | 27/79 [00:11<00:18,  2.76it/s] 35%|███▌      | 28/79 [00:11<00:22,  2.22it/s] 37%|███▋      | 29/79 [00:12<00:21,  2.38it/s] 38%|███▊      | 30/79 [00:12<00:19,  2.47it/s] 39%|███▉      | 31/79 [00:12<00:19,  2.53it/s] 41%|████      | 32/79 [00:13<00:18,  2.60it/s] 42%|████▏     | 33/79 [00:13<00:18,  2.54it/s] 43%|████▎     | 34/79 [00:14<00:19,  2.37it/s] 44%|████▍     | 35/79 [00:14<00:17,  2.49it/s] 46%|████▌     | 36/79 [00:14<00:16,  2.55it/s] 47%|████▋     | 37/79 [00:15<00:16,  2.62it/s] 48%|████▊     | 38/79 [00:15<00:16,  2.52it/s] 49%|████▉     | 39/79 [00:15<00:15,  2.64it/s] 51%|█████     | 40/79 [00:16<00:14,  2.68it/s] 52%|█████▏    | 41/79 [00:16<00:14,  2.67it/s] 53%|█████▎    | 42/79 [00:17<00:13,  2.79it/s] 54%|█████▍    | 43/79 [00:17<00:12,  2.88it/s] 56%|█████▌    | 44/79 [00:17<00:12,  2.82it/s] 57%|█████▋    | 45/79 [00:18<00:11,  2.89it/s] 58%|█████▊    | 46/79 [00:18<00:11,  2.92it/s] 59%|█████▉    | 47/79 [00:18<00:10,  2.96it/s] 61%|██████    | 48/79 [00:19<00:10,  2.89it/s] 62%|██████▏   | 49/79 [00:19<00:10,  2.89it/s] 63%|██████▎   | 50/79 [00:19<00:09,  2.90it/s] 65%|██████▍   | 51/79 [00:20<00:09,  2.91it/s] 66%|██████▌   | 52/79 [00:20<00:09,  2.84it/s] 67%|██████▋   | 53/79 [00:20<00:09,  2.69it/s] 68%|██████▊   | 54/79 [00:21<00:10,  2.29it/s] 70%|██████▉   | 55/79 [00:21<00:09,  2.41it/s] 71%|███████   | 56/79 [00:22<00:09,  2.52it/s] 72%|███████▏  | 57/79 [00:22<00:08,  2.50it/s] 73%|███████▎  | 58/79 [00:23<00:08,  2.49it/s] 75%|███████▍  | 59/79 [00:23<00:07,  2.63it/s] 76%|███████▌  | 60/79 [00:23<00:07,  2.61it/s] 77%|███████▋  | 61/79 [00:24<00:06,  2.74it/s] 78%|███████▊  | 62/79 [00:24<00:06,  2.69it/s] 80%|███████▉  | 63/79 [00:24<00:05,  2.75it/s] 81%|████████  | 64/79 [00:25<00:05,  2.82it/s] 82%|████████▏ | 65/79 [00:25<00:04,  2.80it/s] 84%|████████▎ | 66/79 [00:25<00:04,  2.79it/s] 85%|████████▍ | 67/79 [00:26<00:04,  2.67it/s] 86%|████████▌ | 68/79 [00:26<00:04,  2.71it/s] 87%|████████▋ | 69/79 [00:26<00:03,  2.80it/s] 89%|████████▊ | 70/79 [00:27<00:03,  2.76it/s] 90%|████████▉ | 71/79 [00:27<00:03,  2.66it/s] 91%|█████████ | 72/79 [00:28<00:02,  2.70it/s] 92%|█████████▏| 73/79 [00:28<00:02,  2.82it/s] 94%|█████████▎| 74/79 [00:28<00:01,  2.78it/s] 95%|█████████▍| 75/79 [00:29<00:01,  2.81it/s] 96%|█████████▌| 76/79 [00:29<00:01,  2.26it/s] 97%|█████████▋| 77/79 [00:30<00:00,  2.36it/s] 99%|█████████▊| 78/79 [00:30<00:00,  2.57it/s]100%|██████████| 79/79 [00:30<00:00,  2.80it/s]                                               100%|██████████| 79/79 [00:30<00:00,  2.80it/s]100%|██████████| 79/79 [00:30<00:00,  2.57it/s]
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
{'train_runtime': 30.7456, 'train_samples_per_second': 325.25, 'train_steps_per_second': 2.569, 'train_loss': 0.06653442262094232, 'epoch': 1.0}
Epoch 3/4 complete.
  0%|          | 0/79 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
  1%|▏         | 1/79 [00:01<01:24,  1.09s/it]  3%|▎         | 2/79 [00:01<00:51,  1.50it/s]  4%|▍         | 3/79 [00:01<00:40,  1.89it/s]  5%|▌         | 4/79 [00:02<00:34,  2.18it/s]  6%|▋         | 5/79 [00:02<00:31,  2.34it/s]  8%|▊         | 6/79 [00:03<00:32,  2.25it/s]  9%|▉         | 7/79 [00:03<00:29,  2.44it/s] 10%|█         | 8/79 [00:03<00:27,  2.54it/s] 11%|█▏        | 9/79 [00:04<00:26,  2.60it/s] 13%|█▎        | 10/79 [00:04<00:26,  2.64it/s] 14%|█▍        | 11/79 [00:04<00:25,  2.69it/s] 15%|█▌        | 12/79 [00:05<00:24,  2.73it/s] 16%|█▋        | 13/79 [00:05<00:23,  2.80it/s] 18%|█▊        | 14/79 [00:05<00:23,  2.76it/s] 19%|█▉        | 15/79 [00:06<00:23,  2.72it/s] 20%|██        | 16/79 [00:06<00:23,  2.71it/s] 22%|██▏       | 17/79 [00:07<00:23,  2.59it/s] 23%|██▎       | 18/79 [00:07<00:23,  2.55it/s] 24%|██▍       | 19/79 [00:07<00:24,  2.43it/s] 25%|██▌       | 20/79 [00:08<00:24,  2.45it/s] 27%|██▋       | 21/79 [00:08<00:22,  2.60it/s] 28%|██▊       | 22/79 [00:09<00:21,  2.60it/s] 29%|██▉       | 23/79 [00:09<00:21,  2.64it/s] 30%|███       | 24/79 [00:09<00:20,  2.65it/s] 32%|███▏      | 25/79 [00:10<00:20,  2.68it/s] 33%|███▎      | 26/79 [00:10<00:19,  2.72it/s] 34%|███▍      | 27/79 [00:10<00:18,  2.81it/s] 35%|███▌      | 28/79 [00:11<00:21,  2.34it/s] 37%|███▋      | 29/79 [00:11<00:20,  2.44it/s] 38%|███▊      | 30/79 [00:12<00:19,  2.49it/s] 39%|███▉      | 31/79 [00:12<00:18,  2.58it/s] 41%|████      | 32/79 [00:12<00:18,  2.59it/s] 42%|████▏     | 33/79 [00:13<00:18,  2.53it/s] 43%|████▎     | 34/79 [00:13<00:18,  2.42it/s] 44%|████▍     | 35/79 [00:14<00:17,  2.53it/s] 46%|████▌     | 36/79 [00:14<00:16,  2.61it/s] 47%|████▋     | 37/79 [00:14<00:15,  2.71it/s] 48%|████▊     | 38/79 [00:15<00:15,  2.61it/s] 49%|████▉     | 39/79 [00:15<00:14,  2.70it/s] 51%|█████     | 40/79 [00:15<00:14,  2.69it/s] 52%|█████▏    | 41/79 [00:16<00:14,  2.67it/s] 53%|█████▎    | 42/79 [00:16<00:13,  2.64it/s] 54%|█████▍    | 43/79 [00:17<00:13,  2.67it/s] 56%|█████▌    | 44/79 [00:17<00:13,  2.65it/s] 57%|█████▋    | 45/79 [00:17<00:12,  2.73it/s] 58%|█████▊    | 46/79 [00:18<00:12,  2.72it/s] 59%|█████▉    | 47/79 [00:18<00:11,  2.69it/s] 61%|██████    | 48/79 [00:18<00:11,  2.65it/s] 62%|██████▏   | 49/79 [00:19<00:11,  2.68it/s] 63%|██████▎   | 50/79 [00:19<00:10,  2.77it/s] 65%|██████▍   | 51/79 [00:20<00:10,  2.72it/s] 66%|██████▌   | 52/79 [00:20<00:10,  2.70it/s] 67%|██████▋   | 53/79 [00:20<00:09,  2.61it/s] 68%|██████▊   | 54/79 [00:21<00:11,  2.21it/s] 70%|██████▉   | 55/79 [00:21<00:10,  2.28it/s] 71%|███████   | 56/79 [00:22<00:09,  2.40it/s] 72%|███████▏  | 57/79 [00:22<00:09,  2.42it/s] 73%|███████▎  | 58/79 [00:22<00:08,  2.47it/s] 75%|███████▍  | 59/79 [00:23<00:07,  2.55it/s] 76%|███████▌  | 60/79 [00:23<00:07,  2.50it/s] 77%|███████▋  | 61/79 [00:24<00:06,  2.60it/s] 78%|███████▊  | 62/79 [00:24<00:06,  2.61it/s] 80%|███████▉  | 63/79 [00:24<00:06,  2.64it/s] 81%|████████  | 64/79 [00:25<00:05,  2.75it/s] 82%|████████▏ | 65/79 [00:25<00:05,  2.69it/s] 84%|████████▎ | 66/79 [00:25<00:04,  2.71it/s] 85%|████████▍ | 67/79 [00:26<00:04,  2.65it/s] 86%|████████▌ | 68/79 [00:26<00:04,  2.65it/s] 87%|████████▋ | 69/79 [00:27<00:03,  2.73it/s] 89%|████████▊ | 70/79 [00:27<00:03,  2.75it/s] 90%|████████▉ | 71/79 [00:27<00:03,  2.62it/s] 91%|█████████ | 72/79 [00:28<00:02,  2.66it/s] 92%|█████████▏| 73/79 [00:28<00:02,  2.75it/s] 94%|█████████▎| 74/79 [00:28<00:01,  2.73it/s] 95%|█████████▍| 75/79 [00:29<00:01,  2.78it/s] 96%|█████████▌| 76/79 [00:29<00:01,  2.31it/s] 97%|█████████▋| 77/79 [00:30<00:00,  2.41it/s] 99%|█████████▊| 78/79 [00:30<00:00,  2.61it/s]100%|██████████| 79/79 [00:30<00:00,  2.83it/s]                                               100%|██████████| 79/79 [00:30<00:00,  2.83it/s]100%|██████████| 79/79 [00:30<00:00,  2.56it/s]
{'train_runtime': 30.8376, 'train_samples_per_second': 324.28, 'train_steps_per_second': 2.562, 'train_loss': 0.04029517837717563, 'epoch': 1.0}
Epoch 4/4 complete.
Training complete.
Model saved to /home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_bi_align/gemini/bialign_model
Loss curve plot saved to /home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_bi_align/gemini/loss_curve.png
Evaluating Bi-Align (Trained) on clean data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  12%|█▎        | 4/32 [00:00<00:00, 35.91it/s]Batches:  31%|███▏      | 10/32 [00:00<00:00, 46.71it/s]Batches:  50%|█████     | 16/32 [00:00<00:00, 50.77it/s]Batches:  69%|██████▉   | 22/32 [00:00<00:00, 54.18it/s]Batches:  91%|█████████ | 29/32 [00:00<00:00, 58.62it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 55.23it/s]
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  12%|█▎        | 4/32 [00:00<00:00, 37.57it/s]Batches:  31%|███▏      | 10/32 [00:00<00:00, 46.32it/s]Batches:  50%|█████     | 16/32 [00:00<00:00, 51.41it/s]Batches:  69%|██████▉   | 22/32 [00:00<00:00, 54.59it/s]Batches:  91%|█████████ | 29/32 [00:00<00:00, 58.64it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 55.66it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
Evaluating Bi-Align (Trained) on noisy data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Batches:  12%|█▎        | 4/32 [00:00<00:00, 37.18it/s]Batches:  31%|███▏      | 10/32 [00:00<00:00, 47.22it/s]Batches:  50%|█████     | 16/32 [00:00<00:00, 51.17it/s]Batches:  69%|██████▉   | 22/32 [00:00<00:00, 54.33it/s]Batches:  91%|█████████ | 29/32 [00:00<00:00, 58.07it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 55.53it/s]
Batches:   0%|          | 0/29 [00:00<?, ?it/s]Batches:  14%|█▍        | 4/29 [00:00<00:00, 37.17it/s]Batches:  34%|███▍      | 10/29 [00:00<00:00, 47.34it/s]Batches:  55%|█████▌    | 16/29 [00:00<00:00, 51.88it/s]Batches:  76%|███████▌  | 22/29 [00:00<00:00, 54.77it/s]Batches: 100%|██████████| 29/29 [00:00<00:00, 59.42it/s]Batches: 100%|██████████| 29/29 [00:00<00:00, 55.05it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
Evaluating Base Multilingual (Untrained) on clean data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Batches:  12%|█▎        | 4/32 [00:00<00:00, 35.12it/s]Batches:  28%|██▊       | 9/32 [00:00<00:00, 42.44it/s]Batches:  47%|████▋     | 15/32 [00:00<00:00, 48.09it/s]Batches:  66%|██████▌   | 21/32 [00:00<00:00, 52.49it/s]Batches:  88%|████████▊ | 28/32 [00:00<00:00, 57.62it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 54.24it/s]
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  16%|█▌        | 5/32 [00:00<00:00, 41.86it/s]Batches:  34%|███▍      | 11/32 [00:00<00:00, 49.06it/s]Batches:  53%|█████▎    | 17/32 [00:00<00:00, 52.35it/s]Batches:  75%|███████▌  | 24/32 [00:00<00:00, 55.95it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 61.18it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 56.77it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
Evaluating Base Multilingual (Untrained) on noisy data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)
Batches:  16%|█▌        | 5/32 [00:00<00:00, 43.15it/s]Batches:  34%|███▍      | 11/32 [00:00<00:00, 49.87it/s]Batches:  53%|█████▎    | 17/32 [00:00<00:00, 53.74it/s]Batches:  75%|███████▌  | 24/32 [00:00<00:00, 58.15it/s]Batches:  97%|█████████▋| 31/32 [00:00<00:00, 61.74it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 58.03it/s]
Batches:   0%|          | 0/29 [00:00<?, ?it/s]Batches:  17%|█▋        | 5/29 [00:00<00:00, 45.37it/s]Batches:  38%|███▊      | 11/29 [00:00<00:00, 50.24it/s]Batches:  59%|█████▊    | 17/29 [00:00<00:00, 53.23it/s]Batches:  83%|████████▎ | 24/29 [00:00<00:00, 56.47it/s]Batches: 100%|██████████| 29/29 [00:00<00:00, 56.56it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
Evaluating DistilUSE on clean data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  12%|█▎        | 4/32 [00:00<00:00, 39.48it/s]Batches:  34%|███▍      | 11/32 [00:00<00:00, 52.34it/s]Batches:  56%|█████▋    | 18/32 [00:00<00:00, 59.93it/s]Batches:  84%|████████▍ | 27/32 [00:00<00:00, 69.04it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 65.69it/s]
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  12%|█▎        | 4/32 [00:00<00:00, 39.55it/s]Batches:  31%|███▏      | 10/32 [00:00<00:00, 51.16it/s]Batches:  53%|█████▎    | 17/32 [00:00<00:00, 59.38it/s]Batches:  78%|███████▊  | 25/32 [00:00<00:00, 66.87it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 65.17it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
Evaluating DistilUSE on noisy data...
Batches:   0%|          | 0/32 [00:00<?, ?it/s]Batches:  16%|█▌        | 5/32 [00:00<00:00, 43.36it/s]Batches:  34%|███▍      | 11/32 [00:00<00:00, 51.45it/s]Batches:  59%|█████▉    | 19/32 [00:00<00:00, 61.65it/s]Batches:  84%|████████▍ | 27/32 [00:00<00:00, 68.44it/s]Batches: 100%|██████████| 32/32 [00:00<00:00, 66.01it/s]
Batches:   0%|          | 0/29 [00:00<?, ?it/s]Batches:  17%|█▋        | 5/29 [00:00<00:00, 44.05it/s]Batches:  38%|███▊      | 11/29 [00:00<00:00, 51.53it/s]Batches:  66%|██████▌   | 19/29 [00:00<00:00, 63.30it/s]Batches:  93%|█████████▎| 27/29 [00:00<00:00, 69.39it/s]Batches: 100%|██████████| 29/29 [00:00<00:00, 65.56it/s]
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/metrics/_classification.py:99: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  type_pred = type_of_target(y_pred, input_name="y_pred")
/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/sklearn/utils/multiclass.py:79: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.
  ys_types = set(type_of_target(x) for x in ys)

Full experiment results:
                           Model  ...  F1 Score (Noisy)
0             Bi-Align (Trained)  ...          0.004012
1  Base Multilingual (Untrained)  ...          0.004012
2                      DistilUSE  ...          0.004012

[3 rows x 5 columns]
Performance comparison plot saved to /home/chenhui/mlr-bench/pipeline_gemini-cli/iclr2025_bi_align/gemini/performance_comparison.png

Experiment pipeline finished successfully!
