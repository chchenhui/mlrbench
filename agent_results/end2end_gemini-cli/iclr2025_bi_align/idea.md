**Title:** Proactive Alignment Auditing via Interactive Counterfactual Probing

**Motivation:** Current alignment methods like RLHF are reactive, training models on past human feedback. This fails to proactively uncover a model's hidden biases or misaligned reasoning, which may surface in novel situations post-deployment. To achieve robust bidirectional alignment, we must empower humans to actively explore an AI's decision boundaries and potential failure modes, building a more accurate mental model of the system they are interacting with.

**Main Idea:** We propose an interactive framework where humans can audit a model's alignment by generating and testing counterfactual scenarios. For instance, a user could pose a query and then use an interface to slightly alter a semantically meaningful aspect of the prompt (e.g., changing the subject's demographic attributes). The system would then visualize the semantic difference between the original and counterfactual responses, revealing the modelâ€™s sensitivities and implicit assumptions. This process directly facilitates bidirectional alignment: (1) it empowers the human to better understand the AI's behavior (aligning human to AI), and (2) the generated counterfactuals provide targeted, high-quality data for rectifying uncovered alignment failures (aligning AI to human).