**Title:** Generative Data Symbiosis: Mitigating Model Collapse through Co-Evolving Foundation Models

**Motivation:** Training foundation models on purely synthetic data often leads to "model collapse," where successive generations of data lose diversity and fidelity, degrading model performance. This critical issue limits the potential of synthetic data for scaling FMs while preserving privacy and controlling content. Current methods lack strong incentives for generators to create truly novel and useful information beyond their initial training distribution.

**Main Idea:** We propose a symbiotic training framework involving two models: a "Generator" and a "Student." The Generator's objective is not self-imitation but to produce synthetic data that maximizes the performance of the Student model on a diverse, held-out suite of evaluation tasks. In turn, the Student provides rich feedback (e.g., uncertainty scores, gradients on difficult examples) to guide the Generator toward producing more challenging and informative data. This co-evolutionary loop forces the Generator to explore novel data modes and escape the cycle of self-imitation, actively countering model collapse. The expected outcome is a robust methodology for generating high-quality, diverse synthetic data at scale, pushing the boundaries of what purely model-generated data can achieve.