**Title:** Execution-Trace Alignment: Fine-tuning Code LLMs with Step-wise Causal Program Feedback

**Motivation:** Current alignment techniques for code generation models often rely on binary execution feedback (pass/fail). This sparse signal struggles to inform the model *why* its code failed, hindering its ability to learn complex debugging and reasoning skills. To build more robust models, we need a richer, more explanatory feedback mechanism that mirrors how human developers debug.

**Main Idea:** We propose a novel fine-tuning paradigm using detailed execution traces as feedback. For each generated program, we automatically instrument and collect a trace containing the sequence of executed statements, intermediate variable states, and the precise location and type of any error. This structured, causal data is then used to fine-tune the Large Language Model (LLM). This can be achieved by training a reward model that scores code based on trace quality or by directly using the traces in a preference-based method like DPO, where a correct trace is preferred over a failed one. We expect models trained with Execution-Trace Alignment to demonstrate significantly improved multi-step reasoning, self-correction, and performance on complex programming benchmarks.