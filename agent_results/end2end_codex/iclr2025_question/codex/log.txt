Running baseline...
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
{'loss': 0.6867, 'grad_norm': 1.7678066492080688, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}
{'eval_loss': 0.6622834801673889, 'eval_accuracy': 0.56, 'eval_f1': 0.7142857142857143, 'eval_runtime': 0.0544, 'eval_samples_per_second': 1839.276, 'eval_steps_per_second': 18.393, 'epoch': 1.0}
{'loss': 0.6358, 'grad_norm': 3.368847608566284, 'learning_rate': 2.0833333333333336e-05, 'epoch': 2.0}
{'eval_loss': 0.6400274634361267, 'eval_accuracy': 0.71, 'eval_f1': 0.7642276422764228, 'eval_runtime': 0.0542, 'eval_samples_per_second': 1844.518, 'eval_steps_per_second': 18.445, 'epoch': 2.0}
{'loss': 0.5512, 'grad_norm': 3.0276641845703125, 'learning_rate': 4.166666666666667e-06, 'epoch': 3.0}
{'eval_loss': 0.6273183226585388, 'eval_accuracy': 0.74, 'eval_f1': 0.7547169811320755, 'eval_runtime': 0.0524, 'eval_samples_per_second': 1908.072, 'eval_steps_per_second': 19.081, 'epoch': 3.0}
{'train_runtime': 3.3216, 'train_samples_per_second': 180.638, 'train_steps_per_second': 3.613, 'train_loss': 0.6245636343955994, 'epoch': 3.0}
{'eval_loss': 0.6273183226585388, 'eval_accuracy': 0.74, 'eval_f1': 0.7547169811320755, 'eval_runtime': 0.0565, 'eval_samples_per_second': 1768.443, 'eval_steps_per_second': 17.684, 'epoch': 3.0}
Running head_only...
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/home/chenhui/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
{'loss': 0.6901, 'grad_norm': 0.670832097530365, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.0}
{'eval_loss': 0.6937609910964966, 'eval_accuracy': 0.53, 'eval_f1': 0.6713286713286714, 'eval_runtime': 0.0556, 'eval_samples_per_second': 1799.165, 'eval_steps_per_second': 17.992, 'epoch': 1.0}
{'loss': 0.6966, 'grad_norm': 1.8623908758163452, 'learning_rate': 2.0833333333333336e-05, 'epoch': 2.0}
{'eval_loss': 0.6933422088623047, 'eval_accuracy': 0.51, 'eval_f1': 0.6620689655172414, 'eval_runtime': 0.0535, 'eval_samples_per_second': 1868.77, 'eval_steps_per_second': 18.688, 'epoch': 2.0}
{'loss': 0.6988, 'grad_norm': 1.0848841667175293, 'learning_rate': 4.166666666666667e-06, 'epoch': 3.0}
{'eval_loss': 0.6932817101478577, 'eval_accuracy': 0.51, 'eval_f1': 0.6620689655172414, 'eval_runtime': 0.0531, 'eval_samples_per_second': 1881.706, 'eval_steps_per_second': 18.817, 'epoch': 3.0}
{'train_runtime': 3.1058, 'train_samples_per_second': 193.185, 'train_steps_per_second': 3.864, 'train_loss': 0.6951373815536499, 'epoch': 3.0}
{'eval_loss': 0.6932817101478577, 'eval_accuracy': 0.51, 'eval_f1': 0.6620689655172414, 'eval_runtime': 0.0551, 'eval_samples_per_second': 1816.266, 'eval_steps_per_second': 18.163, 'epoch': 3.0}
Plotting results...
Plots saved to /home/chenhui/mlr-bench/pipeline_codex/iclr2025_question/codex/results/loss_curve.png and /home/chenhui/mlr-bench/pipeline_codex/iclr2025_question/codex/results/metrics.png
Results table saved to /home/chenhui/mlr-bench/pipeline_codex/iclr2025_question/codex/results/results.csv
