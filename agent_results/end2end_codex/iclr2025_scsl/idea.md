Title: AutoSpurDetect: Automated Benchmark and Robustification for Unknown Spurious Correlations in Multimodal Models  

Motivation: We face unknown spurious correlations dominating model predictions, lacking scalable detection and evaluation. Existing benchmarks rely on manual group labels, missing hidden shortcuts. We propose an automated pipeline to detect, benchmark, and mitigate unknown spurious features in LLMs and LMMs, boosting real-world reliability.  

Main Idea: We design a self-supervised framework that extracts latent feature clusters from multimodal data via pretrained encoders, then generates counterfactual samples using generative models (e.g., Stable Diffusion, GPT-4) to perturb individual clusters. We quantify model sensitivity to each feature cluster by measuring prediction shifts across these perturbations, flagging high-sensitivity clusters as spurious. These feature-specific counterfactual sets constitute a new evaluation benchmark spanning images, text, and audio. For robustification, we introduce adversarial consistency training: augmenting the original dataset with spurious-perturbed samples and minimizing divergence between predictions on clean and perturbed inputs. We expect this pipeline to uncover hidden shortcuts, provide comprehensive robustness metrics across modalities, and yield models with improved invariance to spurious correlations.