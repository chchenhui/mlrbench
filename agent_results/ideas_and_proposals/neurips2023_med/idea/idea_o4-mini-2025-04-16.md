Title: Federated Self-Supervised Pretraining for Robust Rare Disease Detection

Motivation:  
Medical imaging datasets are siloed by institution and subject to strict privacy regulations, limiting the amount of labeled data availableâ€”especially for rare pathologies. Centralized training is often infeasible, and supervised models suffer from poor generalization across scanners and patient populations. A privacy-preserving, self-supervised approach can leverage large pools of unlabeled data to learn robust representations that improve downstream rare disease detection.

Main Idea:  
We propose a federated self-supervised learning (FedSSL) pipeline that conducts contrastive pretraining across multiple hospitals without sharing raw images. Each site trains a local encoder using instance and context contrastive losses on its unlabeled scans (MRI, CT, X-ray). Periodic model aggregation via secure federated averaging produces a global encoder. To mitigate domain shifts, we introduce domain-specific batch normalization layers and a global prototypical alignment loss that encourages consistent feature clustering across sites. After convergence, the global encoder is fine-tuned on a small labeled set of rare disease cases for classification or segmentation. We anticipate significant gains in sensitivity and generalization on rare lesion tasks, enabling earlier diagnosis and wider clinical deployment under strict privacy constraints.