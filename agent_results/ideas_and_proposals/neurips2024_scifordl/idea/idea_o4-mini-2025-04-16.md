Title: Controlled Prompt Perturbations to Decipher In-Context Learning Mechanisms

Motivation: The remarkable ability of large language models to solve novel tasks through in-context learning (ICL) remains poorly understood. Disentangling whether ICL relies on memorization, token-similarity retrieval, or implicit algorithmic execution is key to improving model interpretability, robustness, and prompt design.

Main Idea: We propose a suite of controlled perturbation experiments on synthetic and real-world tasks. For each task, we systematically vary demonstration order, introduce content noise, apply syntactic transformations, and shift domains within prompts. We then measure performance degradation alongside changes in cross-attention patterns and representation similarity between queries and exemplars. By correlating these metrics across model scales and architectures, we aim to distinguish retrieval-based ICL (nearest-neighbor matching) from true pattern abstraction or algorithmic inference. Expected outcomes include empirical sensitivity profiles of ICL to different perturbations, guiding theoretical models of few-shot learning and offering practical recommendations for robust prompt engineering.