**Title:** Empirically Testing Algorithmic Hypotheses for Transformer In-Context Learning

**Motivation:** In-context learning (ICL) allows transformers to adapt to new tasks from prompt examples without weight updates, yet its underlying mechanisms remain elusive. Existing theories propose transformers might simulate simple learning algorithms (e.g., gradient descent, Bayesian inference) using their internal computations. This research aims to empirically validate or falsify these hypotheses using controlled experiments, directly addressing the workshop's call for hypothesis-driven empirical analysis.

**Main Idea:** We propose testing specific algorithmic hypotheses for ICL. We will design synthetic tasks (e.g., linear regression, simple classification) where the optimal learning strategy based on in-context examples is known. Pre-trained transformers will be prompted with varying numbers and types of examples. We will meticulously compare the transformer's output function (mapping query inputs to outputs given the context) against the functions learned by explicit algorithms (e.g., ridge regression, gradient descent) trained *only* on the in-context examples. By systematically varying tasks and context, we aim to identify conditions under which transformer ICL mimics specific algorithms, providing concrete evidence for or against theoretical claims about the mechanisms driving this phenomenon.