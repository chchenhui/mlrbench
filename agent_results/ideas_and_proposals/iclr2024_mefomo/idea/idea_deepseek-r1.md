**Title:** Scaling Laws and Phase Transitions: Predicting Emergent Capabilities in Foundation Models  

**Motivation:** Emergent capabilities like in-context learning and grokking arise unpredictably as models scale, posing challenges for efficient training and safety. Understanding the mechanisms behind these phenomena is critical to avoid reliance on brute-force scaling and enable targeted development of smaller, capable models.  

**Main Idea:** This research proposes a dual empirical-theoretical framework to characterize how scaling triggers emergent capabilities. First, systematically train families of FMs (varying model size, data diversity, and training steps) while tracking metrics tied to target capabilities (e.g., in-context learning accuracy). Second, model these dynamics using phase transition theory from statistical physics, identifying critical scaling thresholds and order parameters that signal imminent capability emergence. Experiments will test hypotheses about data composition’s role (e.g., “trigger” task subsets) and architectural inductive biases. Outcomes include predictive scaling laws for specific capabilities and a taxonomy of emergence types (sudden vs. gradual). The impact is twofold: enabling resource-efficient model design and providing tools to anticipate unsafe emergent behaviors pre-deployment.