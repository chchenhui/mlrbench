# **HyperPrompt: Dynamic Hyperparameter Adaptation in RL via LLM-Based Meta-Learning**

## **Introduction**

Reinforcement learning (RL) has demonstrated remarkable capabilities in complex domains such as games, robotics, and logistics. However, its success often hinges on labor-intensive hyperparameter tuning, where parameters like learning rates, discount factors, and exploration strategies must be manually adjusted over the course of training. Recent studies confirm that hyperparameter landscapes in RL evolve dynamically, with configurations yielding peak performance at one training phase often leading to suboptimal or unstable outcomes later. This brittleness, compounded by RL's sensitivity to design choices, severely limits its broad adoption and real-world applicability. Current AutoML solutions, such as OptFormer, offer static hyperparameter optimization via offline training but fall short in adapting to runtime shifts in environment dynamics or agent behavior.

The advent of large language models (LLMs) and their ability to perform in-context learning present a transformative opportunity. LLMs excel at parsing sequential data into high-dimensional representations, enabling dynamic reasoning over time-dependent contexts. By framing hyperparameter adaptation as a meta-learning task—where the LLM predicts optimal settings conditioned on real-time observations—the proposed **HyperPrompt** framework shifts from traditional, reactive tuning to proactive, data-driven adaptation. This is not merely an extension of AutoML but a novel integration of LLMs' sequence modeling capabilities with the principles of meta-RL, treating hyperparameters as the "policy" of a meta-control loop.

The core research objective of HyperPrompt is threefold:  
1. **Dynamic Adaptation**: Replace fixed hyperparameter schedules with context-aware updates generated by an LLM based on current agent-environment interactions.  
2. **Sample Efficiency**: Reduce computational overhead by leveraging pretrained LLMs, requiring only fine-tuning on curated RL datasets rather than extensive retraining.  
3. **Generalization**: Develop a method robust across diverse RL algorithms (e.g., policy gradient, Q-learning) and environments (from grid worlds to 3D simulations), addressing the context-dependency of existing HPO techniques.  

This research directly tackles four critical challenges outlined in prior work:  
- **Challenge 1: Dynamic Hyperparameter Landscapes**  
  AutoRL studies (Mohan et al., 2023) emphasize that optimal hyperparameters drift during training. HyperPrompt addresses this by encoding environmental feedback into LLM prompts every $ k $ episodes, triggering hyperparameter updates.  

- **Challenge 2: Computational Overhead**  
  Traditional HPO methods like Bayesian optimization (Eimer et al., 2023) demand prohibitive compute for iterative evaluations. HyperPrompt utilizes pretrained LLMs (e.g., Llama-3) finetuned on ARLBench datasets, minimizing training costs.  

- **Challenge 3: Generalization**  
  Most optimization tools (e.g., OptFormer) struggle to transfer across domains due to overfitting. HyperPrompt's use of procedurally generated environments (e.g., Procgen, NetHack) during training ensures broad task diversity, enabling robustness.  

- **Challenge 4: LLM Integration in RL**  
  Prior work (ReMA, Wang et al., 2025) explores LLM meta-reasoning via multi-agent scaffolds. HyperPrompt instead uses LLMs as direct meta-controllers for hyperparameters, avoiding hierarchical complexity while ensuring scalability.  

The significance of HyperPrompt transcends technical innovation: it democratizes RL by enabling non-experts to deploy agents in novel, dynamic environments without manual intervention. This aligns with the workshop's mission of cross-community collaboration, merging AutoML's empirical rigor with LLMs' contextual adaptability.

## **Methodology**

### **Data Collection and Benchmarking**
HyperPrompt's foundation lies in a diverse, dynamic dataset spanning multiple RL algorithms and environments. We collect trajectory data from agents trained using PPO, DQN, and A3C on procedurally generated benchmarks (Procgen, NetHack) and MuJoCo control tasks. For each configuration, we log:  
- **Trajectory Snippets**: Sequences of states, actions, and rewards over non-overlapping $ k $-episode windows  
- **Hyperparameter Histories**: Learning rates ($\alpha$), entropy coefficients ($\eta$), discount factors ($\gamma$), and exploration parameters (e.g., $\epsilon$ in epsilon-greedy) at each window  
- **Performance Metrics**: Return ($G_t = \sum_{t'=t}^\infty \gamma^{t'-t} r_{t'}$), policy divergence ($D_{KL}[\pi_{\theta_t} || \pi_{\theta_{t+1}}]$), and training stability  

To ensure generalization, data covers both continuous and discrete action spaces, varying observation complexities (e.g., pixel vs. state observations), and different reward sparsity levels.

### **Prompt Engineering and LLM Architecture**
Trajectories, hyperparameters, and metrics are encoded as structured textual prompts using a template format:  
```  
[ENVIRONMENT]: <Name>  
[NUM EPISODES]: <Integer>  
[LATEST PERFORMANCE]: <Float>  
[LATEST HPARAMS]: {"lr": <Float>, "gamma": <Float>, "entropy": <Float>, ...}  
[RECENT TRAJECTORIES]: <Sequence of tokenized trajectories>  
[NEW HPARAMS]: {"lr": <Float>, "gamma": <Float>, "entropy": <Float>, ...}  
```  
Trajectories are compressed into JSON-formatted state-action-reward transitions, while performance metrics are summarized using statistical features (e.g., mean reward, variance). The LLM (e.g., Llama-3-8B) is fine-tuned with causal language modeling to predict the `[NEW HPARAMS]` field given context from earlier windows, formalized as:  
$$ \mathcal{L}_{\text{prompt}} = -\log p(\text{hparams}_{t+1} \mid \text{prompt}_t; \phi) $$  
where $ \phi $ represents LLM parameters. Special tokens (e.g., `<HPARAM_BEGIN>`, `<METRIC_END>`) are inserted to enhance attention mechanism alignment.

### **Meta-Training Framework**
To bridge meta-learning and LLMs, we treat the hyperparameter adjustment problem as a partially observable Markov decision process (POMDP) where the LLM infers environment states from trajectory snippets. Training proceeds as follows:  
1. **Meta-State Construction**: Aggregate prompt components ($s_t = \{\text{hparams}_{\leq t}, \text{metrics}_{\leq t}, \text{trajectories}_{\leq t}\}$)  
2. **Action Mapping**: Hyperparameter schedules ($a_t \in \mathcal{A}=\mathbb{R}^d$) are discretized into binned ranges (e.g., $\alpha \in \{10^{-5}, 10^{-4}, ..., 10^{-1}\}$) for tokenized output  
3. **Reward Modeling**: Define reward as $r_t = G_t - r_{\text{baseline}}$, where $r_{\text{baseline}}$ is the return from the previous hyperparameter set  

The LLM's policy is optimized via reinforcement learning (RL) to maximize:  
$$ J(\phi) = \mathbb{E}_{s_t, a_t}\left[\sum_{t=0}^\infty \gamma^t r_t\right] $$  
while retaining supervised pretraining via knowledge distillation. A hybrid loss combines HPO ($\mathcal{L}_{\text{prompt}}$) and RL gradients:  
$$ \mathcal{L}_{\text{total}} = \lambda \mathcal{L}_{\text{prompt}} + (1-\lambda)\mathcal{L}_{\text{RL}} $$  
with $ \lambda = 0.5 $ as a heuristic initial value.

### **Experimental Design**
We validate HyperPrompt across four axes of novelty and reproducibility:  

#### **Environments and Algorithms**  
- **Procedural Environments**: Procgen (9 procedurally generated levels), ARLBench (10 domains with curated HPO splits)  
- **3D Interaction**: NetHack (high-dimensional partial observability)  
- **Continuous Control**: 7 MuJoCo tasks (HalfCheetah, Humanoid)  

#### **Baselines and Ablations**  
- **AutoML Baselines**: OptFormer, Hyperopt (Bayesian HPO), Population-based Training (PBT)  
- **LLM Variants**: GPT-3.5 (API-based), Llama-3, Mistral-Nemo  
- **Ablations**:  
  - **Prompt Components**: Remove metric feedback (only trajectories) or trajectories (only metrics)  
  - **Adaptation Frequency**: $ k \in \{5, 10, 20\} $ episodes  
  - **Parameterization**: Full-space vs. subset updates (e.g., varying $\alpha$ while keeping $\gamma$ static)  

#### **Evaluation Metrics**  
- **Performance**: Normalized average reward over 25 environments (per ARLBench guidelines), episode length to reach 90% of the environment's known maximum reward  
- **Hyperparameter Sensitivity**: Compute $ \Delta G_{\text{adapt}} / \Delta G_{\text{static}} $ ratios for varying parameter changes  
- **Stability**: KL-divergence divergence between policy updates and episode-to-episode reward variance  
- **Transferability**: Zero-shot testing on unseen domains (e.g., PyBullet robotic control after MuJoCo/Nethack training)  

#### **Implementation Details**  
- **LLM Fine-Tuning**: Use LoRA (low-rank adaptation) for parameter-efficient training  
- **Training Efficiency**: Train LLMs on $ \leq 10^4 $ prompt-response pairs derived from 500 environment-algorithm configurations in ARLBench  
- **Deployment**: Integrate with RLlib and StableBaselines3 APIs, using prompts to adjust trainer args every $ k $ episodes via LLM-generated JSON  

#### **Theoretical Considerations**  
We hypothesize a correlation between the LLM's attention head activation patterns and critical hyperparameters. To test this:  
1. Calculate $ \text{Attn}^{\text{hparam}} = \sum_{h=1}^{H} \mathbb{I}(\text{Attention Head } h \text{ affects } \theta_{\text{LLM}}) $  
2. Compare attention localization for key parameters (e.g., learning rate) across environments  

This provides insight into the LLM's internal prioritization of contextual cues, potentially revealing task-agnostic adaptation principles.

### **Hyperparameter Management in LLM Training**  
LLM-specific hyperparameters (e.g., learning rate for LoRA fine-tuning) are optimized using ARLBench's own hyperparameter sensitivity analysis. The validation process ensures that any computational overhead from LLM inference is within 15% of baseline agents, maintaining practical utility.

## **Expected Outcomes & Impact**

### **Empirical Improvements in RL Efficiency**  
HyperPrompt is expected to achieve state-of-the-art results in three key areas:  
1. **Reduced Convergence Time**: By dynamically aligning hyperparameters with training phase, agents should reach 90% of max return 30-50% faster across MuJoCo and Procgen, particularly in delayed-reward environments.  
2. **Enhanced Sample Efficiency**: Environments like NetHack and Procgen require extensive exploration. By optimizing entropy coefficients $\eta$ in real-time, HyperPrompt will demonstrate 20-40% higher reward per million environment steps.  
3. **Hyperparameter Adaptation Quality**: Using the ARLBench metric $\mathcal{M}_{\text{adapt}} = \frac{\sum_{t=1}^T \mathbb{I}(\text{LLM HPARAMS}_t \text{ improves reward})}{T}$, we anticipate $\mathcal{M}_{\text{adapt}} > 0.65$, outperforming static HPO baselines  

These improvements will be validated through:  
- **Performance Distributions**: T50 scores (Episodes to Halfway Convergence) across 25 environments, showing narrower variance  
- **Ablation Results**: Removal of KL-divergence metrics from prompts should degrade adaptation quality on continuous control tasks by $ >15\% $, confirming contextual feedback's importance  
- **Latency Analysis**: LLM inference should require $ <100ms $ per update on A10 GPUs, making it feasible for real-time systems  

### **Theoretical Advancements in AutoRL and LLM Integration**  
We anticipate novel contributions to AutoRL theory:  
- **Time-Dependent HPO Priors**: The LLM implicitly learns temporal priors over hyperparameter spaces, challenging the independence assumption in traditional Bayesian HPO. We will analyze this via Markov transition matrices $ P(\theta_{t+1} \mid \theta_t) $.  
- **Meta-Learning Without Reinforcement**: While many frameworks (e.g., MAML) require explicit gradient updates, HyperPrompt uses LLMs' attention mechanisms to encode meta-learning in zero-shot settings. We will compare adaptation rates when initializing with different pretrained LMs.  

### **Practical Implications for RL Users**  
HyperPrompt will offer:  
1. **Zero-Code Deployment**: Users specify RL algorithm and environment, and the system automatically generates optimization signals.  
2. **Resource Accessibility**: By minimizing compute overhead (e.g., using $ k=20 $), it reduces the barrier for entry compared to AutoRL benchmarks (Becktepe et al., 2024) that require 1000-node clusters.  

### **Community Collaboration Potential**  
This work bridges three previously siloed communities:  
- **AutoML**: Shifts from global optimization to online adaptation  
- **Meta-RL**: Demonstrates LLMs as meta-controllers, reducing reliance on hand-crafted rules  
- **LLMs**: Expands use cases beyond NLP/image generation, addressing their untapped potential for continuous control  

The release of the full prompt dataset and training code will catalyze further research into LLM-mediated optimization frameworks, aligning with the workshop's goals of cross-pollination.

## **Conclusion**

HyperPrompt represents a paradigm shift in automated reinforcement learning by merging LLM-based meta-learning with dynamic hyperparameter optimization. Through a structured prompt-based mechanism, it enables real-time adaptation, addressing the critical challenge of non-stationary hyperparameter landscapes without incurring prohibitive computational costs. By grounding this approach in standardized benchmarks like ARLBench and validating across diverse environments, our proposal ensures robustness and generalization. Empirically, we anticipate measurable improvements in convergence speed and sample efficiency, supported by theoretical analyses of LLM attention patterns and temporal priors. Practically, the system democratizes RL deployment, enabling non-experts to benefit from advanced hyperparameter tuning through a zero-config, modular interface.  

Crucially, HyperPrompt fosters cross-disciplinary collaboration. Its architecture leverages RL's reward maximization framework, AutoML's optimization expertise, and LLMs' in-context learning—advancing each domain while building bridges between them. The integration of prompt engineering with real-time control mechanisms could inspire new designs in LLM-based optimization, while its reliance on AutoML benchmarks provides a rigorous validation framework. By validating this hybrid approach in the workshop's AutoRL landscape, we aim to catalyze unified progress toward more autonomous, generalizable reinforcement learning systems. Future directions include transferring the method to physical robotics training, where environmental non-stationarity is even more pronounced, and exploring self-critical control (e.g., LLMs adjusting their own inference hyperparameters).