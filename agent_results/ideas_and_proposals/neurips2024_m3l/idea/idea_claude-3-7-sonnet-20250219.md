# Beyond the Edge of Stability: A Phase Transition Framework for Deep Learning Optimization

## Motivation
The "Edge of Stability" (EoS) phenomenon represents a critical challenge in deep learning optimization, where models operate just beyond theoretical stability boundaries yet still converge. Understanding this paradox is crucial as we scale to billion-parameter models, where each training attempt incurs massive computational costs. Current optimization theory fails to explain why models converge despite violating traditional stability assumptions. Without resolving this disconnect between theory and practice, we lack principled approaches to design reliable training procedures for large models.

## Main Idea
We propose a phase transition framework that reconceptualizes neural network optimization landscapes as systems undergoing state transitions. Rather than viewing EoS as an anomaly, we characterize it as a beneficial transitory state between order (stable regime) and chaos (divergence). Our approach models optimization trajectories using statistical physics concepts, particularly renormalization group theory, to capture how different scale dynamics interact during training. We develop tools to identify phase boundaries specific to architecture-optimizer combinations and derive adaptive learning rate schedules that intentionally navigate through these transitions. This framework enables practitioners to predict when large learning rates will promote beneficial exploration versus destructive instability, potentially reducing computational requirements for large model training by 30-50% while providing theoretical guarantees previously thought impossible in the EoS regime.