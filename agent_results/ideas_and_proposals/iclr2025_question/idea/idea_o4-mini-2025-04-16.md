Title: LatentFlow: Layerwise Normalizing Flows for Uncertainty and Hallucination Detection in LLMs

Motivation: Large language models (LLMs) often generate confident but incorrect or fabricated content (“hallucinations”). Existing uncertainty quantification (e.g., MC-dropout, ensembles) is computationally heavy and lacks fine‐grained signals. A lightweight, scalable approach that detects distributional shifts in hidden representations can enable real-time trust assessment suitable for high-stakes domains.

Main Idea: We propose LatentFlow, a method that fits compact normalizing-flow models to frozen layerwise activations of a pretrained LLM using unlabeled corpora. During inference, LatentFlow computes the likelihood of each layer’s activation; low likelihood indicates out-of-distribution inputs or emerging hallucinations. We aggregate layerwise log-densities into a unified uncertainty score and fuse it with token-level entropy to distinguish low-confidence from confident hallucinations. Methodology steps:
1. Extract activations from each transformer layer over a large text corpus.
2. Train small normalizing flows per layer to learn in-distribution density.
3. At runtime, compute per-layer densities and aggregate into an uncertainty metric.
4. Calibrate thresholds on held-out hallucination benchmarks.
Expected outcomes include real-time, low-overhead UQ, improved hallucination detection accuracy over baselines, and transparent per-layer insights. This enables safer deployment of LLMs in healthcare, law, and autonomous systems.