Title: Cross-Modal Graph Representation Learning for Incomplete Materials Data

Motivation:  
Materials datasets span compositions, crystal structures, synthesis recipes, spectroscopy, and microscopyâ€”yet rarely cover all modalities for every sample. This fragmentation hinders end-to-end AI-driven discovery, as models struggle to integrate partial views and leverage physical priors. A unified framework that imputes missing modalities and learns joint embeddings can accelerate screening of novel compounds and guide targeted synthesis.

Main Idea:  
We propose a self-supervised, heterogeneous graph neural network (GNN) that (1) represents each material sample as a graph with modality-specific node and edge types (e.g., atoms, process steps, spectral peaks), (2) applies physics-informed augmentations (perturbing lattice constants, simulating noise in spectra) to generate positive pairs, and (3) employs contrastive learning to align embeddings across modalities. A cross-modal decoder reconstructs missing views, enabling downstream tasks (property regression, candidate ranking) even when data is partial. We will benchmark on pooled open-access datasets (Materials Project, NOMAD, open X-ray/SEM repositories), demonstrating improved prediction accuracy, robustness to missing data, and reduced experimental cycles. This framework can be extended to active-learning pipelines, guiding experiments where specific characterizations maximize information gain.