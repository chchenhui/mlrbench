**1. Title:**  
Infusing Theory-of-Mind-Driven Explanations into Transformer-Based NLP Models  

**2. Motivation:**  
Modern NLP models lack intrinsic interpretability, making their decisions opaque to users. Teaching models to reason about human-like mental states (e.g., intentions, beliefs) during task execution could bridge this gap. By explicitly modeling ToM components, we can generate explanations aligned with human cognitive processes, enhancing trust in AI for sensitive applications like healthcare and education. This work addresses the critical need for model explainability and human-AI value alignment in ML systems.  

**3. Main Idea:**  
We propose a training framework that integrates ToM supervision into transformer-based architectures (e.g., BERT, GPT). First, annotate existing datasets (e.g., dialogues, narratives) with latent mental state labels (beliefs, desires) using crowdsourced annotations or cognitive science frameworks. Second, train a dual-channel model: one pathway learns standard task objectives (e.g., question answering), while the other jointly predicts associated mental states. The model is regularized to ensure coherence between its task output and inferred mental states. For example, when answering a biased question, the model might highlight its recognition of the speaker’s intent while generating a response. The output layer will also generate natural-language explanations of decisions via posterior ToM reasoning (e.g., “I detected aggression in the query, so I prioritized de-escalation”). Evaluation benchmarks will measure both task performance and explanation faithfulness via human judgments and counterfactual consistency tests. Success could advance transparent AI in high-stakes domains where understanding decision-making processes is as crucial as accuracy.