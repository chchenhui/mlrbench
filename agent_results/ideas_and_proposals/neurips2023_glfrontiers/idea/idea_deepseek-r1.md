**Title: UniGraph: A Unified Foundation Model for Cross-Domain Graph-Structured Data**

**Motivation:** Despite advancements, graph foundation models remain siloed in domains like chemistry, limiting their adaptability to broader scientific use cases. A general-purpose model that captures universal structural patterns across diverse graphs (e.g., social, biological, or material networks) could unlock cross-domain insights and reduce redundant domain-specific training. This is critical for accelerating scientific discovery through transferable relational reasoning.

**Main Idea:** UniGraph proposes a self-supervised pretraining framework leveraging a **hierarchical graph transformer** with two modules: a *local-node transformer* for fine-grained interactions and a *global-graph transformer* for holistic semantics. Pretraining combines multi-domain graph datasets (e.g., protein interaction networks, citation graphs, material databases) under three objectives: (1) contrastive node linking to align similar substructures across domains, (2) graph autoencoding for reconstruction, and (3) domain-adversarial training to ensure invariance to domain-specific features. The model is pretrained on heterogeneous graphs with normalized attribute schemas and then fine-tuned on task-specific data. Expected outcomes include strong zero-shot performance on unseen graph types (e.g., training on biology and testing on social networks) and improved sample efficiency in low-data scientific applications. This approach democratizes graph AI by enabling a single model to serve interdisciplinary use cases.