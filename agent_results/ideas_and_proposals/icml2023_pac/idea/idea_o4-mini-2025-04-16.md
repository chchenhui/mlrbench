Title: Dynamic Data-Dependent Priors for PAC-Bayesian Non-Stationary Contextual Bandits

Motivation:  
Real-world interactive systems (e.g., recommendation engines, robotics) often face non-stationary environments where the reward distributions shift over time. Traditional bandit algorithms either ignore distributional drift or lack rigorous generalization guarantees. By integrating PAC-Bayesian theory with adaptive priors, we can obtain tight, time-uniform performance bounds while reacting efficiently to distributional changes.

Main Idea:  
We propose an online contextual bandit algorithm that maintains a sequence of data-dependent priors, each formed by exponentially reweighting past posteriors over a sliding window. At each round, the learner (1) updates its posterior using observed contexts, actions, and rewards via a fast variational inference step, (2) forms the next prior as a mixture of recent posteriors to capture drift, and (3) selects actions by sampling from the current posterior. We derive a novel time-uniform PAC-Bayes regret bound that explicitly accounts for prior shifts, yielding tighter guarantees under smooth distribution drift. Empirically, our method outperforms fixed-prior baselines on synthetic and real non-stationary bandit benchmarks, demonstrating improved sample efficiency and robustness to change points.