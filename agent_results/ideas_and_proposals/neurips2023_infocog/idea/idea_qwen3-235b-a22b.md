**Title:**  
Optimizing Mutual Information for Human-Centric AI Cooperation via Cognitive-Informed Representation Learning  

**Motivation:**  
Human-AI collaboration remains limited by AI systems' inability to align with human cognitive patterns, such as interpreting ambiguous communication or adapting to individual differences. Traditional reinforcement learning (RL) often neglects the information-processing constraints of humans, leading to suboptimal or uninterpretable agent behavior. This work bridges the gap by enforcing alignment in information flow between AI and human cognitive models, addressing core challenges in explainable and cooperative AI design.  

**Main Idea:**  
We propose a framework combining variational information bottleneck (VIB) and RL to train agents that compress human-relevant information into interpretable representations. First, VIB-derived latent codes encode human cognitive traits (e.g., attention biases, communication signals) from behavioral data, distilling task-critical features. Second, an agentâ€™s policy maximizes mutual information (MI) between its actions and the learned human representation, ensuring predictions align with human expectations. To tackle MI estimation in high dimensions, we integrate neural estimators (e.g., MINE) with contrastive learning, validated against human neuroimaging or interaction datasets. Evaluations focus on tasks requiring shared intentionality (e.g., cooperative navigation, ambiguous instruction resolution). Expected outcomes include agents that (1) reduce information overload for humans, (2) improve robustness to individual cognitive differences, and (3) enhance human trust through interpretable decision-making. This approach merges information theory, RL, and cognitive science, enabling scalable human-AI systems for education, healthcare, and joint decision-making.