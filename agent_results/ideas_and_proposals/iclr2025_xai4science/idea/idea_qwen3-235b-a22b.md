**Title:** **"Knowledge-Guided Self-Explainable Models for Biomedical Discovery"**  

**Motivation:** Interpretable AI is critical in healthcare, where black-box models hinder clinical trust and scientific progress. Current approaches often prioritize performance over transparency or use post-hoc explanations, which may lack fidelity. By embedding domain knowledge into model architecture, we can bridge model predictivity with human-understandable insights, enabling AI to act as a collaborative tool for uncovering novel biological mechanisms and therapeutic targets.  

**Main Idea:** We propose designing **knowledge-guided self-explainable models** that integrate biomedical ontologies (e.g., gene interaction networks, pharmacokinetic pathways) into graph neural networks (GNNs) and additive models. Each module in the architecture will explicitly encode interpretable entities (e.g., genes, drugs) and relations, enabling end-to-end learning of biological processes. For example, using attention mechanisms over known regulatory networks, the model will identify subpopulation-specific mechanisms in cancer treatment response. Domain experts will validate discovered insights (e.g., novel biomarkers) through wet-lab experiments or clinical trials. A hybrid evaluation framework will assess both predictive performance (e.g., survival prediction) and explainability (e.g., alignment with known biology). Expected outcomes include models that achieve state-of-the-art results while revealing actionable scientific insights, such as synergistic drug targets or disease subtypes. This approach bridges the gap between ML and mechanistic understanding, fostering trust and advancing precision medicine.