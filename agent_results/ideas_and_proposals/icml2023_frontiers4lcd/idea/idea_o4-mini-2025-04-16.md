Title: Score-Control: Score-based Diffusion for Stochastic Optimal Control

Motivation:  
Solving stochastic optimal control via Hamilton–Jacobi–Bellman PDEs is intractable in high dimensions. Meanwhile, score-based diffusion models excel at sampling complex distributions. Bridging these enables data-driven control without explicit PDE solvers.

Main Idea:  
We introduce Score-Control, a framework that learns a neural score function to approximate ∇ₓ log p*(τ) of near-optimal trajectories τ. First, we generate pseudo ground-truth trajectories via approximate dynamic programming or model predictive control. We then train the score network with denoising score matching on corrupted versions of these trajectories. At inference, a learned reverse-time SDE combines the system’s drift with the learned score to sample control trajectories directly from the optimal policy distribution in just a few diffusion steps. This model-free approach scales to high-dimensional dynamics, bypasses explicit HJB solutions, and naturally handles stochasticity. We will validate on continuous‐control benchmarks and uncertain dynamical systems, anticipating gains in sample efficiency and robustness. Potential Impact: By unifying diffusion generative modeling with stochastic control, Score-Control opens a new paradigm for scalable, theory-grounded policy synthesis.