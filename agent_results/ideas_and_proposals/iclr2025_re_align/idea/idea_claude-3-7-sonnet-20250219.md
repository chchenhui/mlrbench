# Probing Representational Alignment Without Ground Truth

## Motivation
Current representational alignment metrics between artificial and biological systems often rely on simplistic assumptions about what constitutes "alignment." Most methods compare representations against assumed ground truths or use correlation-based techniques that may capture superficial similarities rather than functional equivalence. This creates a fundamental challenge: how can we measure meaningful representational alignment when we lack a definitive ground truth about the "correct" representation of information in either system?

## Main Idea
I propose developing a new alignment framework based on causal intervention rather than correlation. The key insight is that truly aligned representations should respond similarly to targeted perturbations, regardless of their surface-level structure. The methodology involves: (1) Identifying minimal interventions in input space that cause specific representational changes in one system; (2) Applying equivalent interventions to the second system; (3) Measuring whether these interventions produce analogous functional changes in representation. This approach shifts the focus from static representational similarity to dynamic representational behavior under controlled conditions. By systematically mapping intervention-response patterns across systems, we can construct a "causal alignment space" that reveals which aspects of representations are functionally equivalent despite different encodings. This methodology would work across modalities and architectures without assuming representation isomorphism, potentially revealing deeper insights about computational strategies shared between biological and artificial systems.