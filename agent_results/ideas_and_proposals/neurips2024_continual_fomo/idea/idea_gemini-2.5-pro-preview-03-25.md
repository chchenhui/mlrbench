**Title:** Adaptive Parameter Allocation for Continual Learning in Foundation Models

**Motivation:** Fine-tuning foundation models (FMs) on new tasks often leads to catastrophic forgetting of previously learned knowledge, especially when new datasets are smaller or domain-shifted compared to pre-training data. Existing continual learning (CL) methods struggle with the scale of FMs, while standard parameter-efficient fine-tuning (PEFT) doesn't explicitly prevent forgetting across sequential tasks.

**Main Idea:** We propose "Adaptive Parameter Allocation" (APA), a CL strategy specifically for FMs using PEFT methods like LoRA or Adapters. Instead of allocating a fixed-size adapter for each task, APA dynamically determines the optimal parameter budget (e.g., LoRA rank, number of adapter parameters) per task based on task complexity and similarity to previous tasks. This allocation is guided by a small, efficient meta-controller trained to predict potential forgetting. Furthermore, APA introduces orthogonalization constraints between the parameter subspaces allocated to different tasks, minimizing interference. Expected outcomes include improved knowledge retention across sequential tasks with minimal overhead compared to full fine-tuning or retraining, enabling more efficient and scalable lifelong learning for large FMs.