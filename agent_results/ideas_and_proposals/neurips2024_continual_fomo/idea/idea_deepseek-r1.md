**Title:** Knowledge-Anchored Parameter Isolation for Catastrophic Forgetting Mitigation in Foundation Models  

**Motivation:** Foundation models (FMs) face severe catastrophic forgetting when fine-tuned on smaller, domain-specific datasets, as their vast pretrained knowledge is disrupted by narrow updates. Current continual learning (CL) methods struggle to scale due to computational overhead and lack of mechanisms to distinguish critical vs. adaptable parameters. Integrating structured knowledge sources like dynamic knowledge graphs (KGs) could provide a stable reference to guide updates, enabling efficient, scalable CL.  

**Main Idea:** This work proposes a CL framework where a dynamic KG is jointly maintained and aligned with the FMâ€™s latent space. The KG identifies "knowledge-anchored" parameters (e.g., attention heads or MLP layers) critical for retaining existing knowledge, freezing them during updates. New data triggers sparse, KG-guided activation of adaptable parameters (e.g., via adapters or soft prompts) and incremental KG expansion. To handle domain shifts, KG nodes will encode task-specific metadata, enabling routing of inputs to relevant sub-networks. Benchmarks will measure forgetting rates, adaptation efficiency, and KG-FM alignment. Expected outcomes include a 30-50% reduction in forgetting with <5% additional compute, enabling lifelong FM updates while preserving core knowledge. Impact: Scalable CL for real-world FM applications like personalized AI assistants and domain-adaptive medical models.