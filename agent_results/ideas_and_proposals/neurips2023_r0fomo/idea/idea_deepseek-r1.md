**Title:** Domain-Aware Semi-Supervised Learning for Robust Few-Shot Adaptation in Foundation Models  

**Motivation:** Few-shot learning in foundation models often assumes labeled examples and test data share the same distribution, which fails in real-world scenarios where unlabeled data comes from broader or shifting domains. Current semi-supervised methods, designed for smaller models, do not exploit the self-supervised knowledge inherent in foundation models, limiting their ability to generalize robustly.  

**Main Idea:** This work proposes a framework that combines few-shot labeled examples with diverse unlabeled data to enhance robustness against distribution shifts. Building on foundation modelsâ€™ inherent capabilities, we introduce a *domain-aware consistency loss* that aligns representations of labeled and unlabeled data while penalizing discordant features from out-of-distribution samples. A dynamic weighting mechanism prioritizes unlabeled instances that align with the target task, trained via meta-learning on synthetic distribution shifts. Experiments on benchmarks like DomainNet and WILDS will quantify robustness gains over standard fine-tuning and semi-supervised baselines (e.g., FixMatch). The approach aims to reduce the labeled data requirement by 50% while maintaining performance under shift, enabling safer deployment in distributionally complex settings (e.g., medical imaging with rare diseases).