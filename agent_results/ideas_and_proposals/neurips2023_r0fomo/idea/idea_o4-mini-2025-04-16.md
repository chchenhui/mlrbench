Title: PromptGuard: Automated Adversarial Prompt Generation for Robust Few-Shot Learning

Motivation:  
Few-shot prompt-based methods in large foundation models often exhibit brittle behavior—small changes in phrasing, example ordering, or formatting can cause dramatic performance drops or unsafe outputs. A systematic approach to evaluate and fortify prompt robustness is critical to ensure reliable deployment in real-world applications.

Main Idea:  
We propose PromptGuard, a two-phase framework that first generates adversarial prompts by (1) applying gradient-guided perturbations in prompt embedding space and (2) exploring combinatorial rephrasings of few-shot examples and instructions via a reinforcement-learning scorer that maximizes model error or unsafe content. In the second phase, these failure-inducing prompts are incorporated into an adversarial fine-tuning loop: the foundation model is retrained or prompt-tuned to minimize sensitivity to these variants. We introduce a “Prompt Sensitivity Index” to quantify model invariance across a spectrum of adversarial perturbations. Experiments on classification, question answering, and dialogue safe-generation tasks using GPT-3 and T5 demonstrate up to 30% reduction in worst-case accuracy drops and a significant decrease in unsafe outputs, paving the way for more dependable few-shot deployments.