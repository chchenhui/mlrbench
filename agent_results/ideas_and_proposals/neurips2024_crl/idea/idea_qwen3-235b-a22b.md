**Title:**  
**Latent Causal Disentanglement via Unsupervised Interventions in Deep Generative Models**

**Motivation:**  
Current deep learning models often exploit spurious correlations in high-dimensional data (e.g., images, text) due to a lack of causal understanding, leading to unreliable inferences and algorithmic bias. Traditional causal discovery struggles with latent causal variables in such data. Enabling unsupervised disentanglement of latent causal factors while preserving causal relationships would enhance model interpretability, fairness, and generalization in critical applications like healthcare, economics, and large language models (LLMs).

**Main Idea:**  
We propose a novel causal representation learning (CRL) framework that integrates unsupervised causal discovery with deep generative models. Our method uses a variational autoencoder (VAE) with a structured latent space, where each latent dimension is encouraged to capture independent causal factors through a self-contrastive loss. During training, pseudo-interventions are simulated by perturbing individual latents in the decoding process. A contrastive loss aligns these perturbations with localized input regions (e.g., object parts in images, semantic phrases in text), creating inductive bias toward disentangled causal representations. Additionally, a graph neural network (GNN) infers the causal graph among latents by enforcing acyclicity and sparsity constraints. The model is validated on benchmarks like Causal3DIdent, with applications to LLMs for interpretable decision-making and medical imaging for distinguishing causal disease markers from artifacts. Expected outcomes include robust, fair, and causally faithful AI systems capable of intervention-based reasoning and counterfactual generation.