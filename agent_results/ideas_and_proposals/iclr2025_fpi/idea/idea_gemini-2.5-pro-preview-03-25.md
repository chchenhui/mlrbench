**Title:** Amortized Score Guidance for Weighted Diffusion Sampling

**Motivation:** Sampling from complex, high-dimensional distributions, such as posterior distributions in Bayesian inference or aligned distributions for generative models (e.g., safety-aligned LLMs), is challenging. Existing methods often require costly model retraining (fine-tuning) or computationally expensive sampling steps (e.g., MCMC, rejection sampling) at inference time, especially when weighting a base distribution by a target density.

**Main Idea:** We propose learning a lightweight, amortized guidance network that modifies the score function (`∇log p(x)`) used in diffusion model sampling (or related score-based/flow-based models). Given a pre-trained base model `p(x)` and a target weighting function `w(x)`, we want to sample from `p(x)w(x)/Z`. Our guidance network learns to efficiently approximate `∇log w(x)` or directly learns the required perturbation to the sampling trajectory/score. This network is trained offline on examples illustrating the target property or using related objectives (e.g., energy-based modeling of `w(x)`). At inference, this pre-trained guidance network module is plugged into the diffusion sampling process, steering generation towards the target distribution efficiently without requiring expensive iterative optimization or retraining the large base model. This could enable fast, high-quality sampling for Bayesian inference and inference-time alignment of generative models.