**Title:** Information-Theoretic Data Selection for Efficient Fine-Tuning of Foundation Models  

**Motivation:** Fine-tuning foundation models (FMs) on downstream tasks requires substantial labeled data, leading to high computational costs and environmental impact. Current methods for data selection (e.g., random sampling) lack principled criteria to identify maximally informative training samples, resulting in inefficiency. This work addresses the gap by developing a rigorous framework to optimize data efficiency in FM fine-tuning, reducing resource consumption while maintaining performance.  

**Main Idea:** We propose a novel data selection strategy based on *information-theoretic principles*, specifically leveraging *joint mutual information* between data points and model parameters. The approach involves two stages: (1) theoretical analysis of how individual and grouped samples influence parameter updates during fine-tuning, quantified via mutual information bounds; (2) an adaptive algorithm that prioritizes batches of data points maximizing uncertainty reduction and task-relevant information. The framework will integrate gradient-based approximations for scalability and employ kernelized embeddings to handle high-dimensional FM outputs. Expected outcomes include a mathematically grounded selection protocol that reduces data requirements by 40â€“60% for comparable accuracy, validated on vision-language and text FMs. This would lower computational barriers to FM customization, democratizing access to resource-constrained users and aligning with sustainable AI goals.