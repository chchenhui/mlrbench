**Title**: Benchmark Cards: Standardizing Context and Holistic Evaluation for ML Benchmarks

**Motivation**: Current ML benchmarks often rely on single aggregate metrics, promoting leaderboard optimization rather than comprehensive model understanding. This neglects crucial aspects like fairness across subgroups, robustness to distribution shifts, and efficiency, hindering the selection of truly suitable models for real-world, contextualized applications.

**Main Idea**: We propose "Benchmark Cards," a standardized documentation framework accompanying ML benchmarks. Analogous to Model Cards, these cards will detail: 1) The benchmark's intended evaluation context and scope. 2) Key characteristics and potential biases of the underlying dataset(s). 3) A recommended suite of holistic evaluation metrics (e.g., subgroup performance, fairness indicators, robustness tests, computational cost) beyond the primary leaderboard metric. 4) Known limitations and potential misuse scenarios. This framework aims to shift focus from single-score rankings to multi-faceted, context-aware model assessment, promoting more responsible and informative benchmarking practices within the ML community. We will develop a template and populate initial cards for popular benchmarks.