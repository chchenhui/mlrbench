**Title:** Self-Supervised Learning of Temporal-Aware Tactile Representations via Active Interaction  

**Motivation:** Modern tactile sensors generate high-dimensional, temporally rich data, but existing models (e.g., CNNs) struggle to capture the dynamic, active nature of touch. Labeled tactile datasets are scarce, and passive sensing fails to model how humans/robots interactively explore surfaces. Developing self-supervised methods that exploit temporal coherence and active exploration could unlock robust tactile understanding without reliance on manual annotations.  

**Main Idea:** We propose a self-supervised framework that jointly learns temporal-aware tactile representations and active exploration policies. First, a contrastive learning module leverages temporal coherence between tactile sequences (e.g., sliding motions) to cluster similar interactions while separating dissimilar ones. Second, a reinforcement learning (RL) agent learns optimal exploration strategies (e.g., pressure, speed) to maximize information gain for downstream tasks like texture recognition. The model will be trained on a new large-scale tactile dataset of diverse materials, with open-source tools provided for reproducibility. We hypothesize that explicitly modeling temporal dynamics and active sensing will outperform static, supervised baselines in accuracy and data efficiency. Expected outcomes include benchmarks for tactile representation learning and insights into how active interaction shapes perception. This work could advance robotic manipulation, prosthetics, and haptic interfaces by enabling systems to "understand" touch through self-supervised exploration.