**Title:** Confidence-Aware Filtering for Robust Self-Improvement without Collapse

**Motivation:** Standard self-improvement methods risk model collapse by indiscriminately training on self-generated data filtered by imperfect learned verifiers or reward models. These verifiers can be unreliable, leading the generator model astray by reinforcing flawed outputs or rejecting high-quality ones.

**Main Idea:** This research proposes a confidence-aware filtering mechanism within the self-improvement loop. Alongside predicting the quality (e.g., reward score) of generated data, the verifier model will also estimate its prediction confidence (e.g., using uncertainty quantification methods like ensembles or Bayesian inference). During the data selection phase for generator training, only samples verified with high confidence will be used for updates. Low-confidence samples can be discarded, down-weighted, or flagged for potential minimal human review (if applicable). This selective filtering based on verifier reliability aims to stabilize training, prevent the generator from overfitting to verifier errors, mitigate model collapse, and ensure more robust and trustworthy self-improvement.