**Title:** Adaptive Uncertainty-aware Self-Improvement via Dynamic Calibration of Synthetic Data  

**Motivation:**  
Self-improvement through synthetic data generation risks model collapse due to overconfidence in errors from generated samples and imperfect verifiers. Current methods lack mechanisms to quantify and adapt to uncertainty in synthetic data validity, leading to unreliable training. Addressing this is critical to ensure safe, continuous learning without human supervision.  

**Main Idea:**  
Propose an adaptive self-improvement framework integrating calibrated uncertainty estimation. First, train an ensemble of verifier models to assess the quality of self-generated data, leveraging disagreement among verifiers as uncertainty signals. Second, during training, prioritize samples with low uncertainty (high consensus) while downweighting ambiguous or conflicting cases. Third, dynamically recalibrate the verifier ensemble using a small buffer of trusted, high-quality data (e.g., curated real-world samples or proven synthetic data) to prevent drift. The framework iteratively improves the model’s ability to distinguish reliable synthetic data, mitigating feedback loops of error. Expected outcomes include reduced collapse risk, stable long-term training, and improved generalization. This approach bridges the verification-generation gap by grounding self-improvement in uncertainty-aware learning, enabling safer scalability and aligning with weak-to-strong generalization principles for safety-critical applications.  

**Response:**  
1. **Title:** Adaptive Uncertainty-aware Self-Improvement via Dynamic Calibration of Synthetic Data  
2. **Motivation:** Self-improvement through synthetic data generation risks model collapse due to overconfidence in errors from generated samples and imperfect verifiers. Current methods lack mechanisms to quantify and adapt to uncertainty in synthetic data validity, leading to unreliable training. Addressing this is critical to ensure safe, continuous learning without human supervision.  
3. **Main Idea:** Propose an adaptive self-improvement framework integrating calibrated uncertainty estimation. First, train an ensemble of verifier models to assess the quality of self-generated data, leveraging disagreement among verifiers as uncertainty signals. Second, during training, prioritize samples with low uncertainty (high consensus) while downweighting ambiguous or conflicting cases. Third, dynamically recalibrate the verifier ensemble using a small buffer of trusted, high-quality data (e.g., curated real-world samples or proven synthetic data) to prevent drift. The framework iteratively improves the model’s ability to distinguish reliable synthetic data, mitigating feedback loops of error. Expected outcomes include reduced collapse risk, stable long-term training, and improved generalization. This approach bridges the verification-generation gap by grounding self-improvement in uncertainty-aware learning, enabling safer scalability and aligning with weak-to-strong generalization principles for safety-critical applications.