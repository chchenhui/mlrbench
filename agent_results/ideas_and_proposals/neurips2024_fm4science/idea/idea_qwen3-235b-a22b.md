**Title:** Autoregressive Foundation Models with Integrated Scientific Simulators for Accelerated Hypothesis Validation  

**Motivation:** Scientific discovery cycles are often hindered by manual hypothesis validation via precise but computationally intensive simulations (e.g., molecular dynamics, finite element analysis). While foundation models (FMs) excel in generalization, their inability to directly interface with domain-specific solvers limits their utility in rigorous scientific workflows. Bridging this gap could enable autonomous, rapid iteration over hypotheses, reducing discovery timelines in critical areas like drug design, materials science, and climate modeling.  

**Main Idea:** Propose a hybrid framework where autoregressive FMs (e.g., GPT-5) are trained to generate scientific hypotheses *and* interact with classical simulators through in-context API calls. The FM would: (1) encode hypotheses as structured inputs (e.g., molecular SMILES strings, PDE boundary conditions), (2) trigger simulations via embedded tool calls, (3) parse numerical outputs (e.g., energy levels, stress tensors), and (4) refine subsequent hypotheses using attention-based feedback loops. Training would combine pre-training on scientific literature/code with reinforcement learning to optimize discovery efficiency, guided by reward functions balancing accuracy and computational cost. Expected outcomes include a 10–100× reduction in hypothesis validation cycles and novel discoveries in benchmark tasks (e.g., high-stability battery materials). This approach directly addresses the challenge of integrating FMs with classical tools, enabling closed-loop scientific exploration where FMs act as autonomous agents orchestrating discovery pipelines.