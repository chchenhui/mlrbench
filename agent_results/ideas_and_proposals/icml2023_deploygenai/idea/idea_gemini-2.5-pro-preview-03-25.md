**Title:** SAFEGEN: Interpretable Safety Checks for Generative Medical Imaging

**Motivation:** Generative models hold promise for augmenting medical image datasets or synthesizing scans for training. However, deploying them risks generating unrealistic or artifact-ridden images that could mislead diagnostic AI or clinicians, posing safety hazards. Current quality checks are often manual or lack fine-grained interpretability regarding *why* an image might be unsafe or flawed.

**Main Idea:** We propose SAFEGEN, a framework to automatically assess the safety and realism of generated medical images (e.g., CT, MRI) with interpretable feedback. SAFEGEN integrates an anomaly detection module trained on real medical images with an interpretability component (e.g., Grad-CAM, SHAP). When evaluating a synthetic image, it flags potentially unrealistic or artifact-prone regions and provides visual heatmaps highlighting the features contributing to the anomaly score. This allows developers to understand failure modes and clinicians to verify generated data quality before use, enhancing trust and safety in deployment. Evaluation will involve assessing SAFEGEN's ability to detect known artifacts and correlate its interpretations with radiologist assessments.