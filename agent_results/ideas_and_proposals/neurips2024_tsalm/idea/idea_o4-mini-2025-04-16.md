Title: CrossModalTS: Pretraining Multimodal Time Series Foundation Models with Text and Sensor Data

Motivation:  
Real-world time series applications—such as predictive maintenance, healthcare monitoring, and finance—often come with correlated textual records (e.g., maintenance logs, clinical notes, news reports). Current time series foundation models ignore these exogenous modalities, limiting forecasting accuracy, anomaly detection, and interpretability. By jointly modeling signals and related text, we can capture richer context and improve downstream performance.

Main Idea:  
We propose CrossModalTS, a transformer-based pretraining framework for joint time series and text data. First, we assemble a large multimodal dataset by pairing sensor streams with aligned textual logs across diverse domains. We design a dual-encoder architecture: a temporal encoder processes raw or patched time series, and a language encoder ingests corresponding text. Pretraining objectives include masked time series reconstruction, masked language modeling, and cross-modal contrastive alignment to learn shared representations. After pretraining, we fine-tune CrossModalTS for forecasting, anomaly detection, and classification tasks. We expect significant gains in predictive accuracy and robustness, along with enhanced interpretability via cross-attention visualizations, paving the way for real-world multimodal time series applications.