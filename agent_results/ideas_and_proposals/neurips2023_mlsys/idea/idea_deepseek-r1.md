**Title:** Learning Optimal Communication Schedules in Distributed LLM Training via GNNs and Reinforcement Learning  

**Motivation:** Training Large Language Models (LLMs) across thousands of devices faces significant communication bottlenecks due to inefficient synchronization and device topology mismatches. Current compiler-led heuristic scheduling often results in suboptimal resource utilization, prolonging training time and energy consumption. Addressing this is critical to reduce the environmental and financial costs of LLM development.  

**Main Idea:** We propose a framework combining graph neural networks (GNNs) and reinforcement learning (RL) to learn dynamic communication schedules tailored for distributed LLM training. A GNN encodes the LLM’s computation graph and hardware topology, capturing dependencies and connectivity. An RL agent then iteratively selects communication actions (e.g., prioritization, overlapping computation/communication) to minimize step time, with rewards based on observed latency and GPU utilization. The model is trained offline on diverse LLM architectures and hardware configurations, enabling adaptability. Expected outcomes include a compiler-integrated tool that autonomously optimizes schedules, reducing training time by 15–30% and energy use by 20% compared to heuristic baselines. This methodology fosters sustainable, scalable LLM development while providing a generalizable approach for compiler-driven system optimization.