**Title:** Optimizer Trajectory Curvature as an Indicator of Implicit Regularization

**Motivation:** While optimizers like SGD and Adam converge to different solutions with varying generalization performance, the precise mechanism of their implicit regularization tied to the *path* taken through the high-dimensional loss landscape is not fully understood. Understanding how trajectory properties relate to final solution quality could unlock better optimizer design.

**Main Idea:** We propose investigating the geometric properties of the *optimization trajectory* itself, not just the final solution point. Specifically, we hypothesize that the curvature and torsion of the path taken by parameters during training correlate with the optimizer's implicit regularization. We will track parameter trajectories for various optimizers (SGD, Momentum, Adam) and measure their differential geometric properties (e.g., integrated curvature). We will correlate these trajectory metrics with measures of landscape geometry at the endpoint (e.g., Hessian eigenvalues) and, critically, with generalization performance. The expected outcome is a quantitative link between trajectory shape and implicit bias, potentially revealing how optimizers navigating landscape features influence convergence to generalizable (e.g., flatter) minima.