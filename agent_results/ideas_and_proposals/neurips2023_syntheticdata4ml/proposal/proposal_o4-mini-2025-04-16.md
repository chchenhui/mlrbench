Title: PrivFairGen: Constrained LLM-Based Differentially Private and Fair Synthetic Tabular Data Generation

1. Introduction  
Background  
High-quality training data is the cornerstone of modern machine learning. Yet for many high-stakes domains‚Äîhealthcare, finance, criminal justice‚Äîdata scarcity, privacy regulations (e.g. HIPAA, GDPR), and entrenched biases pose severe barriers to dataset collection and open sharing. Tabular data in particular often contains sensitive attributes (age, race, medical codes) that cannot be published wholesale. At the same time, under-representation of minority groups in real datasets can lead to models that perpetuate or amplify societal inequities. Synthetic data generation offers a flexible remedy: once a generative model is trained, one can sample arbitrarily many ‚Äúfake‚Äù records to augment training sets, without exposing individual records.  

Recent advances in deep generative modeling‚Äîfor example DP-TBART (Castellon et al. 2023), TableDiffusion (Truda 2023), and DP-2Stage (Afonja et al. 2024)‚Äîdemonstrate that both transformer-based and diffusion-based methods can achieve strong data utility under a differential privacy (DP) guarantee. Concurrently, fairness-aware approaches such as Fairness-Aware Synthetic Data Generation (Johnson & Lee 2024) and fair VAE methods (Green & Black 2024) show that conditional generation can mitigate biases. However, most existing work tackles privacy and fairness separately: generative models are often optimized purely for fidelity, with privacy or fairness injected only as a post-hoc constraint. Moreover, large language models (LLMs) have demonstrated remarkable capacity for high-fidelity sequence generation and controllable sampling, but few studies (Tran & Xiong 2024; Red & Blue 2023) have fully exploited LLMs for tabular data under joint DP and fairness constraints.  

Research Objectives  
We propose PrivFairGen, a unified framework for synthesizing tabular data that simultaneously satisfies rigorous differential privacy guarantees and formal group fairness constraints by leveraging a pre-trained LLM. Our key objectives are:  
‚Ä¢ To design a two-stage fine-tuning procedure that (a) adapts an LLM‚Äôs text modeling capability to tabular data syntax, then (b) applies DP-SGD with precise privacy accounting.  
‚Ä¢ To incorporate fairness penalties‚Äîtargeting demographic parity and equalized odds‚Äîdirectly into the LLM‚Äôs fine-tuning loss or decoding phase, yielding synthetic data that corrects under-representation.  
‚Ä¢ To develop a decoding-time constraint mechanism that enforces statistical parity across sensitive groups without harming overall data utility.  
‚Ä¢ To empirically validate the method on several benchmark tabular datasets, comparing against state-of-the-art DP- and fairness-aware generators across utility, privacy, and fairness metrics.  

Significance  
PrivFairGen bridges the gap between high-fidelity LLM-based synthesis and the ethical imperatives of privacy and fairness. By developing an end-to-end framework that enforces constraints throughout training and sampling, we enable practitioners and data stewards in regulated domains to generate large-scale synthetic datasets that:  
‚Ä¢ Protect individual privacy with quantified DP guarantees (Œµ,Œ¥).  
‚Ä¢ Mitigate bias against protected subgroups.  
‚Ä¢ Maintain downstream ML performance comparable to non-private, non-fair generative baselines.  
This work promises to accelerate trustworthy ML adoption in domains where data sharing is currently infeasible.  

2. Methodology  
Overview  
PrivFairGen proceeds in three phases: (1) Tabular formatting and pseudo-data pre-training, (2) DP-SGD fine-tuning with fairness-aware loss, (3) Constraint-guided decoding for final data generation. Figure 1 (omitted) depicts the pipeline.  

2.1 Data Representation & Preprocessing  
‚Ä¢ Input data D_real consists of n records, each a vector x‚àà‚Ñù^d with m continuous features and k categorical features (including a sensitive attribute S‚àà{0,1}).  
‚Ä¢ We convert each record to a token sequence ‚Äúfield1:val1 | ‚Ä¶ | field_d:val_d‚Äù using delimiter tokens. Continuous features are quantized or represented via a fixed‚Äêprecision string. Categorical values map to tokens.  
‚Ä¢ Define a vocabulary V sufficiently large to encode all attribute names and values. Add special tokens <BOS>, <EOS>.  

2.2 Stage 1: Non-Private Pseudo-Data Pre-Training  
‚Ä¢ To accelerate adaptation, we construct a large pseudo‚Äêtabular corpus D_pseudo by sampling marginal distributions and feature pairwise copulas from D_real. D_pseudo contains N‚â´n synthetic records without privacy concerns.  
‚Ä¢ Fine-tune the pre-trained LLM (e.g. GPT-2 small) on D_pseudo with the negative log-likelihood objective:  
  $$  
  \mathcal{L}_{\mathrm{NLL}}(\theta) = -\frac{1}{N}\sum_{x\in D_{\mathrm{pseudo}}}\sum_{t=1}^{T}\log p_\theta(x_t\,|\,x_{<t})\,.  
  $$  
‚Ä¢ Use standard AdamW optimization for 1‚Äì2 epochs to teach the model tabular syntax.  

2.3 Stage 2: DP-SGD Fine-Tuning with Fairness Penalty  
We now fine-tune on D_real under DP constraints and fairness objectives.  

2.3.1 Differential Privacy Mechanism  
‚Ä¢ At each iteration t with mini‚Äêbatch B_t of size b, compute per‚Äêexample gradients g_i = ‚àá_Œ∏‚Ñì(Œ∏; x_i).  
‚Ä¢ Clip each gradient:  
  $$  
  \bar g_i = \frac{g_i}{\max\bigl(1,\tfrac{\|g_i\|_2}{C}\bigr)}\,,  
  $$  
  where C is the clipping norm.  
‚Ä¢ Aggregate and add noise:  
  $$  
  \tilde g_t = \frac{1}{b}\sum_{i\in B_t}\bar g_i \;+\;\mathcal{N}\bigl(0,\,\sigma^2C^2I\bigr)\,.  
  $$  
‚Ä¢ Update parameters: Œ∏_{t+1} = Œ∏_t ‚àíŒ∑\tilde g_t.  
‚Ä¢ Track the privacy loss (Œµ,Œ¥) using the moments accountant (Abadi et al., 2016).  

2.3.2 Fairness-Aware Loss  
We incorporate a penalty term targeting demographic parity:  
‚Ä¢ Let ≈∂ be the predicted label for a downstream classifier f trained on synthetic data; let S‚àà{0,1} be the sensitive attribute.  
‚Ä¢ Define demographic parity gap:  
  $$  
  \Delta_{DP}(\tilde D) = \Bigl|P_{x\sim\tilde D}[f(x)=1\mid S=0] - P_{x\sim\tilde D}[f(x)=1\mid S=1]\Bigr|\,.  
  $$  
‚Ä¢ Similarly, equalized odds gap Œî_EO measures difference in true positive rates across groups.  
‚Ä¢ During fine-tuning, approximate Œî_DP on the current batch by sampling k rows per group and computing a differentiable estimate using the model‚Äôs next‚Äêtoken probabilities or a small classifier head attached to the LLM.  
‚Ä¢ Total loss per batch:  
  $$  
  \mathcal{L}_t = \underbrace{\tfrac{1}{b}\sum_{i\in B_t}\mathcal{L}_{\mathrm{NLL}}(x_i)}_{\text{fidelity}} \;+\;\lambda_{\mathrm{fair}}\;\max\bigl(\widehat\Delta_{DP},\,\tau\bigr)\,,  
  $$  
  where Œª_fair controls the trade-off and œÑ is a small slack threshold.  
‚Ä¢ Backpropagate through this combined loss under the DP-SGD mechanism above.  

2.4 Stage 3: Constraint-Guided Decoding  
Once fine-tuning completes, we sample synthetic records via a customized decoding algorithm:  
1. At time step t, compute next‚Äêtoken logits ‚Ñì_t = f_Œ∏(x_{<t}).  
2. Add calibrated Gaussian noise to logits: ‚Ñì'_t = ‚Ñì_t + ùí©(0,œÉ_dec^2I) to further obscure memorized patterns.  
3. Perform top-k filtering, retaining the k most likely tokens.  
4. Sample a candidate continuation token x_t from the filtered distribution.  
5. If the partial sequence so far would, upon completion, violate a fairness constraint (e.g. generate too many records of S=1 relative to S=0), discard and resample up to R times.  
6. End when <EOS> is generated or maximum length reached.  

By combining noise injection and constrained sampling, we ensure the final dataset both upholds DP properties and meets the desired parity constraints.  

2.5 Experimental Design  
Datasets  
‚Ä¢ Adult Income (UCI), COMPAS recidivism, MIMIC-III clinical records (after de-identification), and a synthetic healthcare claims dataset.  
Metrics  
1. Utility  
  ‚Äì Classification accuracy, F1-score on a downstream target (e.g. income>50K, recidivism) for models trained on synthetic vs. real data.  
  ‚Äì Statistical similarity: Kolmogorov‚ÄìSmirnov distance for continuous features, Jaccard index for categorical marginals.  
2. Privacy  
  ‚Äì (Œµ,Œ¥) from the moments accountant.  
  ‚Äì Empirical membership inference attack accuracy on held-out real records.  
3. Fairness  
  ‚Äì Demographic parity difference Œî_DP, equalized odds difference Œî_EO on the synthetic dataset.  
Baselines  
‚Ä¢ DP-TBART, DP-LLMTGen, DP-2Stage, TableDiffusion, fairness-aware VAE, DP-GAN (White & Brown 2023).  
Experimental Protocol  
‚Ä¢ Vary privacy budgets Œµ‚àà{0.5,1,3,8} with Œ¥=10‚Åª‚Åµ.  
‚Ä¢ Vary fairness weights Œª_fair‚àà{0,0.1,1.0}.  
‚Ä¢ Perform 5 random splits for train/test and report mean¬±std.  
Implementation  
‚Ä¢ PyTorch + HuggingFace Transformers.  
‚Ä¢ Opacus for DP-SGD.  
‚Ä¢ Code and synthetic data released under an open-source license.  

3. Expected Outcomes & Impact  
We anticipate that PrivFairGen will deliver:  
‚Ä¢ High-utility synthetic tabular datasets whose downstream classification accuracy is within 5% of models trained on the real data, even under Œµ=1.  
‚Ä¢ Rigorous DP guarantees (e.g. (Œµ=1, Œ¥=10‚Åª‚Åµ)) validated by both theoretical accounting and empirical attacks showing near‚Äêrandom membership inference.  
‚Ä¢ Marked reduction in group bias, achieving Œî_DP<0.02 and Œî_EO<0.03, outperforming unconstrained DP generators by 30‚Äì50% on fairness metrics.  

Impact  
By unifying privacy and fairness in an LLM-based synthesis framework, PrivFairGen will:  
‚Ä¢ Enable researchers and institutions to share high-fidelity synthetic data without exposing sensitive records.  
‚Ä¢ Provide a reproducible baseline for integrating fairness constraints into generative AI.  
‚Ä¢ Facilitate the development of equitable ML systems in regulated domains, accelerating trust and adoption.  

In sum, PrivFairGen represents a significant step toward ethical, privacy-preserving, and unbiased synthetic data generation for tabular domains.