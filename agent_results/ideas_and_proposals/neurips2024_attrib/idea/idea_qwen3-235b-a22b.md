1. **Title**: Causal Tracing of Model Biases to Data Sources in Large-Scale Training  

2. **Motivation**:  
Training models on internet-scale data, including LLM-generated content, risks creating feedback loops where biases amplify over generations. Existing attribution methods struggle to efficiently disentangle how mixed data sources (e.g., authentic vs. synthetic) causally influence model behavior at scale. This gap limits our ability to govern data quality, mitigate emergent biases, and ensure robustness. Addressing this could clarify pathways to "data-aware" AI development.  

3. **Main Idea**:  
Propose a randomized influence analysis framework to trace model behaviors to data sources. Training uses a mixture of clean (e.g., human-generated) and contaminated data (e.g., LLM-generated outputs), with each sample tagged by source. During training, randomly perturb or exclude subsets of data sources, then estimate how these perturbations correlate with variations in bias metrics (e.g., stereotypical outputs) and capability scores (e.g., accuracy). Use causal mediation analysis to quantify the direct contribution of each source to downstream behaviors. Methodologically, leverage efficient randomized estimators (e.g., influence functions) and ablate sources in distributed batches to scale. Expected outcomes include (1) a scalable metric attributing model biases to data provenance and (2) guidelines for pruning harmful sources while preserving capabilities. This would enable proactive governance of training pipelines and reduce risks of positive feedback loops in future model generations.