**Title:** Scalable Fingerprinting and Impact Analysis of Synthetic Data Contamination

**Motivation:** As Large Language Models (LLMs) proliferate, their outputs increasingly contaminate the web data used for training subsequent models. This creates hidden feedback loops, risking "model collapse"â€”a degradation in diversity, creativity, and factual accuracy. Addressing this requires methods to detect synthetic data at scale and understand its influence.

**Main Idea:** We propose developing lightweight, scalable techniques to identify potential synthetic data within massive pre-training corpora. This involves creating "fingerprints" based on statistical artifacts (e.g., perplexity distributions against known models, token frequency anomalies, stylistic markers) rather than relying solely on intensive model-based detection. We will inject controlled amounts of synthetic data (from various known LLMs) into clean datasets and train models to systematically quantify the impact of contamination levels on downstream metrics like output diversity, benchmark performance, susceptibility to specific biases, and hallucination rates. The outcome will be a framework for estimating synthetic data prevalence and its causal effect on model behavior, informing strategies for data curation or filtering.