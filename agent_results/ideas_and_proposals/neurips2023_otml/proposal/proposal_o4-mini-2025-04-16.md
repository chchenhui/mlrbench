1. Title  
Adaptive Unbalanced Optimal Transport for Robust Domain Adaptation under Label Shift  

2. Introduction  
Background  
Domain adaptation seeks to transfer knowledge from a labeled source domain to an unlabeled (or sparsely labeled) target domain. Optimal transport (OT) has emerged as a powerful tool for domain adaptation by aligning source and target distributions in feature space. In its classical formulation, OT enforces massâ€conservation constraints, implicitly assuming that the marginal distributions (including classâ€conditional and label marginals) of source and target are identical or balanced. In practice, realâ€world domain shifts often involve label shiftsâ€”that is, changes in the targetâ€™s class proportions relative to the source. Standard OT is illâ€suited to handle such label shifts: it enforces the wrong marginal and thus may misalign samples across classes, leading to negative transfer or degraded performance.

Unbalanced OT (UOT) generalizes OT by relaxing the marginal constraints via divergence penalties, allowing source and target marginals to differ. However, existing UOTâ€based adaptation methods require the user to preâ€specify relaxation parameters that govern how much mass may be created or destroyed on each side. In the context of label shift, these relaxation parameters have a direct impact on the inferred target label proportions, yet in most situations the true shift is unknown and difficult to tune manually.

Research Objectives  
This proposal aims to develop an Adaptive Unbalanced Optimal Transport (A-UOT) framework that automatically learns the degree of marginal relaxation needed per class from data. We will integrate A-UOT into an end-to-end deep domain adaptation pipeline. Specifically, our objectives are:  
â€¢ Formulate a class-wise UOT problem with learnable relaxation parameters that capture labelâ€shift effects.  
â€¢ Embed this adaptive UOT layer within a deep feature extractor and classifier, optimizing jointly for feature alignment and parameter estimation.  
â€¢ Theoretically analyze the consistency and generalization properties of joint feature adaptation and relaxationâ€parameter learning.  
â€¢ Empirically demonstrate robustness and improved performance across benchmarks exhibiting label imbalance or shift.  

Significance  
A-UOT will relieve practitioners from laborious hyperparameter tuning of marginal relaxation and will enable domain adaptation methods that are robust to unknown label shifts. This is critical for applications in medical imaging, natural language processing, and other highâ€stakes domains where class proportions often vary in deployment. By learning the relaxation parameters, the model implicitly infers target label proportions, offering insights into the nature of the domain shift itself.

3. Methodology  

3.1 Problem Setup  
Let ğ’Ÿâ‚›={ (xáµ¢Ë¢,yáµ¢Ë¢) }_{i=1}^{nâ‚›} denote the source dataset with feature vectors xáµ¢Ë¢âˆˆâ„áµˆ and labels yáµ¢Ë¢âˆˆ{1,â€¦,C}. Let ğ’Ÿâ‚œ={ xâ±¼áµ— }_{j=1}^{nâ‚œ} be the unlabeled target dataset. Denote by Î¼â‚› and Î¼â‚œ the empirical feature distributions in the source and target domains over â„áµˆ. Under label shift, the target label marginal b=(bâ‚,â€¦,b_C), b_c=Pâ‚œ(y=c), differs from the source marginal a=(aâ‚,â€¦,a_C), a_c=Pâ‚›(y=c). We seek a coupling Î³âˆˆâ„_{+}^{nâ‚›Ã—nâ‚œ} aligning Î¼â‚› to Î¼â‚œ while adapting for label mass differences across classes.

3.2 Adaptive Unbalanced OT Formulation  
We adopt an entropic UOT formulation with divergence penalties on the marginals and introduce classâ€wise relaxation parameters Ï„=(Ï„â‚,â€¦,Ï„_C):  

Block equation start  
$$  
\min_{Î³\in\mathbb R_{+}^{nâ‚›\times nâ‚œ}}  
\;  
\langle Î³,\,C\rangle  
\;+\;\sum_{c=1}^{C}  
\bigl[  
Ï„_c \,D_{\mathrm{KL}}(Î³\,1_{nâ‚œ}\bigm\|a_c\,e_{c}^{(â‚›)})  
\;+\;  
Ï„_c \,D_{\mathrm{KL}}(Î³^{\top}1_{nâ‚›}\bigm\|b_c\,e_{c}^{(â‚œ)})  
\bigr]  
\;-\;  
Îµ\,H(Î³)\,.  
$$  
Block equation end  

Here:  
â€¢ C_{ij} = âˆ¥f_Î¸(xáµ¢Ë¢) âˆ’ f_Î¸(xâ±¼áµ—)âˆ¥Â²â‚‚ is the squaredâ€Euclidean cost in the learned feature space f_Î¸:â„áµˆâ†’â„áµ.  
â€¢ D_{KL}(pâ€–q)=âˆ‘_i p_iâ€‰log(p_i/q_i) is the Kullbackâ€“Leibler divergence.  
â€¢ a_câ€‰e_c^{(â‚›)} is a degenerate distribution placing mass a_c on source samples of class c (similarly for b_câ€‰e_c^{(â‚œ)} on target pseudoâ€class c).  
â€¢ H(Î³)=âˆ’âˆ‘_{i,j}Î³_{ij}logâ€‰Î³_{ij} is the entropy.  
â€¢ Îµ>0 is the entropic regularization weight.  
â€¢ Ï„_c>0 (c=1â€¦C) are learned relaxation weights controlling the allowed classâ€wise mass variation.  

By making Ï„ learnable, the coupling Î³ can create or destroy mass per class in accordance with the true (unknown) shift, rather than a single global slack parameter.

3.3 End-to-End Learning  
We integrate the above A-UOT layer into a deep network comprising a feature extractor f_Î¸ and classifier g_Ï†. The joint objective is:  

Inline equation  
$  
\mathcal{L}(Î¸,Ï†,Ï„)  
=  
\underbrace{\frac{1}{nâ‚›}\sum_{i=1}^{nâ‚›}\ell_{\mathrm{CE}}\bigl(g_Ï†(f_Î¸(xáµ¢Ë¢)),yáµ¢Ë¢\bigr)}_{\text{source classification}}  
\;+\;  
\lambda_{\mathrm{OT}}\;\mathcal{L}_{\mathrm{A\textâ€UOT}}(Î¸,Ï„)  
\;+\;  
\lambda_{\mathrm{ent}}\;\mathcal{L}_{\mathrm{ent}}(Î¸,Ï†)  
\;+\;  
\lambda_{Ï„}\,\|Ï„\|_{2}^{2}\,.  
$  

Components:  
1. Source classification loss: standard crossâ€entropy â„“_{CE}.  
2. A-UOT loss: the optimal value of the UOT problem defined above, parameterized by Î¸ and Ï„. We denote this value by ğ“›_{A-UOT}(Î¸,Ï„).  
3. Entropy minimization on target classifier outputs:  
Inline equation  
$  
\mathcal{L}_{\mathrm{ent}}  
=  
-\frac{1}{nâ‚œ}\sum_{j=1}^{nâ‚œ}\sum_{c=1}^{C}p_{jc}\log p_{jc},  
\quad  
p_{jc}= \bigl[g_Ï†(f_Î¸(xâ±¼áµ—))\bigr]_{c}.  
$  
This encourages confident predictions on target.  
4. Regularization on Ï„ to avoid degenerate solutions.

Optimization proceeds by alternating (or jointly) updating Î¸, Ï†, and Ï„ via stochastic gradient descent. The A-UOT loss is computed via a differentiable Sinkhornâ€like algorithm with classâ€wise KL penalties. Gradients âˆ‚ğ“›_{A-UOT}/âˆ‚Ï„ are obtained by automatic differentiation through the Sinkhorn iterations or via implicit differentiation of the optimality conditions.

3.4 Algorithmic Details  
1. Parameterization of Ï„: we set Ï„_c=exp(Î±_c) to ensure positivity, and learn Î±âˆˆâ„^C.  
2. Mini-batch UOT: at each iteration we sample miniâ€batches of source and target samples. We estimate source marginals aÌ‚_c from true labels and target marginals bÌ‚_c from classifier probabilities p_{jc}.  
3. Sinkhorn with classâ€wise margins: we build cost matrix C and perform the generalized Sinkhornâ€“Knopp scaling:  

Block equation  
$$  
u \leftarrow \frac{ aÌ‚}{K\,v},  
\quad  
v \leftarrow \frac{ bÌ‚}{K^{\top}\,u},  
\quad  
K_{ij} = \exp\bigl(-C_{ij}/Îµ\bigr).  
$$  
Here aÌ‚âˆˆâ„^{batchâ‚›} collects weighted source marginals (duplicating Ï„_c factors per sample).  
4. Convergence criterion: a fixed number T of Sinkhorn iterations (e.g. T=50) or until marginals match within tolerance.  
5. Complexity: each Sinkhorn iteration costs O(mâ€‰n) per miniâ€batch (m source, n target). Learning Ï„ adds O(C) overhead.  

3.5 Theoretical Analysis  
We will analyze the following aspects:  
â€¢ Consistency of Ï„â€estimation: under mild assumptions on feature separation, the learned Ï„ converges to values proportional to true targetâ€source labelâ€ratio b_c/a_c.  
â€¢ Generalization bound: extending the domain adaptation bound  
Block equation  
$$  
Îµâ‚œ(h)  
\le  
Îµâ‚›(h)  
+  
W_{A\textâ€UOT}(Pâ‚›^f,Pâ‚œ^f;\tau)  
+  
\Lambda( h ),  
$$  
where Îµâ‚›,Îµâ‚œ are source/target risk; W_{A-UOT} is the optimal transport cost with learned Ï„, and Î›(h) is the joint optimal risk on the two domains.  
We will show that adapting Ï„ tightens the bound in the presence of label shift.

3.6 Experimental Design  
Datasets  
â€¢ Officeâ€31, Officeâ€Home, VisDAâ€2017: classical domain adaptation benchmarks. We will induce synthetic label shift by reâ€sampling target classes (e.g. reduce some classes to 10% frequency).  
â€¢ Digits (MNISTâ†’USPS, SVHNâ†’MNIST): model lowâ€toâ€low domain shifts with class imbalance.  
â€¢ Realistic medical imaging tasks (e.g. chest X-ray classification with varying disease prevalence).  
Baselines  
â€¢ Standard OTDA (Ganin et al. 2016; Courty et al. 2017).  
â€¢ Unbalanced OT with fixed relaxation (Fatras et al. 2021).  
â€¢ Importanceâ€weighted OT (Rakotomamonjy et al. 2020).  
â€¢ MixUp + OT methods (\textsc{mixunbot}, Fatras et al. 2022).  
Metrics  
â€¢ Target classification accuracy and perâ€class accuracy.  
â€¢ Hâ€score: harmonic mean between source and target accuracies.  
â€¢ Labelâ€proportion estimation error: MSE between estimated bÌ‚ and true b.  
â€¢ Computational cost: runtime per epoch.  
Ablations  
â€¢ Compare classâ€wise learnable Ï„ vs single global Ï„.  
â€¢ Impact of entropy loss weight Î»_{ent} and Ï„â€regularization Î»_Ï„.  
â€¢ Sensitivity to Sinkhorn regularization Îµ.  

4. Expected Outcomes & Impact  
Expected Outcomes  
â€¢ A novel A-UOT algorithm that automatically adapts marginalâ€relaxation weights per class, leading to improved domain adaptation under label shifts.  
â€¢ Theoretical guarantees on consistency of Ï„â€learning and tighter generalization bounds.  
â€¢ A publicly released PyTorch implementation, enabling reproducibility and adoption.  
â€¢ Empirical evidence of performance gains (5â€“10% absolute improvement in target accuracy) over fixedâ€parameter UOT and other stateâ€ofâ€theâ€art methods, especially in highâ€imbalance regimes.  
Scientific Impact  
â€¢ Provides a principled way to handle unknown label shift in OTâ€based adaptation, filling a gap in the literature.  
â€¢ Advances understanding of how marginal relaxation interacts with feature learning in deep models.  
Practical Impact  
â€¢ Facilitates robust domain adaptation in medical diagnostics, remote sensing, and NLP tasks where class prevalence may vary unpredictably.  
â€¢ Reduces hyperparameter tuning burden for practitioners by learning relaxation automatically.  
Broader Impacts  
â€¢ Encourages fairer models by compensating for underâ€represented classes.  
â€¢ Opens a pathway to joint estimation of domain shift statistics and adaptation, with applications beyond classification (e.g. object detection, segmentation).  

In summary, this proposal outlines a comprehensive plan to develop, analyze, and validate an Adaptive Unbalanced Optimal Transport framework that learns how much mass to create or destroy per class, thereby automatically correcting for label shifts in domain adaptation. The combination of theoretical analysis, algorithmic innovation, and extensive empirical validation will advance both the theory and practice of OT in machine learning.