**Title:** Foundation Models as Interpretable Behavioral Engines for Agent-Based Social Simulations

**Motivation:** Understanding complex social phenomena requires realistic models of human behavior. Traditional agent-based models (ABMs) often use simplistic rules, while powerful foundation models (FMs) can generate human-like behavior but typically lack interpretability within simulation contexts. This research aims to leverage FMs for more realistic agent behavior in ABMs while ensuring the decision-making process remains interpretable.

**Main Idea:** We propose integrating pre-trained FMs (e.g., LLMs) as the cognitive core of agents within ABMs simulating societal dynamics (like opinion formation or market behavior). Each agent, characterized by a profile (beliefs, goals, demographics), interacts with its environment and other agents. Its decisions are generated by querying the FM, prompted with the agent's state and context. Crucially, we will employ techniques like constitutional AI principles or chain-of-thought prompting during generation, forcing the FM to explicitly output a structured, interpretable rationale alongside the action. This allows researchers not only to observe emergent macroscopic patterns but also to trace them back to interpretable micro-level decisions, enhancing model validation and understanding of algorithm-human-society interactions.