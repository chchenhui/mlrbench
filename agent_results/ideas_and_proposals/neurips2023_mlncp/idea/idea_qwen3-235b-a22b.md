**Title:** Physics-Informed Neural Architectures for Robust Training on Noisy Analog Hardware  

**Motivation:**  
Analog and neuromorphic accelerators offer energy-efficient alternatives to digital compute but are limited by inherent noise, device mismatch, and low precision. Most ML models, optimized for deterministic hardware, fail in these environments. Developing algorithms that not only tolerate but actively exploit these physical properties could unlock scalable, sustainable AI, particularly for energy-intensive tasks like training large models.  

**Main Idea:**  
Propose a hybrid training paradigm where neural networks are co-designed with analog hardware constraints by embedding physical noise models into the forward and backward passes during training. Introduce "stochastic residual layers" that adaptively model hardware noise as probabilistic perturbations, allowing gradients to propagate through noise-aware pathways. Combine this with a physics-informed loss term that regularizes weight updates to match hardware-achievable dynamics (e.g., asymmetric activation functions, limited bit-depth). Train these models on physical hardware in the loop or via differentiable surrogate models of analog accelerators. Expected outcomes include models that (1) achieve comparable accuracy to digital baselines at lower precision and (2) exhibit resilience to hardware non-idealities without post-training quantization. This could enable efficient training of emerging model classes like energy-based models on analog accelerators, where noise itself could serve as a free source of regularization. Impact includes reducing reliance on high-precision computation and aiding the deployment of compute-hungry generative AI in low-power edge devices.