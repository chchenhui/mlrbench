**Title:** Action-Aware Pretraining for Foundation Models via Inverse Dynamics Prediction

**Motivation:** Foundation models (FMs) excel on passive data but lack grounding in interaction dynamics crucial for decision-making tasks, as their pretraining datasets typically lack explicit action labels. This hinders their direct application in robotics, embodied AI, and interactive systems where understanding cause-and-effect is paramount.

**Main Idea:** We propose pretraining FMs (specifically Vision-Language Models) not just on observation data (text, images, video), but also on action prediction. We will leverage large-scale unlabeled video datasets (e.g., Ego4D, Something-Something) and apply an inverse dynamics model (trained separately or jointly) to predict the likely action `a_t` that caused the transition from state `s_t` to `s_{t+1}`. The FM will then be pretrained using a masked prediction objective: predict the inferred action `a_t` given the surrounding visual context (`s_t`, `s_{t+1}`). This implicitly teaches the model about physical interactions and affordances. We expect this action-aware pretraining to yield models that require significantly less task-specific fine-tuning for downstream control and planning tasks, improving few-shot adaptation and sample efficiency in decision-making domains.