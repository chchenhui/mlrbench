**Title:** Adversarial Counterfactual Augmentation for Spurious Correlation Robustness

**Motivation:** Machine learning models often fail when deployed because they exploit spurious correlations present in the training data (e.g., background features in images, lexical overlap in NLP) instead of learning the underlying causal mechanisms. Existing methods often require group labels (which are scarce) or struggle with complex, unknown spurious features.

**Main Idea:** We propose an *Adversarial Counterfactual Augmentation* (ACA) framework. Given a model trained on the original data, we first identify potentially spurious features using influence functions or gradient-based attribution methods. Then, a conditional generative model (e.g., a CycleGAN or diffusion model conditioned on masks/attributes) is trained to create "counterfactual" examples by modifying only these identified spurious features while preserving the true label. Finally, the original task model is retrained using both original data and these generated counterfactuals, incorporating a consistency loss to encourage similar predictions for original-counterfactual pairs. This forces the model to become invariant to the identified spurious features, leading to improved OOD generalization and stability without requiring explicit group labels.