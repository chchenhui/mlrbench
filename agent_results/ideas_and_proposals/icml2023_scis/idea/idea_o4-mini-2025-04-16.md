Title: Adaptive Counterfactual Stress Testing for Spurious Correlation Discovery

Motivation:  
Machine-learning models often rely on irrelevant “shortcut” features—e.g., X-ray scanner marks or lexical overlap—leading to failure under realistic distribution shifts. A systematic, automated stress-testing framework is needed to uncover and quantify these hidden dependencies before deployment.

Main Idea:  
We propose an adaptive counterfactual stress tester that (1) learns a structured perturbation graph over input features using generative models; (2) systematically generates counterfactual variants by perturbing candidate nuisance features or their combinations; and (3) measures the resulting prediction drift to score spurious dependencies. The perturbation graph is built incrementally: at each node, we select the feature or feature-subset whose manipulation maximizes model instability, guiding deeper exploration only where spurious effects are strongest. Once identified, we integrate an invariance-regularizer during retraining to penalize sensitivity to those features. We will validate on chest X-rays (scanner artifacts vs. pathology) and NLI benchmarks (lexical overlap vs. semantic entailment). Expected outcomes include an open-source stress-testing suite that outputs a ranked list of spurious features, and a demonstration that invariance-regularized retraining restores robustness. Potential impact spans domains: from medical imaging to NLP, empowering practitioners to detect and mitigate hidden shortcuts in their models.