**Title:** Spatio-Temporal Grounding of LLMs via Continuous Embodied Perception in Urban Environments

**Motivation:** LLM agents struggle with dynamic spatial awareness in open cities. Their understanding of concepts like "nearby," "recently passed," or relative object motion is often abstract and disconnected from continuous real-world perception, hindering navigation and task execution.

**Main Idea:** We propose a framework to continuously ground LLM spatial and temporal reasoning in embodied sensor streams (e.g., video, LiDAR). A perception module extracts dynamic objects, landmarks, and self-motion estimates. These are translated into a structured spatio-temporal representation (e.g., a dynamic scene graph with timestamps and relative coordinates). This representation is periodically integrated into the LLM's context, potentially via fine-tuning or specialized input tokens. The LLM learns to correlate its language-based spatial concepts (e.g., "turn left at the *next* intersection," "the bus *just* passed") with features in this grounded representation. Expected outcomes include improved instruction following involving dynamic elements, better spatial query answering ("What did we pass 1 minute ago?"), and enhanced situational awareness for agents navigating complex cityscapes.