Title: GeoLLM: Integrating Geospatial Priors and Dynamic Perception for LLM Agents’ Spatial Intelligence in Open City Environments

Motivation: Large Language Model (LLM) agents struggle with spatial awareness and real-time perception in dynamic, unstructured city environments, limiting their abilities in navigation, urban search, and context-aware planning. Existing LLM-based methods lack robust models for spatial intelligence, relying on static text or indoor priors. Addressing these gaps could unlock applications in smart mobility, urban robotics, and interactive city-scale assistive systems.

Main Idea: We propose “GeoLLM”—a novel framework that imbues LLM agents with spatial intelligence by integrating three key modalities: (i) pre-learned geospatial priors (e.g., open city maps, semantic city zoning, multimodal satellite and street-level imagery); (ii) embodied sensor streams (visual, audio, GPS, IMU) for real-time perception; and (iii) spatial memory modules that let the agent build, update, and reason over dynamic mental maps as it explores. The GeoLLM pipeline fuses sensory embeddings into LLM attention layers, enabling context-aware dialogue, instruction-following, and multimodal reasoning grounded in the city landscape. Evaluation will rely on benchmarks in open city navigation, landmark localization, and spatial reasoning tasks. This approach will advance the spatial intelligence of embodied LLM agents, creating foundations for robust, interactive city-scale AI.