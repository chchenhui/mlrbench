Title: CitySense: Enhancing Spatial Intelligence and Embodied Perception in Urban LLM Agents

Motivation:  
Urban environments present dynamic challenges with complex spatial layouts, diverse sensory inputs, and unpredictable events. Conventional LLMs, primarily trained on text, lack the ability to interpret and navigate these environments. Improving embodied perception and spatial intelligence in LLM agents can bridge this gap, enabling efficient navigation, task planning, and decision-making in open city environments.

Main Idea:  
CitySense proposes an integrated framework that combines large language models with sensory modules and spatial reasoning algorithms. The approach involves training a multi-modal LLM using simulated urban datasets enriched with spatial and temporal annotations. This training is complemented by reinforcement learning to calibrate real-time decision-making under dynamic urban scenarios. By incorporating sensor fusion techniques (e.g., LiDAR, camera feeds, GPS data) and spatial mapping, CitySense aims to enhance the agent’s embodied perception. Expected outcomes include improved navigation, robust search and rescue strategies, and adaptive task planning. The framework’s modular structure allows iterative refinement and benchmarking against real-world urban tasks, potentially paving the way for advanced applications in autonomous urban robotics and human-agent collaborative systems.