**Title:** Grounding In-Context Learning Theory in Practical Prompt Engineering Heuristics

**Motivation:** Theoretical models of In-Context Learning (ICL) struggle to explain the profound sensitivity of Large Language Models (LLMs) to subtle variations in promptsâ€”a cornerstone of practical prompt engineering. Bridging this gap is crucial for developing both more accurate theories and more reliable, principled prompting strategies, moving beyond empirical trial-and-error.

**Main Idea:** This research proposes to systematically connect established ICL theories (e.g., models viewing ICL as implicit fine-tuning, specific attention head roles) to the empirical effectiveness of known prompt engineering heuristics. We will conduct controlled experiments where prompt elements (e.g., demonstration order, formatting, instruction phrasing) are systematically varied. By analyzing the corresponding changes in internal LLM representations (attention patterns, layer activations) and correlating these with theoretically predicted mechanisms and downstream task performance, we aim to validate, refine, or challenge existing ICL theories. The expected outcome is a framework that better explains *why* certain prompting techniques succeed, enabling more theoretically-informed and effective prompt design.