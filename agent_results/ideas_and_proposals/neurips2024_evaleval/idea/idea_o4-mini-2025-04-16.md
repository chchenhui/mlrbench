Title: CoEval – A Collaborative Multi-Stakeholder Framework for Assessing Generative AI’s Societal Impact

Motivation:
Current impact assessments of generative AI are ad hoc, expert-centric, and lack standardized protocols. CoEval aims to bridge this gap by weaving evaluation science with participatory methods so that developers, end-users, domain experts, and policymakers co-design and conduct impact evaluations. This ensures underrepresented perspectives shape AI accountability and drives broader community adoption of best practices.

Main Idea:
CoEval introduces a three-phase, open-source framework:
1. Co-Design Workshops: Convene diverse stakeholders to co-create context-specific impact criteria, using structured facilitation and card-sorting to prioritize social, ethical, and economic dimensions.  
2. Mixed-Methods Toolkit: Offer modular survey instruments, focus-group scripts, scenario simulations, and lightweight computational metrics (e.g., bias amplification scores).  
3. Living Repository & Policy Templates: Publish evaluation protocols, anonymized pilot data, and model policy briefs on a public platform.  

We will pilot CoEval across three generative AI domains (text, vision, audio), iteratively refine metrics via user feedback, and distill a standardized guideline. Expected outcomes include validated participatory metrics, an open toolkit, and community-endorsed policy recommendations—empowering inclusive, reproducible, and transparent impact evaluations.