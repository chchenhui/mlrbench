**Title:** Context-Aware Activation Interventions via Adversarial Latent Modulation  

**Motivation:** While activation engineering shows promise for controlling harmful outputs, current approaches apply static interventions that lack adaptability to input context, risking under-suppression of toxicity or over-correction that degrades utility. Dynamic, input-sensitive intervention is critical for real-world deployment where harm manifests unpredictably.  

**Main Idea:** Develop a lightweight adversarial network that predicts *input-conditioned* activation edits to suppress harmful content while preserving general capabilities. The adversarial modulator (1) analyzes intermediate activations to estimate toxicity risk, (2) generates sparse, low-rank offset vectors applied to critical transformer layers, and (3) is trained via contrastive learning: minimize harm in target outputs while reconstructing benign reference outputs from similar inputs. This enables context-aware steering without full model fine-tuning. Expected outcomes: a parameter-efficient (<0.1% added parameters) method that outperforms static intervention baselines in toxicity reduction (measured via ToxiGen benchmark) while maintaining >95% of original model accuracy on downstream tasks.