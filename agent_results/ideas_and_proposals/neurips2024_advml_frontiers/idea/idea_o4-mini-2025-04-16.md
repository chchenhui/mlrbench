Title: LMM-Driven Hard Negative Mining for Cross-Modal Robustness

Motivation: Large multimodal models (LMMs) suffer from subtle cross-modal perturbations that escape standard adversarial training. Generating diverse, semantically challenging negative samples by hand or brute-force search is costly and scales poorly.

Main Idea: We propose a closed-loop framework in which an LMM is prompted to synthesize “hard negatives”—image–text pairs that maximize confusion in a target multimodal classifier. 1) Adversarial Generation: Given a seed image and caption, the LMM is prompted (via few-shot or chain-of-thought examples) to produce minimal semantic or visual edits that cause the classifier to misalign modalities. 2) Reinforced Selection: A lightweight reward model scores generated pairs by attack success plus diversity; highest-reward examples become training data. 3) Adversarial Training: The target model is fine-tuned on both original and LMM-synthesized negatives, improving its decision boundary. Expected outcomes include marked gains in robustness under cross-modal PGD and real-world perturbations. This approach automates hard negative mining, reduces annotation burden, and lays groundwork for self-improving adversarial defenses in multimodal AI.