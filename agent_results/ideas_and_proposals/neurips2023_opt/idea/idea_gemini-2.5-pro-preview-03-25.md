**Title:** Budget-Constrained Neural Architecture and Optimizer Co-Search via Extrapolated Scaling Laws

**Motivation:** Training large models like LLMs is computationally expensive. Determining the optimal model architecture (width, depth) and optimizer hyperparameters (learning rate, batch size) under a fixed compute budget is critical for efficiency, yet challenging due to the vast search space and high evaluation cost. This research aims to develop a cost-effective method for finding near-optimal configurations.

**Main Idea:** We propose an algorithm that leverages scaling laws derived from cheaper, smaller model training runs to guide the search for optimal configurations under a strict compute budget. The method involves: 1) Training a diverse set of smaller model variants with different architectures and optimizer settings for a limited number of steps. 2) Fitting empirical scaling laws (loss vs. compute) parameterized by architecture and optimizer choices. 3) Using these extrapolated scaling laws as a cheap proxy function within a budget-aware search algorithm (e.g., Bayesian Optimization or evolutionary algorithm) to predict the final performance of larger-scale configurations. This allows efficient exploration of the configuration space, predicting which combination of architecture and optimizer settings will likely yield the lowest loss for the target compute budget without needing full-scale training for every candidate. Expected outcome is a significantly accelerated method for identifying resource-efficient training configurations.