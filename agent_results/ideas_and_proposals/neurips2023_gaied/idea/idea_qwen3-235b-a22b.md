**Title**  
Affective GenAI Tutor: Integrating Real-Time Multimodal Affective Feedback into Generative AI for Personalized Learning  

**Motivation**  
Generative AI tutors often prioritize cognitive aspects of learning (e.g., knowledge gaps) but neglect students’ emotional states (e.g., frustration, disengagement), which critically affect learning outcomes. Current systems fail to dynamically adapt to emotions, leading to suboptimal engagement. This gap highlights the need for AI tutors that combine cognitive and affective alignment to foster motivation and retention in personalized education.  

**Main Idea**  
Propose a framework to build emotionally intelligent generative AI tutors that adapt content using real-time multimodal affective feedback (e.g., facial expressions, vocal tone, posture). The system will:  
1) Use multimodal transformers (e.g., Vision Transformers, audio encoders) to infer affective states from live student data.  
2) Feed these insights into a prompting/fine-tuning workflow for large language models (LLMs), redirecting pedagogical strategies (e.g., switching to encouragement, analogies, or breaks).  
3) Validate the approach through user studies measuring engagement, learning gains, and perceived empathy in K-12 and higher education settings.  
Expected outcomes include improved student satisfaction and performance while setting a precedent for ethical affective AI design. This work merges GAI→ED and ED→GAI thrusts by enhancing educational efficacy while addressing risks like over-reliance on AI or privacy concerns through transparent, opt-in data protocols.