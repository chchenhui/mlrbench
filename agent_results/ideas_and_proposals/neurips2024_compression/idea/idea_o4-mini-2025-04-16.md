Title: IB-NeRF: Information Bottleneck–Driven Neural Radiance Field Compression

Motivation: Neural Radiance Fields (NeRFs) achieve photorealistic novel-view synthesis but incur large memory and computation costs, limiting deployment in real-time or resource-constrained settings. We need principled compression methods that preserve rendering quality while offering theoretical performance guarantees.

Main Idea: We integrate the information bottleneck (IB) principle into NeRF training to learn compact, task-relevant representations. Concretely, at each MLP layer we introduce a variational IB term β·I(Z;X) that penalizes mutual information between layer activations Z and inputs X (3D coordinates + view direction), encouraging minimal sufficient encodings. Our overall loss combines reconstruction error (rendering MSE) with the IB penalty. After training, we estimate each parameter’s information contribution via local Fisher information and prune low-information weights. The remaining weights are quantized using a learned codebook and entropy-encoded. We derive rate–distortion bounds from the IB framework, providing theoretical guarantees on compression versus rendering quality. Empirical results predict ≈5× model size reduction with <1 dB PSNR loss and real-time rendering, enabling scalable NeRF deployment in AR/VR and mobile robotics.