Title: Noise-Resilient Deep Equilibrium Models via In-Situ Analog Training

Motivation:  
Resistive analog crossbar arrays promise orders-of-magnitude gains in energy efficiency but suffer from device noise, mismatch and reduced bit-depth. Deep equilibrium models (DEQs) offer expressive capacity without deep stacking but are notoriously sensitive to hardware perturbations. Bridging DEQs with analog accelerators calls for training algorithms that embrace—and even exploit—physical noise to ensure robust, sustainable large-scale ML.

Main Idea:  
We propose a co-design of DEQs with analog hardware-in-the-loop noise injection and adaptive precision control. During training, forward solves of the equilibrium equation are performed directly on analog substrates or via high-fidelity noise simulators, exposing the model to realistic perturbations. Implicit differentiation is augmented by stochastic perturbation regularization and Hessian-based preconditioning to stabilize gradient estimates under noise. An adaptive solver dynamically adjusts iteration counts and precision per layer to balance convergence speed with error tolerance. We will validate the approach on classification and generative tasks using memristive crossbar prototypes, demonstrating robust inference and training with up to 10× lower energy consumption compared to digital baselines.