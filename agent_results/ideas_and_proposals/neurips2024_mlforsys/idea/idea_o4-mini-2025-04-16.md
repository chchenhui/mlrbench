Title: GreenRL-Scheduler: Reinforcement Learning for Carbon-Aware Cloud Workload Management

Motivation:  
Cloud datacenters account for a substantial share of global energy use and carbon emissions. Existing schedulers optimize for performance or cost but rarely adapt to real-time carbon intensity signals or dynamic workload patterns. A learning-based, carbon-aware approach can reduce emissions without sacrificing Service Level Agreements (SLAs).

Main Idea:  
We propose GreenRL-Scheduler, a multi-agent reinforcement learning (RL) framework that jointly assigns jobs, controls server power states (DVFS and sleep modes), and consolidates workloads based on real-time carbon intensity forecasts and workload forecasts. Each agent represents a rack or cluster; their policies are parameterized by Graph Neural Networks (GNNs) capturing topology and resource constraints. The state includes pending jobs’ profiles, historic power usage, and regional carbon predictions. We train with multi-objective Proximal Policy Optimization (PPO), using a reward combining energy consumption, carbon footprint, and SLA adherence, with constraints to bound latency. Offline training uses historical traces and simulated carbon grids; online deployment continuously fine-tunes as forecasts arrive. We expect 15–25% reductions in carbon emissions and 10–15% energy savings with negligible SLA violations, enabling more sustainable cloud operations.