# Dynamic Human Feedback Modeling: Capturing Context and Cognitive States

## Motivation
Human feedback is central to AI alignment strategies like RLHF, yet current models fail to account for the dynamic, context-dependent nature of human responses. Humans provide feedback differently based on their cognitive state (tired vs. alert), emotional context, and evolving knowledge. This oversimplification leads to AI systems that misinterpret human preferences and potentially amplify biases. We need more sophisticated models that capture the temporal and situational variability of human feedback to build truly aligned AI systems.

## Main Idea
This research proposes a dynamic feedback modeling framework that represents human feedback not as static signals but as outputs of a context-aware system with internal cognitive states. We'll develop a Bayesian approach that maintains probabilistic beliefs about a human's current cognitive state (attention level, expertise, cognitive load) and how these states evolve throughout interaction sessions. The framework will use a combination of explicit feedback signals and implicit behavioral cues (response times, consistency patterns, correction behaviors) to continuously update its model of the human's feedback-generating process. By treating human feedback as coming from a non-stationary distribution, AI systems can appropriately weight and interpret feedback based on contextual factors, leading to more accurate preference learning and better alignment with true human intent rather than captured artifacts of interaction contexts.