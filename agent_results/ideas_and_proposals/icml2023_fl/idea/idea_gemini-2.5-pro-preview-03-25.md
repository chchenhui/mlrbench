**Title:** FedPEFT: Parameter-Efficient Federated Fine-Tuning for Foundation Models on Heterogeneous Devices

**Motivation:** Deploying large Foundation Models (FMs) in federated learning (FL) is challenging. Their immense size prohibits full model fine-tuning on resource-constrained client devices due to prohibitive communication and computation costs. Centralized fine-tuning, conversely, compromises user data privacy, undermining a core principle of FL.

**Main Idea:** We propose FedPEFT, a framework adapting Parameter-Efficient Fine-Tuning (PEFT) techniques (e.g., LoRA, Adapters) for the federated setting. Instead of transmitting entire models or gradients, clients train and communicate only small, task-specific PEFT modules. This drastically reduces communication overhead. The research will explore adaptive PEFT module allocation based on client device capabilities (computation, memory) and data characteristics, alongside novel aggregation strategies tailored for sparse, low-rank PEFT updates. Expected outcomes include enabling efficient, privacy-preserving personalization of large FMs on diverse edge devices and demonstrating improved model utility with significantly reduced communication costs compared to traditional federated fine-tuning approaches.