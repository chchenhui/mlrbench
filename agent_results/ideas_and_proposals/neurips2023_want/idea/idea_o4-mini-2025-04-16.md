Title: MetaSched – Meta-Learned Scheduling for Heterogeneous Neural Network Training

Motivation:  
Modern large-scale models often run on clusters combining GPUs, CPUs and specialized accelerators. Static heuristics for task placement, communication and checkpointing lead to resource under-utilization, longer training times and higher energy costs—barriers especially for smaller teams lacking bespoke HPC expertise.

Main Idea:  
We propose MetaSched, a meta-learning framework that automatically learns scheduling policies conditioned on a model’s computation graph and the cluster’s hardware/network topology. First, we encode the training DAG and resource graph into a joint representation processed by a Graph Neural Network. The GNN outputs a policy over actions—layer-to-device assignment, pipeline ordering, activation checkpoint points, and communication routing. Using meta-reinforcement learning across a corpus of models and infrastructure scenarios, MetaSched acquires transferable priors and can be fine-tuned on new setups with minimal data. Integrated into existing deep-learning frameworks, our method is expected to reduce end-to-end training time by 20–30% and energy consumption by 15–25%, democratizing efficient large-scale training for diverse research teams.