**Title:** Geodesic Diffusion Models for Generative Modeling on Riemannian Manifolds

**Motivation:** Many real-world datasets, like molecular structures, shapes, or brain connectivity data, naturally reside on non-Euclidean manifolds. Standard diffusion models designed for Euclidean space struggle to capture the intrinsic geometry, potentially generating invalid or low-quality samples when adapted naively. Preserving the manifold structure during the generative process is crucial for meaningful results.

**Main Idea:** We propose Geodesic Diffusion Models (GDMs), where both the forward noising process and the reverse denoising process operate intrinsically on a known Riemannian manifold. The forward process simulates diffusion along manifold geodesics using stochastic differential equations (SDEs) grounded in Riemannian geometry (e.g., using the Laplace-Beltrami operator). The reverse process learns a time-dependent score function (gradient of the log-density) represented by a neural network designed to operate on manifold data (e.g., using equivariant layers or manifold convolutions) and predict updates along tangent spaces, which are then mapped back to the manifold via the exponential map. This ensures generated samples always respect the manifold constraints, leading to higher-quality, geometrically valid generations for complex non-Euclidean data.