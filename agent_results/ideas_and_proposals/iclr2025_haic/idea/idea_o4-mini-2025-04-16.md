Title: Coevolutionary Evaluation Framework: Capturing Mutual Adaptation in Long-Term Human-AI Interactions

Motivation: Traditional AI benchmarks focus exclusively on model performance and ignore the ways prolonged interactions reshape both AI behaviors and human cognition, trust, and decision-making. Without coevolution-sensitive metrics, aligning AI with evolving human values remains ad hoc and fragile.

Main Idea: We propose a unified evaluation framework combining behavioral, cognitive, and algorithmic metrics to quantify human-AI coevolution.  
Methodology:  
1. Deploy longitudinal human-AI tasks (e.g., diagnostic support in healthcare) where both agent policies and user decisions co-adapt over sessions.  
2. Continuously record human metrics (accuracy, response time, trust surveys, cognitive load via NASA-TLX) alongside AI metrics (policy shift magnitude, predictive uncertainty, bias drift).  
3. Compute mutual adaptation indices using information-theoretic measures (e.g., transfer entropy) and dynamic time-warping to capture feedback loop strength and direction.  
Expected Outcomes: A composite “Coevolution Score” detailing adaptation speed, alignment drift, and mutual dependency.  
Potential Impact: Equip researchers and practitioners with actionable insights into dynamic alignment, guiding the design of robust, fair, and trust-aware AI systems across socially impactful domains.