**Title:** Co-Adapt Metrics: Evaluating AI through Longitudinal Human Cognitive Shifts

**Motivation:** Standard AI benchmarks neglect the crucial aspect of human adaptation during prolonged interaction. This limits our understanding of true AI utility and potential risks like over-reliance or skill degradation. New evaluation metrics are needed to capture the dynamics of Human-AI Coevolution (HAIC).

**Main Idea:** We propose "Co-Adapt Metrics," an evaluation framework assessing AI systems based on *how* human collaborators adapt cognitively over time. We will conduct longitudinal studies where humans use AI tools for complex, iterative tasks (e.g., information synthesis, scientific discovery). By tracking not only joint task performance but also shifts in human strategies, decision time, reliance patterns (measured behaviourally), and potentially cognitive skill change (via targeted assessments), we can quantify the co-adaptive process. Expected outcome: Quantifiable metrics that reveal how different AI interaction paradigms influence human cognitive adaptation, distinguishing systems that foster synergistic partnerships from those causing detrimental dependencies. Impact: Guide AI design towards promoting beneficial long-term human-AI coevolution.