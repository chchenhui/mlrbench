Title: ActiveHCI – Adaptive UI Generation with Human-in-the-Loop Learning

Motivation:  
Current AI-driven UI generators often produce one-size-fits-all designs that lack personalization and are hard to correct post hoc. This hampers adoption in fast-paced design workflows, reduces usability, and forces manual refinements. Embedding structured human feedback directly into the generation loop can yield interfaces that better align with individual user needs, accessibility requirements, and brand guidelines.

Main Idea:  
ActiveHCI combines a generative UI model (e.g., a layout-diffusion network) with active learning and reinforcement learning from human feedback (RLHF).  
1. Initialization: The generator proposes multiple candidate interfaces for a given task description.  
2. Feedback Loop: Designers provide quick component-level feedback—binary accept/reject flags, drag-and-drop adjustments, or natural-language comments.  
3. Reward Modeling: A lightweight reward network is trained online to score future proposals based on aggregated feedback.  
4. Active Querying: Using Bayesian optimization, the system selects the next set of high-uncertainty UI candidates to request feedback on, maximizing learning efficiency.  
5. Iterative Refinement: The generator is fine-tuned with policy gradients guided by the reward model, progressively aligning outputs with user preferences.  

Expected outcomes include faster, personalized prototyping, reduced post-generation edits, and a publicly available toolkit demonstrating improved usability metrics on standard UI benchmarks.