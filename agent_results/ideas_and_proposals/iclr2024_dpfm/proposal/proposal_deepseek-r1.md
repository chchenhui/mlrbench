**Title: Reinforcement Learning–Guided Data Curation for Safety-Aligned Foundation Models**

---

### 1. Introduction

#### Background  
Foundation Models (FMs) like GPT-4 and LLaMA have revolutionized AI by achieving state-of-the-art performance across diverse tasks. However, their reliance on massive, uncurated datasets often results in unintended behaviors, including toxicity, bias, and misalignment with human values. Traditional approaches to mitigate these issues—such as manual data filtering, post-hoc fine-tuning, or rule-based safety filters—are labor-intensive, static, and struggle to scale with the exponential growth of training data. Recent work in data-centric AI highlights the critical role of training data quality in addressing these challenges, emphasizing the need for automated, dynamic curation mechanisms that prioritize safety and alignment without compromising model utility.

#### Research Objectives  
This research proposes a **reinforcement learning (RL)-guided data curation framework** to dynamically optimize the selection of training samples for FMs. The objectives are:  
1. To develop an automated pipeline that prioritizes safety-aligned data points using RL-driven policies.  
2. To design a composite reward function integrating toxicity detection, alignment proxies, and downstream task performance.  
3. To validate the framework’s ability to reduce harmful outputs while preserving linguistic capabilities.  
4. To establish scalable, closed-loop data curation protocols adaptable to evolving safety requirements.  

#### Significance  
Current methods for safety alignment, such as manual annotation or static filtering, lack scalability and adaptability. By leveraging RL to dynamically curate training data, this work addresses key challenges in data-centric AI:  
- **Scalability**: Automating data selection reduces reliance on human labor.  
- **Adaptability**: Continuous reward model updates enable alignment with evolving safety norms.  
- **Balanced Performance**: Joint optimization of safety and utility metrics prevents performance degradation.  
This framework bridges the gap between data curation and model alignment, offering a systematic solution to enhance FM reliability in real-world applications.

---

### 2. Methodology  

#### Research Design  
The framework comprises four stages: (1) data pool initialization, (2) reward modeling, (3) RL-driven curation, and (4) iterative refinement.  

**Stage 1: Data Pool Initialization**  
- **Data Sources**: Utilize large-scale text corpora (e.g., C4, The Pile) and synthetic datasets (e.g., Safety Pretraining’s refusal dialogues [1]).  
- **Preprocessing**: Remove duplicates and apply basic toxicity filters (e.g., Perspective API) to create a candidate pool $\mathcal{D}_{\text{candidate}}$.  

**Stage 2: Composite Reward Design**  
The reward $R(s)$ for a sample $s$ combines safety and alignment signals:  
$$
R(s) = \underbrace{\lambda_1 \cdot \text{ToxicityScore}(s)}_{\text{Safety}} + \underbrace{\lambda_2 \cdot \text{AlignmentScore}(s)}_{\text{Alignment}} + \underbrace{\lambda_3 \cdot \text{UtilityScore}(s)}_{\text{Utility}},
$$  
where:  
- $\text{ToxicityScore}(s)$: Output of a pretrained safety classifier (e.g., Detoxify).  
- $\text{AlignmentScore}(s)$: Generated by probing a small human-labeled dataset (e.g., Safer-Instruct [2]) using similarity metrics.  
- $\text{UtilityScore}(s)$: Perplexity-based measure of linguistic coherence via a reference FM.  
- $\lambda_{1-3}$: Tunable weights balancing safety, alignment, and utility.  

**Stage 3: RL-Driven Data Curation**  
- **Agent Architecture**: A policy network $\pi_\theta(a|s)$ (e.g., transformer-based) assigns selection probabilities to samples.  
- **RL Algorithm**: Proximal Policy Optimization (PPO) [3] maximizes the expected cumulative reward:  
$$
\mathcal{L}^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right],
$$  
where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ and $\hat{A}_t$ is the advantage estimate.  
- **Batch Selection**: The agent samples mini-batches $\mathcal{B}_t \sim \pi_\theta(\mathcal{D}_{\text{candidate}})$ to prioritize high-reward samples.  

**Stage 4: Iterative Refinement**  
1. **Fine-Tuning**: Periodically update a lightweight FM (e.g., LLaMA-7B) on $\mathcal{B}_t$.  
2. **Reward Model Updates**: Recalibrate $\text{AlignmentScore}(s)$ using human feedback on model outputs.  
3. **Safety Evaluation**: Assess toxicity reduction via benchmarks like ToxiGen and performance on tasks like GLUE.  

#### Experimental Design  
- **Datasets**:  
  - Training: C4 (raw corpus), Safer-Instruct (alignment probes).  
  - Evaluation: RealToxicityPrompts, ETHICS benchmark, GLUE.  
- **Baselines**: Compare against RAFT [3], Safety Pretraining [1], and standard fine-tuning.  
- **Metrics**:  
  - **Safety**: Toxicity probability (% toxic outputs), attack success rate (ASR).  
  - **Alignment**: Human evaluation scores on harmlessness and helpfulness.  
  - **Utility**: Perplexity, accuracy on GLUE, MMLU.  
- **Ablation Studies**: Test variants of $R(s)$ (e.g., removing $\text{AlignmentScore}$) to isolate component contributions.  

---

### 3. Expected Outcomes & Impact  

#### Expected Outcomes  
1. **Scalable Data Curation Pipeline**: An automated framework for selecting safety-aligned training data, reducing manual intervention by >70% compared to human-in-the-loop methods.  
2. **Improved Safety Metrics**: Target ≥50% reduction in toxicity scores and ASR relative to baseline FMs, matching or exceeding Safety Pretraining’s results [1].  
3. **Preserved Model Utility**: <5% degradation in perplexity and downstream task accuracy (GLUE, MMLU) compared to unfiltered training.  
4. **Dynamic Adaptability**: Demonstrated ability to adjust safety-alignment trade-offs via $\lambda_{1-3}$ tuning, validated on CoSA’s controllability benchmark [4].  

#### Broader Impact  
- **Technical**: Establishes RL as a viable paradigm for data-centric safety alignment, enabling real-time adaptation to emerging risks.  
- **Societal**: Reduces the propagation of harmful content in FMs, mitigating ethical and legal risks (e.g., misinformation, bias).  
- **Economic**: Lowers the cost of large-scale data curation, making safety alignment accessible to resource-constrained organizations.  

This work aligns with the workshop’s focus on data-centric solutions for FM challenges, offering a novel integration of RL and safety alignment with actionable insights for researchers and practitioners.  

---

**References**  
[1] Maini, P., et al. "Safety Pretraining: Toward the Next Generation of Safe AI." arXiv:2504.16980 (2025).  
[2] Shi, T., et al. "Safer-Instruct: Aligning Language Models with Automated Preference Data." arXiv:2311.08685 (2023).  
[3] Dong, H., et al. "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment." arXiv:2304.06767 (2023).  
[4] Zhang, J., et al. "Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements." arXiv:2410.08968 (2024).