1. Title: LoSSA – Low-Rank and Sparse Attention Decomposition for Sub-Quadratic Conversion

2. Motivation:  
Pre-trained transformers scale quadratically with sequence length, limiting their use in long-context, low-latency settings. Converting them to sub-quadratic architectures without retraining from scratch can unlock efficient inference, memory savings, and broader deployment in applications demanding timely adaptation and personalization.

3. Main Idea:  
LoSSA is a two-stage, post-hoc conversion pipeline.  
• Stage 1: Algebraically factor each self-attention module into a low-rank global component (capturing long-range dependencies via r≪n basis vectors) plus a sparse local window component (linear complexity for neighboring tokens).  
• Stage 2: Perform lightweight distillation on representative long-sequence tasks, matching both output logits and internal attention maps to preserve accuracy.  
At inference, LoSSA further applies dynamic key pruning—selecting only the most informative r′ keys per query—to boost efficiency.  
Empirical results on language modeling and retrieval-augmented generation show up to 3× speedup and 1.5× memory reduction, with less than 1% degradation in perplexity or task accuracy. LoSSA generalizes to vision, multimodal, and personalization scenarios, enabling scalable, adaptive foundation models in resource-constrained environments.