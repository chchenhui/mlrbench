# Multi-Objective Preference-Based Reinforcement Learning for Healthcare

## Motivation
Healthcare decisions often involve multiple conflicting objectives (e.g., efficacy, side effects, cost, quality of life). Traditional reinforcement learning (RL) approaches struggle in healthcare because numerical reward functions are difficult to define precisely and often fail to capture the complex trade-offs made by physicians. Preference-based learning offers a more intuitive approach for capturing clinician expertise, but most current methods assume a single underlying objective, limiting their applicability to healthcare's inherently multi-objective nature.

## Main Idea
We propose a novel framework that combines multi-objective optimization with preference-based reinforcement learning for clinical decision support. Our approach maintains a Pareto front of policies representing different trade-offs between competing healthcare objectives. Rather than asking clinicians to directly assign numerical weights to objectives, we present them with pairs of treatment trajectories and ask which they prefer. These preferences are used to learn a distribution over the weights of different objectives that best explains clinical decision-making. The system then recommends personalized treatment policies aligned with both individual patient priorities and physician expertise. We will evaluate our approach on medication dosing for chronic conditions (e.g., diabetes, hypertension) where balancing treatment efficacy against side effects is crucial. This framework has the potential to create more transparent, personalized clinical decision support systems that align with how physicians actually reason about complex healthcare decisions.