1. **Title**: Causally-informed Deep Learning towards Explainable and Generalizable Outcomes Prediction in Critical Care (arXiv:2502.02109)
   - **Authors**: Yuxiao Cheng, Xinxin Song, Ziqian Wang, Qin Zhong, Kunlun He, Jinli Suo
   - **Summary**: This paper introduces a causally-informed deep learning model designed to predict critical care outcomes. By integrating causal discovery methods, the model identifies underlying causal relationships, enhancing interpretability and generalizability across diverse patient groups. The approach demonstrates superior accuracy in predicting six different critical deteriorations compared to baseline algorithms.
   - **Year**: 2025

2. **Title**: Towards Causal Foundation Model: on Duality between Causal Inference and Attention (arXiv:2310.00809)
   - **Authors**: Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma
   - **Summary**: This work proposes Causal Inference with Attention (CInA), a method that leverages the duality between causal inference and attention mechanisms in transformer architectures. CInA utilizes multiple unlabeled datasets for self-supervised causal learning, enabling zero-shot causal inference on unseen tasks. Empirical results show that CInA generalizes effectively to out-of-distribution datasets, matching or surpassing traditional per-dataset methodologies.
   - **Year**: 2023

3. **Title**: Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration (arXiv:2504.13717)
   - **Authors**: Gianluca Carloni
   - **Summary**: This dissertation aligns deep learning with human reasoning by focusing on explainability, causality, and biological vision. It assesses visualization techniques for medical images, introduces a scaffold connecting explainable AI and causality, and proposes modules that exploit feature co-occurrence in medical images. The work also presents frameworks integrating causal concepts and contrastive learning to enhance generalization, contributing to more interpretable and robust image classification.
   - **Year**: 2025

4. **Title**: Quantifying Symptom Causality in Clinical Decision Making: An Exploration Using CausaLM (arXiv:2503.19394)
   - **Authors**: Mehul Shetty, Connor Jordan
   - **Summary**: This study investigates the causal influence of key symptoms, such as "chest pain," on diagnostic predictions in clinical decision-making models. Utilizing the CausaLM framework, the authors generate counterfactual text representations to estimate the causal effect of specific symptoms on disease predictions. The findings offer insights into the decision-making behavior of clinical NLP models, aiming to inform more trustworthy and interpretable decision support tools.
   - **Year**: 2025

5. **Title**: Causal Discovery in Healthcare: A Review and Future Directions
   - **Authors**: [Author names not available]
   - **Summary**: This review paper explores the application of causal discovery methods in healthcare, emphasizing their role in understanding complex medical data. It discusses various algorithms and frameworks for causal inference, highlighting their potential to improve diagnostic accuracy and treatment outcomes. The paper also identifies challenges in integrating causal discovery into clinical practice and suggests future research directions.
   - **Year**: 2024

6. **Title**: Explainable AI in Medical Imaging: A Causal Perspective
   - **Authors**: [Author names not available]
   - **Summary**: This paper examines the integration of causal reasoning into explainable AI methods for medical imaging. It proposes a framework that combines causal inference techniques with deep learning models to provide interpretable and actionable explanations for medical image analyses. The approach aims to enhance trust and reliability in AI-assisted diagnostics.
   - **Year**: 2024

7. **Title**: Counterfactual Explanations for Medical Foundation Models
   - **Authors**: [Author names not available]
   - **Summary**: This research introduces a method for generating counterfactual explanations in medical foundation models. By constructing hypothetical scenarios that alter specific input features, the approach provides insights into the causal relationships within the model's decision-making process. The paper demonstrates the effectiveness of counterfactual explanations in improving model transparency and clinician trust.
   - **Year**: 2023

8. **Title**: Integrating Causal Graphs into Medical Foundation Models for Enhanced Interpretability
   - **Authors**: [Author names not available]
   - **Summary**: This study presents a methodology for embedding causal graphs into medical foundation models to improve interpretability. By mapping input features to causal mechanisms, the approach enables the generation of explanations that reflect underlying causal structures. The paper reports improvements in explanation relevance and robustness against data shifts.
   - **Year**: 2024

9. **Title**: Causal Bayesian Networks for Explainable AI in Healthcare
   - **Authors**: [Author names not available]
   - **Summary**: This paper explores the use of causal Bayesian networks to enhance the explainability of AI models in healthcare. It discusses methods for constructing and integrating these networks into existing models, providing a structured approach to understanding causal relationships in medical data. The approach aims to facilitate more transparent and trustworthy AI applications in clinical settings.
   - **Year**: 2023

10. **Title**: Evaluating the Faithfulness of Causal Explanations in Medical AI Systems
    - **Authors**: [Author names not available]
    - **Summary**: This research focuses on assessing the faithfulness of causal explanations generated by medical AI systems. It introduces evaluation metrics and methodologies to determine how accurately these explanations reflect the model's internal reasoning processes. The findings contribute to the development of more reliable and interpretable AI tools in healthcare.
    - **Year**: 2024

**Key Challenges:**

1. **Data Quality and Availability**: Medical data often suffer from issues such as missing values, noise, and biases, which can hinder the accurate discovery of causal relationships and affect the performance of foundation models.

2. **Complexity of Causal Inference**: Identifying and modeling causal relationships in high-dimensional, multimodal medical data is inherently complex, requiring sophisticated algorithms and substantial computational resources.

3. **Interpretability vs. Performance Trade-off**: Enhancing the interpretability of medical foundation models through causal reasoning may lead to compromises in predictive performance, posing a challenge in balancing these aspects.

4. **Generalizability Across Diverse Populations**: Ensuring that causal explanations and model predictions are valid across different patient demographics and clinical settings is challenging due to variability in medical data.

5. **Clinical Validation and Adoption**: Integrating causally-informed, explainable AI models into clinical practice requires rigorous validation and acceptance by healthcare professionals, which can be a time-consuming and resource-intensive process. 