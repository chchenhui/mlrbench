Title: Cycle-Consistent Autoformalization with Self-Supervised Theorem Generation

Motivation:  
Autoformalization—translating informal mathematical text into machine-checkable proofs—suffers from scarce parallel corpora and brittle alignment between natural language and formal syntax. Moreover, current systems rarely leverage the wealth of formal libraries to expand training data. By jointly learning to autoformalize and generate new formal theorems, we can both enrich training resources and enforce semantic consistency, reducing errors and improving translation quality.

Main Idea:  
We propose a dual-Transformer framework that alternates between two tasks: (1) autoformalizing informal proof sketches into a proof assistant language (Lean/Coq), and (2) generating natural-language descriptions for freshly synthesized formal theorems. Starting from existing formal libraries, we randomly sample and slightly perturb formal theorems, then train the generator to produce plausible informal statements. The informalizer maps them back to formal code, and a cycle-consistency loss ensures semantic fidelity. We further retrieve similar lemmas to augment context and employ a self-critical learning objective to penalize misalignments. Expected outcomes include a richer pseudo-parallel corpus, higher autoformalization accuracy, and a library of machine-generated conjectures. This approach can significantly lower the barrier to formalizing new mathematics and accelerate collaborative human–AI theorem discovery.