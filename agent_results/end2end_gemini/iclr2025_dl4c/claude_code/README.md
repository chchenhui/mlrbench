# Interactive Execution-Trace Alignment (IETA) Framework

This repository contains the implementation of the IETA framework for aligning code generation models with execution trace feedback.

## Overview

The IETA framework aims to improve the robustness and reliability of code generated by Large Language Models (LLMs) by directly incorporating execution trace feedback into the model's post-training. The framework executes generated code snippets in a sandboxed environment, captures detailed execution traces, and uses this feedback to fine-tune the models using techniques like Direct Preference Optimization (DPO) or Reinforcement Learning from AI Feedback (RLAIF).

## Project Structure

```
claude_code/
├── models/                 # Model implementations
│   ├── base_model.py       # Base code generation model
│   ├── dpo_model.py        # DPO model implementation
│   └── rlaif_model.py      # RLAIF model implementation
├── utils/                  # Utility modules
│   ├── trace_capture.py    # Execution trace capture utilities
│   ├── data_utils.py       # Dataset loading and processing
│   ├── llm_utils.py        # LLM API utilities
│   ├── preference_utils.py # Preference pair generation
│   └── visualization.py    # Visualization utilities
├── run_experiments.py      # Main experiment runner
├── run_all_experiments.py  # Script to run all experiments
└── requirements.txt        # Project dependencies
```

## Installation

1. Clone the repository
2. Install the dependencies:

```bash
pip install -r requirements.txt
```

## Running Experiments

### Single Method Experiment

To run an experiment with a specific method:

```bash
python run_experiments.py --method [dpo|rlaif|baseline] --dataset humaneval --num_samples 20 --use_synthetic
```

### All Experiments

To run all experiments (baseline, DPO, and RLAIF) and generate comparison visualizations:

```bash
python run_all_experiments.py --dataset humaneval --num_samples 20 --use_synthetic
```

## Command Line Arguments

### Main Arguments

- `--method`: Method to use (dpo, rlaif, or baseline)
- `--dataset`: Dataset to use (humaneval, mbpp, or apps)
- `--num_samples`: Number of samples to use from the dataset
- `--model_type`: Type of model to use (api or huggingface)
- `--model_name`: Model name (API model or HuggingFace model ID)
- `--output_dir`: Directory to save results (default: "results")

### Training Arguments

- `--num_iterations`: Number of iterations for the training loop
- `--batch_size`: Batch size for training
- `--learning_rate`: Learning rate
- `--training_steps`: Number of training steps

### Evaluation Arguments

- `--pass_k`: Values of k for pass@k evaluation (default: 1 10 100)

### Other Arguments

- `--seed`: Random seed
- `--debug`: Enable debug mode
- `--capture_trace`: Capture execution traces
- `--max_execution_time`: Maximum execution time in seconds
- `--use_synthetic`: Use synthetic data and results for demonstration

## Results

The experiment results are saved in the output directory specified by `--output_dir`. The results include:

- JSON files with all metrics and data for each method
- Visualizations of pass rates, execution rates, error frequencies, and training losses
- Comparison visualizations between different methods
- A summary README with an overview of the experiments

## Example

```bash
# Run all experiments with synthetic data
python run_all_experiments.py --dataset humaneval --num_samples 20 --use_synthetic --output_dir results

# Run a single experiment with the DPO method
python run_experiments.py --method dpo --dataset humaneval --num_samples 20 --use_synthetic --output_dir results
```

## Notes

- The `--use_synthetic` flag is useful for demonstrations and testing the framework without requiring real executions.
- For real experiments, remove this flag and ensure that the execution environment is properly set up.
- API keys for OpenAI and Anthropic should be set as environment variables (`OPENAI_API_KEY` and `ANTHROPIC_API_KEY`) if using API models.