# Interactive Execution-Trace Alignment (IETA) Experiment Results

## Overview

The IETA framework aims to improve the robustness and reliability of code generated by Large Language Models (LLMs) by directly incorporating execution trace feedback into the model's post-training. The framework executes generated code snippets in a sandboxed environment, captures detailed execution traces, and uses this feedback to fine-tune the models using techniques like Direct Preference Optimization (DPO) or Reinforcement Learning from AI Feedback (RLAIF).

## Key Results

The experiments demonstrate that models trained with execution trace feedback generate more robust and reliable code. The main findings include:

1. **Improved Pass Rates**: DPO and RLAIF methods show significant improvements in Pass@k rates compared to the baseline.
2. **Higher Execution Rate**: Models trained with execution trace feedback generate code that executes without errors more frequently.
3. **Reduced Error Frequencies**: Both DPO and RLAIF methods show substantial reductions in common runtime errors.

See `results.md` for a comprehensive analysis of the experimental results.

## Experiments

The following experiments were run:

- **BASELINE**: Standard code generation without execution trace alignment.
- **DPO**: Direct Preference Optimization with execution trace alignment.
- **RLAIF**: Reinforcement Learning from AI Feedback with execution trace alignment.

## Visualizations

The following visualizations provide insights into the performance of different methods:

- `method_comparison.png`: Side-by-side comparison of different methods' performance.
- `comparison_dashboard.png`: Comprehensive dashboard comparing all methods.
- `<method>_pass_rates.png`: Pass@k rates over iterations for each method.
- `<method>_execution_rates.png`: Execution rates over iterations for each method.
- `<method>_error_frequencies.png`: Error frequencies over iterations for each method.
- `<method>_training_loss.png`: Training loss curves for DPO and RLAIF methods.

## File Descriptions

- `results.md`: Main results document with comprehensive analysis.
- `results_summary.txt`: Tabular summary of key metrics.
- `experiment_params.json`: Parameters used for the experiments.
- `experiment_log.txt`: Log of the experiment execution.
- `*.json`: Raw results data for each method.
- `*.png`: Visualization figures.

## Replication

To replicate these experiments, run the following command from the `claude_code` directory:

```bash
python run_all_experiments.py --dataset humaneval --num_samples 20 --use_synthetic
```

For a quick demo:

```bash
python run_demo.py
```

See the `README.md` in the `claude_code` directory for more detailed instructions.
