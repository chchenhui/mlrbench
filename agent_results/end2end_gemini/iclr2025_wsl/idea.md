**Title:** Permutation-Invariant Transformer for Cross-Architecture Model Property Prediction

**Motivation:** Understanding model properties (e.g., robustness, fairness, generalization gap) directly from weights, without extensive evaluation, is crucial for efficient model development and selection. However, neural network weights possess inherent permutation symmetries (neurons in a layer can be reordered without changing functionality), making direct comparison across different initializations or even slightly different architectures challenging for standard models.

**Main Idea:** We propose a Transformer-based architecture specifically designed to ingest flattened model weights and predict target model properties. To handle permutation symmetries, we will incorporate a permutation-invariant attention mechanism or a canonicalization preprocessing step (e.g., sorting neuron activations statistics or using optimal transport to align layers). The model will be trained on a large dataset of diverse pre-trained models (from a model zoo) where weights are paired with their empirically measured properties. This "WeightNet" aims to learn a mapping from the weight distribution to high-level semantic properties, enabling rapid property estimation for new, unseen models, even with slight architectural variations. This could significantly accelerate model auditing and selection.