# Adversarially Evolved Benchmarks (AEB)

This repository contains the implementation of the Adversarially Evolved Benchmark (AEB) system for dynamic and holistic model evaluation. The AEB system utilizes evolutionary computation techniques to generate challenging benchmark instances that expose weaknesses in machine learning models, promoting the development of more robust and generalizable AI.

## Overview

The AEB system consists of two main components:
1. **Benchmark Evolver (BE)**: An agent that generates challenging and diverse benchmark instances
2. **Target Models (TMs)**: The machine learning models being evaluated

The system implements a co-evolutionary loop where:
- The BE generates challenges to exploit weaknesses in current TMs
- TMs are evaluated on these challenges
- Performance feedback is used to evolve better benchmarks
- Models can be retrained/fine-tuned using evolved benchmarks to improve robustness

## Requirements

```
pip install -r requirements.txt
```

## Project Structure

```
├── data/               # Data storage
├── models/             # Model definitions and weights
├── benchmark_evolver/  # Implementation of the Benchmark Evolver
├── target_models/      # Implementation of Target Models
├── utils/              # Utility functions
├── experiments/        # Experiment configurations
├── results/            # Experimental results
├── notebooks/          # Jupyter notebooks for analysis
└── main.py             # Main script to run experiments
```

## Running Experiments

To run the experiments:

```bash
# Run the full experiment pipeline
python main.py

# Run a specific experiment
python run_experiment.py --config experiments/config_image.json

# Run evaluation only
python evaluate.py --model_path models/trained/model_aeb_hardened.pt
```

## Experiment Types

1. **AEB-Evaluated**: Standard models evaluated with benchmarks generated by AEB
2. **AEB-Hardened**: Models retrained/fine-tuned using challenging instances from AEB
3. **Control**: Models trained and evaluated using only static benchmarks

## Results

Experiment results are saved in the `results/` directory, including:
- Performance metrics
- Visualization of evolved benchmarks
- Comparison of model robustness
- Analysis of failure modes

## Citation

If you use this code in your research, please cite:

```
@article{aeb2025,
  title={Adversarially Evolved Benchmarks for Dynamic and Holistic Model Evaluation},
  author={},
  journal={},
  year={2025}
}
```