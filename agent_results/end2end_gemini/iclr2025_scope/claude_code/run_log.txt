2025-05-11 07:04:08 - INFO - ================================================================================
2025-05-11 07:04:08 - INFO - Starting MeLPA experiment: melpa_exp_20250511_070408
2025-05-11 07:04:08 - INFO - ================================================================================
2025-05-11 07:04:08 - INFO - Experiment started at: 2025-05-11 07:04:08.986084
2025-05-11 07:04:08 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:04:12 - INFO - Traceback (most recent call last):
2025-05-11 07:04:12 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 20, in <module>
2025-05-11 07:04:12 - INFO - from models.meta_learning import MeLPA, InitializationNetwork, UpdateMechanism
2025-05-11 07:04:12 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 8, in <module>
2025-05-11 07:04:12 - INFO - import higher
2025-05-11 07:04:12 - INFO - ModuleNotFoundError: No module named 'higher'
2025-05-11 07:04:12 - ERROR - Experiment failed with return code: 1
2025-05-11 07:04:12 - INFO - Experiment ended at: 2025-05-11 07:04:12.519888
2025-05-11 07:04:12 - INFO - Total duration: 0:00:03.533804
2025-05-11 07:04:12 - INFO - ================================================================================
2025-05-11 07:04:12 - INFO - MeLPA experiment melpa_exp_20250511_070408 completed
2025-05-11 07:04:12 - INFO - ================================================================================
2025-05-11 07:05:13 - INFO - ================================================================================
2025-05-11 07:05:13 - INFO - Starting MeLPA experiment: melpa_exp_20250511_070513
2025-05-11 07:05:13 - INFO - ================================================================================
2025-05-11 07:05:13 - INFO - Experiment started at: 2025-05-11 07:05:13.918328
2025-05-11 07:05:13 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Using device: cuda
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Adapter type: pfeiffer
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Starting meta-learning phase
2025-05-11 07:05:17 - INFO - 2025-05-11 07:05:17 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:05:19 - INFO - 2025-05-11 07:05:19.637136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:05:19 - INFO - 2025-05-11 07:05:19.652378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:05:19 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:05:19 - INFO - E0000 00:00:1746918319.669839  570457 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:05:19 - INFO - E0000 00:00:1746918319.675100  570457 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:05:19 - INFO - W0000 00:00:1746918319.689128  570457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:05:19 - INFO - W0000 00:00:1746918319.689145  570457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:05:19 - INFO - W0000 00:00:1746918319.689146  570457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:05:19 - INFO - W0000 00:00:1746918319.689148  570457 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:05:19 - INFO - 2025-05-11 07:05:19.693323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:05:19 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:05:21 - INFO - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-11 07:05:21 - INFO - 2025-05-11 07:05:21 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
2025-05-11 07:05:44 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:05:44 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:05:45 - INFO - Traceback (most recent call last):
2025-05-11 07:05:45 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1133, in <module>
2025-05-11 07:05:45 - INFO - main()
2025-05-11 07:05:45 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1106, in main
2025-05-11 07:05:45 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:05:45 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:05:45 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 174, in run_meta_learning_phase
2025-05-11 07:05:45 - INFO - model_with_adapters = TransformerWithAdapters(base_model, adapter_config)
2025-05-11 07:05:45 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:05:45 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/adapters.py", line 128, in __init__
2025-05-11 07:05:45 - INFO - self.adapter_controllers = self._create_adapter_controllers()
2025-05-11 07:05:45 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:05:45 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/adapters.py", line 160, in _create_adapter_controllers
2025-05-11 07:05:45 - INFO - raise ValueError("Unsupported base model architecture")
2025-05-11 07:05:45 - INFO - ValueError: Unsupported base model architecture
2025-05-11 07:05:46 - ERROR - Experiment failed with return code: 1
2025-05-11 07:05:46 - INFO - Experiment ended at: 2025-05-11 07:05:46.488535
2025-05-11 07:05:46 - INFO - Total duration: 0:00:32.570207
2025-05-11 07:05:46 - INFO - ================================================================================
2025-05-11 07:05:46 - INFO - MeLPA experiment melpa_exp_20250511_070513 completed
2025-05-11 07:05:46 - INFO - ================================================================================
2025-05-11 07:07:12 - INFO - ================================================================================
2025-05-11 07:07:12 - INFO - Starting MeLPA experiment: melpa_exp_20250511_070712
2025-05-11 07:07:12 - INFO - ================================================================================
2025-05-11 07:07:12 - INFO - Experiment started at: 2025-05-11 07:07:12.897868
2025-05-11 07:07:12 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Using device: cuda
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Adapter type: pfeiffer
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Starting meta-learning phase
2025-05-11 07:07:16 - INFO - 2025-05-11 07:07:16 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:07:18 - INFO - 2025-05-11 07:07:18.312195: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:07:18 - INFO - 2025-05-11 07:07:18.327347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:07:18 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:07:18 - INFO - E0000 00:00:1746918438.344908  571195 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:07:18 - INFO - E0000 00:00:1746918438.350190  571195 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:07:18 - INFO - W0000 00:00:1746918438.364155  571195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:07:18 - INFO - W0000 00:00:1746918438.364171  571195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:07:18 - INFO - W0000 00:00:1746918438.364173  571195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:07:18 - INFO - W0000 00:00:1746918438.364175  571195 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:07:18 - INFO - 2025-05-11 07:07:18.368317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:07:18 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:07:19 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:07:19 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:07:20 - INFO - Warning: Using fallback adapter for unsupported model architecture
2025-05-11 07:07:20 - INFO - Traceback (most recent call last):
2025-05-11 07:07:20 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1146, in <module>
2025-05-11 07:07:20 - INFO - main()
2025-05-11 07:07:20 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1119, in main
2025-05-11 07:07:20 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:07:20 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:07:20 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 188, in run_meta_learning_phase
2025-05-11 07:07:20 - INFO - melpa = MeLPA(
2025-05-11 07:07:20 - INFO - ^^^^^^
2025-05-11 07:07:20 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 275, in __init__
2025-05-11 07:07:20 - INFO - self.init_network = InitializationNetwork(
2025-05-11 07:07:20 - INFO - ^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:07:20 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 44, in __init__
2025-05-11 07:07:20 - INFO - self.output_heads[name] = nn.Linear(prev_dim, total_params)
2025-05-11 07:07:20 - INFO - ~~~~~~~~~~~~~~~~~^^^^^^
2025-05-11 07:07:20 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/container.py", line 495, in __setitem__
2025-05-11 07:07:20 - INFO - self.add_module(key, module)
2025-05-11 07:07:20 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 643, in add_module
2025-05-11 07:07:20 - INFO - raise KeyError(f'module name can\'t contain ".", got: {name}')
2025-05-11 07:07:20 - INFO - KeyError: 'module name can\'t contain ".", got: adapter_controllers.fallback_adapter.adapters.__temp_adapter__.down.weight'
2025-05-11 07:07:21 - ERROR - Experiment failed with return code: 1
2025-05-11 07:07:21 - INFO - Experiment ended at: 2025-05-11 07:07:21.545103
2025-05-11 07:07:21 - INFO - Total duration: 0:00:08.647235
2025-05-11 07:07:21 - INFO - ================================================================================
2025-05-11 07:07:21 - INFO - MeLPA experiment melpa_exp_20250511_070712 completed
2025-05-11 07:07:21 - INFO - ================================================================================
2025-05-11 07:08:08 - INFO - ================================================================================
2025-05-11 07:08:08 - INFO - Starting MeLPA experiment: melpa_exp_20250511_070808
2025-05-11 07:08:08 - INFO - ================================================================================
2025-05-11 07:08:08 - INFO - Experiment started at: 2025-05-11 07:08:08.340125
2025-05-11 07:08:08 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Using device: cuda
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Adapter type: pfeiffer
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Starting meta-learning phase
2025-05-11 07:08:11 - INFO - 2025-05-11 07:08:11 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:08:13 - INFO - 2025-05-11 07:08:13.764841: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:08:13 - INFO - 2025-05-11 07:08:13.780307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:08:13 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:08:13 - INFO - E0000 00:00:1746918493.798236  571647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:08:13 - INFO - E0000 00:00:1746918493.803642  571647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:08:13 - INFO - W0000 00:00:1746918493.817760  571647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:08:13 - INFO - W0000 00:00:1746918493.817776  571647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:08:13 - INFO - W0000 00:00:1746918493.817777  571647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:08:13 - INFO - W0000 00:00:1746918493.817779  571647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:08:13 - INFO - 2025-05-11 07:08:13.821903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:08:13 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:08:15 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:08:15 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:08:15 - INFO - 2025-05-11 07:08:15 - INFO - Creating 20 meta-training tasks
2025-05-11 07:08:15 - INFO - Warning: Using fallback adapter for unsupported model architecture
2025-05-11 07:08:15 - INFO - Traceback (most recent call last):
2025-05-11 07:08:15 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1146, in <module>
2025-05-11 07:08:15 - INFO - main()
2025-05-11 07:08:15 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1119, in main
2025-05-11 07:08:15 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:08:15 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:08:15 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 215, in run_meta_learning_phase
2025-05-11 07:08:15 - INFO - meta_train_tasks = task_generator.create_meta_learning_tasks(
2025-05-11 07:08:15 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:08:15 - INFO - TypeError: MockTaskGenerator.create_meta_learning_tasks() got an unexpected keyword argument 'dataset_names'
2025-05-11 07:08:17 - ERROR - Experiment failed with return code: 1
2025-05-11 07:08:17 - INFO - Experiment ended at: 2025-05-11 07:08:17.046670
2025-05-11 07:08:17 - INFO - Total duration: 0:00:08.706545
2025-05-11 07:08:17 - INFO - ================================================================================
2025-05-11 07:08:17 - INFO - MeLPA experiment melpa_exp_20250511_070808 completed
2025-05-11 07:08:17 - INFO - ================================================================================
2025-05-11 07:09:27 - INFO - ================================================================================
2025-05-11 07:09:27 - INFO - Starting MeLPA experiment: melpa_exp_20250511_070927
2025-05-11 07:09:27 - INFO - ================================================================================
2025-05-11 07:09:27 - INFO - Experiment started at: 2025-05-11 07:09:27.072052
2025-05-11 07:09:27 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Using device: cuda
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Adapter type: pfeiffer
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Starting meta-learning phase
2025-05-11 07:09:30 - INFO - 2025-05-11 07:09:30 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:09:32 - INFO - 2025-05-11 07:09:32.478904: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:09:32 - INFO - 2025-05-11 07:09:32.494026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:09:32 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:09:32 - INFO - E0000 00:00:1746918572.511488  572257 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:09:32 - INFO - E0000 00:00:1746918572.516713  572257 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:09:32 - INFO - W0000 00:00:1746918572.530931  572257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:09:32 - INFO - W0000 00:00:1746918572.530947  572257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:09:32 - INFO - W0000 00:00:1746918572.530949  572257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:09:32 - INFO - W0000 00:00:1746918572.530950  572257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:09:32 - INFO - 2025-05-11 07:09:32.535074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:09:32 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:09:34 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:09:34 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:09:34 - INFO - 2025-05-11 07:09:34 - INFO - Creating 20 meta-training tasks
2025-05-11 07:09:50 - INFO - 2025-05-11 07:09:50 - INFO - Creating 5 meta-validation tasks
2025-05-11 07:09:54 - INFO - 2025-05-11 07:09:54 - INFO - Starting meta-training for 5 epochs
2025-05-11 07:09:54 - INFO - Warning: Using fallback adapter for unsupported model architecture
2025-05-11 07:09:54 - INFO - 
2025-05-11 07:09:54 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:09:54 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:09:54 - INFO - Traceback (most recent call last):
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1141, in <module>
2025-05-11 07:09:54 - INFO - main()
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1114, in main
2025-05-11 07:09:54 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:09:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 255, in run_meta_learning_phase
2025-05-11 07:09:54 - INFO - meta_metrics = meta_trainer.meta_train(
2025-05-11 07:09:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 156, in meta_train
2025-05-11 07:09:54 - INFO - loss = self.meta_train_step(task_batch)
2025-05-11 07:09:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 66, in meta_train_step
2025-05-11 07:09:54 - INFO - self.model.model.add_adapter(adapter_name)
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/adapters.py", line 177, in add_adapter
2025-05-11 07:09:54 - INFO - controller.add_adapter(adapter_name, adapter_type)
2025-05-11 07:09:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/adapters.py", line 103, in add_adapter
2025-05-11 07:09:54 - INFO - self.adapters[adapter_name] = adapter
2025-05-11 07:09:54 - INFO - ~~~~~~~~~~~~~^^^^^^^^^^^^^^
2025-05-11 07:09:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/container.py", line 495, in __setitem__
2025-05-11 07:09:54 - INFO - self.add_module(key, module)
2025-05-11 07:09:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 643, in add_module
2025-05-11 07:09:54 - INFO - raise KeyError(f'module name can\'t contain ".", got: {name}')
2025-05-11 07:09:54 - INFO - KeyError: 'module name can\'t contain ".", got: temp_adapter_1746918594.8135674'
2025-05-11 07:09:56 - ERROR - Experiment failed with return code: 1
2025-05-11 07:09:56 - INFO - Experiment ended at: 2025-05-11 07:09:56.309398
2025-05-11 07:09:56 - INFO - Total duration: 0:00:29.237346
2025-05-11 07:09:56 - INFO - ================================================================================
2025-05-11 07:09:56 - INFO - MeLPA experiment melpa_exp_20250511_070927 completed
2025-05-11 07:09:56 - INFO - ================================================================================
2025-05-11 07:11:11 - INFO - ================================================================================
2025-05-11 07:11:11 - INFO - Starting MeLPA experiment: melpa_exp_20250511_071111
2025-05-11 07:11:11 - INFO - ================================================================================
2025-05-11 07:11:11 - INFO - Experiment started at: 2025-05-11 07:11:11.104016
2025-05-11 07:11:11 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Using device: cuda
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Adapter type: pfeiffer
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Starting meta-learning phase
2025-05-11 07:11:14 - INFO - 2025-05-11 07:11:14 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:11:17 - INFO - 2025-05-11 07:11:17.144748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:11:17 - INFO - 2025-05-11 07:11:17.160019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:11:17 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:11:17 - INFO - E0000 00:00:1746918677.177552  572924 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:11:17 - INFO - E0000 00:00:1746918677.182818  572924 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:11:17 - INFO - W0000 00:00:1746918677.196887  572924 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:11:17 - INFO - W0000 00:00:1746918677.196903  572924 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:11:17 - INFO - W0000 00:00:1746918677.196905  572924 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:11:17 - INFO - W0000 00:00:1746918677.196906  572924 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:11:17 - INFO - 2025-05-11 07:11:17.201122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:11:17 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:11:18 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:11:18 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:11:19 - INFO - 2025-05-11 07:11:19 - INFO - Creating 20 meta-training tasks
2025-05-11 07:11:35 - INFO - 2025-05-11 07:11:35 - INFO - Creating 5 meta-validation tasks
2025-05-11 07:11:39 - INFO - 2025-05-11 07:11:39 - INFO - Starting meta-training for 5 epochs
2025-05-11 07:11:39 - INFO - Warning: Using fallback adapter for unsupported model architecture
2025-05-11 07:11:39 - INFO - 
2025-05-11 07:11:39 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:11:39 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:11:39 - INFO - Traceback (most recent call last):
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1141, in <module>
2025-05-11 07:11:39 - INFO - main()
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1114, in main
2025-05-11 07:11:39 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 255, in run_meta_learning_phase
2025-05-11 07:11:39 - INFO - meta_metrics = meta_trainer.meta_train(
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 156, in meta_train
2025-05-11 07:11:39 - INFO - loss = self.meta_train_step(task_batch)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 69, in meta_train_step
2025-05-11 07:11:39 - INFO - meta_loss = self.model.meta_train_step(support_batch, query_batch, adapter_name)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 403, in meta_train_step
2025-05-11 07:11:39 - INFO - query_loss = maml(support_batch, query_batch)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
2025-05-11 07:11:39 - INFO - return self._call_impl(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
2025-05-11 07:11:39 - INFO - return forward_call(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 219, in forward
2025-05-11 07:11:39 - INFO - adapted_model = self.adapt(support_batch, inner_steps)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 191, in adapt
2025-05-11 07:11:39 - INFO - loss = self._compute_loss(adapted_model, support_batch)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 201, in _compute_loss
2025-05-11 07:11:39 - INFO - outputs = model(inputs, adapter_name=self.adapter_name)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
2025-05-11 07:11:39 - INFO - return self._call_impl(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
2025-05-11 07:11:39 - INFO - return forward_call(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/adapters.py", line 214, in forward
2025-05-11 07:11:39 - INFO - outputs = self.base_model(**inputs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
2025-05-11 07:11:39 - INFO - return self._call_impl(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
2025-05-11 07:11:39 - INFO - return forward_call(*args, **kwargs)
2025-05-11 07:11:39 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:11:39 - INFO - TypeError: TransformerWithAdapters.forward() got an unexpected keyword argument 'input_ids'
2025-05-11 07:11:41 - ERROR - Experiment failed with return code: 1
2025-05-11 07:11:41 - INFO - Experiment ended at: 2025-05-11 07:11:41.215102
2025-05-11 07:11:41 - INFO - Total duration: 0:00:30.111086
2025-05-11 07:11:41 - INFO - ================================================================================
2025-05-11 07:11:41 - INFO - MeLPA experiment melpa_exp_20250511_071111 completed
2025-05-11 07:11:41 - INFO - ================================================================================
2025-05-11 07:12:25 - INFO - ================================================================================
2025-05-11 07:12:25 - INFO - Starting MeLPA experiment: melpa_exp_20250511_071225
2025-05-11 07:12:25 - INFO - ================================================================================
2025-05-11 07:12:25 - INFO - Experiment started at: 2025-05-11 07:12:25.388950
2025-05-11 07:12:25 - INFO - Running command: python run_experiments.py --seed 42 --output_dir /home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/results --log_file experiment.log --model_name distilbert-base-uncased --adapter_type pfeiffer --dataset_names glue/sst2 imdb ag_news tweet_eval --run_meta_learning --run_baselines --run_melpa --run_analysis --n_meta_epochs 5 --n_meta_train_tasks 20 --n_meta_val_tasks 5 --n_tasks 3 --n_examples_per_task 50 --n_epochs_per_task 3
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Starting MeLPA experiments with seed 42
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Using device: cuda
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Base model: distilbert-base-uncased
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Adapter type: pfeiffer
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Datasets: ['glue/sst2', 'imdb', 'ag_news', 'tweet_eval']
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Starting meta-learning phase
2025-05-11 07:12:29 - INFO - 2025-05-11 07:12:29 - INFO - Loading base model: distilbert-base-uncased
2025-05-11 07:12:30 - INFO - 2025-05-11 07:12:30.815927: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-05-11 07:12:30 - INFO - 2025-05-11 07:12:30.830938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-05-11 07:12:30 - INFO - WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
2025-05-11 07:12:30 - INFO - E0000 00:00:1746918750.848344  573481 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-05-11 07:12:30 - INFO - E0000 00:00:1746918750.853590  573481 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-05-11 07:12:30 - INFO - W0000 00:00:1746918750.867465  573481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:12:30 - INFO - W0000 00:00:1746918750.867480  573481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:12:30 - INFO - W0000 00:00:1746918750.867482  573481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:12:30 - INFO - W0000 00:00:1746918750.867483  573481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-05-11 07:12:30 - INFO - 2025-05-11 07:12:30.871595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2025-05-11 07:12:30 - INFO - To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-05-11 07:12:32 - INFO - Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
2025-05-11 07:12:32 - INFO - You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-11 07:12:32 - INFO - 2025-05-11 07:12:32 - INFO - Creating 20 meta-training tasks
2025-05-11 07:12:49 - INFO - 2025-05-11 07:12:49 - INFO - Creating 5 meta-validation tasks
2025-05-11 07:12:53 - INFO - 2025-05-11 07:12:53 - INFO - Starting meta-training for 5 epochs
2025-05-11 07:12:53 - INFO - Warning: Using fallback adapter for unsupported model architecture
2025-05-11 07:12:53 - INFO - 
2025-05-11 07:12:54 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:12:54 - INFO - Meta-epoch 1/5:   0%|          | 0/20 [00:00<?, ?it/s]
2025-05-11 07:12:54 - INFO - Traceback (most recent call last):
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1141, in <module>
2025-05-11 07:12:54 - INFO - main()
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 1114, in main
2025-05-11 07:12:54 - INFO - melpa_model = run_meta_learning_phase(args, logger)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/run_experiments.py", line 255, in run_meta_learning_phase
2025-05-11 07:12:54 - INFO - meta_metrics = meta_trainer.meta_train(
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 156, in meta_train
2025-05-11 07:12:54 - INFO - loss = self.meta_train_step(task_batch)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/utils/training.py", line 69, in meta_train_step
2025-05-11 07:12:54 - INFO - meta_loss = self.model.meta_train_step(support_batch, query_batch, adapter_name)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 414, in meta_train_step
2025-05-11 07:12:54 - INFO - query_loss = maml(support_batch, query_batch)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
2025-05-11 07:12:54 - INFO - return self._call_impl(*args, **kwargs)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
2025-05-11 07:12:54 - INFO - return forward_call(*args, **kwargs)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 230, in forward
2025-05-11 07:12:54 - INFO - adapted_model = self.adapt(support_batch, inner_steps)
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - File "/home/chenhui/mlr-bench/pipeline_gemini/iclr2025_scope/claude_code/models/meta_learning.py", line 193, in adapt
2025-05-11 07:12:54 - INFO - loss.backward()
2025-05-11 07:12:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
2025-05-11 07:12:54 - INFO - torch.autograd.backward(
2025-05-11 07:12:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
2025-05-11 07:12:54 - INFO - _engine_run_backward(
2025-05-11 07:12:54 - INFO - File "/home/chenhui/miniconda3/envs/mlrbench/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
2025-05-11 07:12:54 - INFO - return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2025-05-11 07:12:54 - INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-11 07:12:54 - INFO - RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
2025-05-11 07:12:56 - ERROR - Experiment failed with return code: 1
2025-05-11 07:12:56 - INFO - Experiment ended at: 2025-05-11 07:12:56.041021
2025-05-11 07:12:56 - INFO - Total duration: 0:00:30.652071
2025-05-11 07:12:56 - INFO - ================================================================================
2025-05-11 07:12:56 - INFO - MeLPA experiment melpa_exp_20250511_071225 completed
2025-05-11 07:12:56 - INFO - ================================================================================
