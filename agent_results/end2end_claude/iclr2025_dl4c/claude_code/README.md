# Multi-Agent Collaborative Programming (MACP) Framework

This repository contains the implementation and evaluation of the Multi-Agent Collaborative Programming (MACP) framework - a novel approach to software development that structures AI agents into specialized roles mirroring human software development teams.

## Overview

The MACP framework implements a collaborative environment where AI agents with distinct roles (architect, implementer, tester, reviewer) work together to solve programming tasks under the coordination of a moderator meta-agent. This approach aims to overcome limitations of single-agent systems by leveraging specialized knowledge, distributing workload, and implementing checks and balances similar to human development teams.

## Repository Structure

```
claude_code/
├── data/                    # Task dataset and other data files
│   └── tasks.json           # Programming tasks for evaluation
├── experimental_plan.md     # Detailed experimental plan
├── evaluator.py             # Code evaluation and metrics tools
├── macp_framework.py        # MACP framework implementation
├── run_experiment.py        # Main experiment runner
├── single_agent.py          # Baseline single-agent implementation
├── utils.py                 # Utility functions for experiments
└── README.md                # This file
```

## Requirements

The code requires the following dependencies:
- Python 3.8+
- Anthropic API (anthropic package)
- matplotlib
- numpy
- networkx

You can install the required packages with:

```bash
pip install anthropic matplotlib numpy networkx
```

## Running the Experiments

You can run the experiments with the following command:

```bash
python run_experiment.py
```

By default, this will run both the baseline single-agent and MACP framework on all tasks using the Claude-3.7-sonnet model.

### Command-line Options

The experiment runner supports several command-line options:

```
--tasks TASKS         Task IDs to run (comma-separated) or 'all'
--model MODEL         LLM model to use
--output-dir DIR      Directory to save results
--verbose             Enable verbose logging
--skip-baseline       Skip baseline single-agent experiments
--skip-macp           Skip MACP experiments
```

### Examples

To run only a specific task:
```bash
python run_experiment.py --tasks task1
```

To run only the MACP framework:
```bash
python run_experiment.py --skip-baseline
```

To run only the baseline:
```bash
python run_experiment.py --skip-macp
```

## Task Dataset

The task dataset (`data/tasks.json`) contains programming tasks of varying complexity:

1. **Task 1:** String Manipulation Library (Simple)
2. **Task 2:** Data Structure Implementation (Moderate)
3. **Task 3:** API Client Library (Moderate)
4. **Task 4:** Event Management System (Complex)
5. **Task 5:** Mini Web Framework (Complex)

Each task includes a description, requirements, expected outputs, and test cases.

## Experimental Evaluation

The experimental evaluation compares the MACP framework against a baseline single-agent approach across multiple dimensions:

1. **Solution Correctness**: Functional correctness, bug density, test coverage
2. **Solution Quality**: Code complexity, maintainability, best practices adherence
3. **Efficiency**: Time to solution, computational resources, iterations required
4. **Collaboration**: Communication patterns, knowledge distribution, conflict resolution

## Results

After running the experiments, results are saved to the specified output directory (default: `../results/`). The results include:

- JSON file with all experiment results (`experiment_results.json`)
- Markdown summary of results (`results.md`)
- Visualizations of performance metrics
- Solutions generated by both systems
- Log file with experiment details (`log.txt`)

## Extending the Framework

The MACP framework can be extended in several ways:

1. **Additional Agent Roles**: Add new specialized agent types
2. **Different LLM Models**: Experiment with different foundation models
3. **Communication Protocols**: Implement alternative communication mechanisms
4. **Task Types**: Evaluate on different programming tasks or domains
5. **Evaluation Metrics**: Add new metrics for assessing solution quality

## References

This implementation is based on research described in:
- The "Multi-Agent Collaborative Programming Framework: A Team-Based Approach to Software Development" proposal
- Related work in multi-agent LLM frameworks such as MetaGPT, AgentVerse, and CAMEL

## License

[MIT License](LICENSE)