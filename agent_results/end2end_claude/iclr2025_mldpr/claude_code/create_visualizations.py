"""
Visualization Tools for Contextual Dataset Deprecation Framework

This script provides functions to create customized visualizations from
the experiment results, beyond those generated by the evaluation module.
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from pathlib import Path
import logging
from typing import List, Dict, Any, Optional, Tuple, Union
import argparse

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(os.path.dirname(os.path.dirname(__file__)), 'log.txt')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("visualizations")

# Constants
RESULTS_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'results')
FIGURES_DIR = os.path.join(RESULTS_DIR, 'figures')
os.makedirs(FIGURES_DIR, exist_ok=True)

# Set Seaborn style for all plots
sns.set(style="whitegrid")
plt.rcParams.update({
    'font.size': 12,
    'axes.labelsize': 14,
    'axes.titlesize': 16,
    'xtick.labelsize': 12,
    'ytick.labelsize': 12,
    'legend.fontsize': 12,
    'figure.titlesize': 18
})

def load_experiment_results(results_file: str = None) -> Dict[str, Any]:
    """
    Load experiment results from a JSON file.
    
    Args:
        results_file: Path to the experiment results JSON file.
            If None, tries to find the most recent results file.
            
    Returns:
        Dictionary containing the experiment results
    """
    if results_file is None:
        # Try to find the most recent results file
        experiment_files = []
        for root, _, files in os.walk(RESULTS_DIR):
            for file in files:
                if file == "experiment_results.json":
                    experiment_files.append(os.path.join(root, file))
        
        if not experiment_files:
            logger.error("No experiment results found")
            return {}
        
        # Use the most recent file
        results_file = sorted(experiment_files, key=os.path.getmtime)[-1]
    
    if not os.path.exists(results_file):
        logger.error(f"Results file not found: {results_file}")
        return {}
    
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    logger.info(f"Loaded experiment results from {results_file}")
    return results

def prepare_user_response_data(results: Dict[str, Any]) -> pd.DataFrame:
    """
    Extract and prepare user response data for visualization.
    
    Args:
        results: Dictionary containing experiment results
        
    Returns:
        DataFrame with user response metrics for different strategies
    """
    strategies = []
    metrics = []
    datasets = []
    values = []
    errors = []
    
    user_response = results.get("user_response", {})
    
    for dataset_id, dataset_data in user_response.items():
        for strategy, strategy_data in dataset_data.items():
            for sim_result in strategy_data:
                # Extract acknowledgment time
                if "acknowledgment_time" in sim_result:
                    strategies.append(strategy)
                    metrics.append("acknowledgment_time")
                    datasets.append(dataset_id)
                    values.append(sim_result["acknowledgment_time"])
                    errors.append(0)  # No error available for individual points
                
                # Extract alternative adoption
                if "alternative_adoption" in sim_result:
                    strategies.append(strategy)
                    metrics.append("alternative_adoption")
                    datasets.append(dataset_id)
                    values.append(sim_result["alternative_adoption"])
                    errors.append(0)
                
                # Extract continued usage
                if "continued_usage" in sim_result:
                    strategies.append(strategy)
                    metrics.append("continued_usage")
                    datasets.append(dataset_id)
                    values.append(sim_result["continued_usage"])
                    errors.append(0)
    
    # Create DataFrame
    df = pd.DataFrame({
        "strategy": strategies,
        "metric": metrics,
        "dataset": datasets,
        "value": values,
        "error": errors
    })
    
    return df

def prepare_system_performance_data(results: Dict[str, Any]) -> pd.DataFrame:
    """
    Extract and prepare system performance data for visualization.
    
    Args:
        results: Dictionary containing experiment results
        
    Returns:
        DataFrame with system performance metrics for different strategies
    """
    strategies = []
    metrics = []
    datasets = []
    values = []
    errors = []
    
    system_performance = results.get("system_performance", {})
    
    for dataset_id, dataset_data in system_performance.items():
        for strategy, strategy_data in dataset_data.items():
            for sim_result in strategy_data:
                # Extract recommendation accuracy
                if "recommendation_accuracy" in sim_result:
                    strategies.append(strategy)
                    metrics.append("recommendation_accuracy")
                    datasets.append(dataset_id)
                    values.append(sim_result["recommendation_accuracy"])
                    errors.append(0)
                
                # Extract processing time
                if "processing_time" in sim_result:
                    strategies.append(strategy)
                    metrics.append("processing_time")
                    datasets.append(dataset_id)
                    values.append(sim_result["processing_time"])
                    errors.append(0)
                
                # Extract notification success
                if "notification_success" in sim_result:
                    strategies.append(strategy)
                    metrics.append("notification_success")
                    datasets.append(dataset_id)
                    values.append(sim_result["notification_success"])
                    errors.append(0)
    
    # Create DataFrame
    df = pd.DataFrame({
        "strategy": strategies,
        "metric": metrics,
        "dataset": datasets,
        "value": values,
        "error": errors
    })
    
    return df

def prepare_research_impact_data(results: Dict[str, Any]) -> pd.DataFrame:
    """
    Extract and prepare research impact data for visualization.
    
    Args:
        results: Dictionary containing experiment results
        
    Returns:
        DataFrame with research impact metrics for different strategies
    """
    strategies = []
    metrics = []
    datasets = []
    values = []
    errors = []
    time_periods = []
    
    research_impact = results.get("research_impact", {})
    
    for dataset_id, dataset_data in research_impact.items():
        for strategy, strategy_data in dataset_data.items():
            for sim_result in strategy_data:
                # Extract benchmark diversity
                if "benchmark_diversity" in sim_result:
                    strategies.append(strategy)
                    metrics.append("benchmark_diversity")
                    datasets.append(dataset_id)
                    values.append(sim_result["benchmark_diversity"])
                    errors.append(0)
                    time_periods.append(None)
                
                # Extract alternative performance
                if "alternative_performance" in sim_result:
                    strategies.append(strategy)
                    metrics.append("alternative_performance")
                    datasets.append(dataset_id)
                    values.append(sim_result["alternative_performance"])
                    errors.append(0)
                    time_periods.append(None)
                
                # Extract citation patterns (time series)
                if "citation_pattern" in sim_result:
                    citation_pattern = sim_result["citation_pattern"]
                    for i, citation in enumerate(citation_pattern):
                        strategies.append(strategy)
                        metrics.append("citation_pattern")
                        datasets.append(dataset_id)
                        values.append(citation)
                        errors.append(0)
                        time_periods.append(i + 1)
    
    # Create DataFrame
    df = pd.DataFrame({
        "strategy": strategies,
        "metric": metrics,
        "dataset": datasets,
        "value": values,
        "error": errors,
        "time_period": time_periods
    })
    
    return df

def prepare_aggregate_metrics_data(results: Dict[str, Any]) -> pd.DataFrame:
    """
    Extract and prepare aggregate metrics data for visualization.
    
    Args:
        results: Dictionary containing experiment results
        
    Returns:
        DataFrame with aggregate metrics for different strategies
    """
    strategies = []
    metrics = []
    categories = []
    values = []
    errors = []
    
    aggregate_metrics = results.get("aggregate_metrics", {})
    
    for strategy, strategy_data in aggregate_metrics.items():
        # User response aggregates
        user_response = strategy_data.get("user_response", {})
        for metric, metric_data in user_response.items():
            if isinstance(metric_data, dict) and "mean" in metric_data:
                strategies.append(strategy)
                metrics.append(metric)
                categories.append("user_response")
                values.append(metric_data["mean"])
                errors.append(metric_data.get("std", 0))
        
        # System performance aggregates
        system_performance = strategy_data.get("system_performance", {})
        for metric, metric_data in system_performance.items():
            if isinstance(metric_data, dict) and "mean" in metric_data:
                strategies.append(strategy)
                metrics.append(metric)
                categories.append("system_performance")
                values.append(metric_data["mean"])
                errors.append(metric_data.get("std", 0))
        
        # Research impact aggregates
        research_impact = strategy_data.get("research_impact", {})
        for metric, metric_data in research_impact.items():
            if metric != "citation_pattern" and isinstance(metric_data, dict) and "mean" in metric_data:
                strategies.append(strategy)
                metrics.append(metric)
                categories.append("research_impact")
                values.append(metric_data["mean"])
                errors.append(metric_data.get("std", 0))
    
    # Create DataFrame
    df = pd.DataFrame({
        "strategy": strategies,
        "metric": metrics,
        "category": categories,
        "value": values,
        "error": errors
    })
    
    return df

def plot_strategy_comparison(
    df: pd.DataFrame,
    metric: str,
    title: str = None,
    ylabel: str = None,
    figsize: Tuple[int, int] = (10, 6),
    save_path: str = None
) -> plt.Figure:
    """
    Create a bar chart comparing strategies for a specific metric.
    
    Args:
        df: DataFrame containing the data
        metric: Name of the metric to plot
        title: Title for the plot
        ylabel: Label for the y-axis
        figsize: Size of the figure
        save_path: Path to save the figure
        
    Returns:
        The matplotlib Figure object
    """
    # Filter data for the metric
    metric_data = df[df["metric"] == metric]
    
    if metric_data.empty:
        logger.warning(f"No data found for metric {metric}")
        return None
    
    # Group by strategy and calculate mean and std
    grouped = metric_data.groupby("strategy").agg({
        "value": ["mean", "std"]
    }).reset_index()
    
    # Flatten the multi-level columns
    grouped.columns = ["strategy", "mean", "std"]
    
    # Set missing std to 0
    grouped["std"] = grouped["std"].fillna(0)
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Set colors based on strategy
    colors = []
    for strategy in grouped["strategy"]:
        if strategy == "CONTROL":
            colors.append("#FF9999")  # Light red
        elif strategy == "BASIC":
            colors.append("#99CCFF")  # Light blue
        elif strategy == "FULL":
            colors.append("#99FF99")  # Light green
        else:
            colors.append("#CCCCCC")  # Gray
    
    # Create bar chart with error bars
    bars = ax.bar(
        grouped["strategy"],
        grouped["mean"],
        yerr=grouped["std"],
        capsize=10,
        color=colors,
        alpha=0.8,
        edgecolor="black",
        linewidth=1
    )
    
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width()/2.,
            height + 0.02 * max(grouped["mean"]),
            f'{height:.2f}',
            ha='center',
            va='bottom',
            fontweight='bold'
        )
    
    # Set labels and title
    ax.set_xlabel('Deprecation Strategy', fontweight='bold')
    ax.set_ylabel(ylabel or metric.replace('_', ' ').title(), fontweight='bold')
    ax.set_title(title or f'Comparison of {metric.replace("_", " ").title()} Across Strategies', fontweight='bold')
    
    # Add grid
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved strategy comparison plot to {save_path}")
    
    return fig

def plot_citation_patterns_by_strategy(
    df: pd.DataFrame,
    figsize: Tuple[int, int] = (12, 7),
    save_path: str = None
) -> plt.Figure:
    """
    Create a line plot of citation patterns over time for different strategies.
    
    Args:
        df: DataFrame containing the data
        figsize: Size of the figure
        save_path: Path to save the figure
        
    Returns:
        The matplotlib Figure object
    """
    # Filter data for citation patterns
    citation_data = df[df["metric"] == "citation_pattern"]
    
    if citation_data.empty:
        logger.warning("No citation pattern data found")
        return None
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Process and plot data for each strategy
    for strategy in citation_data["strategy"].unique():
        strategy_data = citation_data[citation_data["strategy"] == strategy]
        
        # Group by time period and calculate mean and std
        grouped = strategy_data.groupby("time_period").agg({
            "value": ["mean", "std"]
        }).reset_index()
        
        # Flatten the multi-level columns
        grouped.columns = ["time_period", "mean", "std"]
        
        # Set missing std to 0
        grouped["std"] = grouped["std"].fillna(0)
        
        # Sort by time period
        grouped = grouped.sort_values("time_period")
        
        # Set color based on strategy
        color = "#FF9999" if strategy == "CONTROL" else "#99CCFF" if strategy == "BASIC" else "#99FF99"
        
        # Plot line with error bands
        ax.plot(
            grouped["time_period"],
            grouped["mean"],
            label=strategy,
            color=color,
            linewidth=2,
            marker='o'
        )
        
        ax.fill_between(
            grouped["time_period"],
            grouped["mean"] - grouped["std"],
            grouped["mean"] + grouped["std"],
            alpha=0.2,
            color=color
        )
    
    # Set labels and title
    ax.set_xlabel('Time Period', fontweight='bold')
    ax.set_ylabel('Citation Count', fontweight='bold')
    ax.set_title('Citation Patterns Over Time by Deprecation Strategy', fontweight='bold')
    
    # Add legend
    ax.legend(fontsize=12)
    
    # Add grid
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved citation patterns plot to {save_path}")
    
    return fig

def plot_heatmap_by_dataset(
    df: pd.DataFrame,
    metric: str,
    title: str = None,
    figsize: Tuple[int, int] = (12, 8),
    save_path: str = None
) -> plt.Figure:
    """
    Create a heatmap showing the metric values for different strategies and datasets.
    
    Args:
        df: DataFrame containing the data
        metric: Name of the metric to plot
        title: Title for the plot
        figsize: Size of the figure
        save_path: Path to save the figure
        
    Returns:
        The matplotlib Figure object
    """
    # Filter data for the metric
    metric_data = df[df["metric"] == metric]
    
    if metric_data.empty:
        logger.warning(f"No data found for metric {metric}")
        return None
    
    # Group by strategy and dataset to calculate mean
    grouped = metric_data.groupby(["strategy", "dataset"]).agg({
        "value": "mean"
    }).reset_index()
    
    # Create pivot table for heatmap
    pivot_data = grouped.pivot(index="dataset", columns="strategy", values="value")
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Create heatmap
    sns.heatmap(
        pivot_data,
        annot=True,
        cmap="YlGnBu",
        fmt=".2f",
        linewidths=.5,
        ax=ax,
        cbar_kws={"label": metric.replace('_', ' ').title()}
    )
    
    # Set labels and title
    ax.set_title(title or f'{metric.replace("_", " ").title()} by Dataset and Strategy', fontweight='bold')
    
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved heatmap to {save_path}")
    
    return fig

def plot_distribution_by_strategy(
    df: pd.DataFrame,
    metric: str,
    title: str = None,
    xlabel: str = None,
    figsize: Tuple[int, int] = (12, 8),
    save_path: str = None
) -> plt.Figure:
    """
    Create violin plots showing the distribution of metric values for different strategies.
    
    Args:
        df: DataFrame containing the data
        metric: Name of the metric to plot
        title: Title for the plot
        xlabel: Label for the x-axis
        figsize: Size of the figure
        save_path: Path to save the figure
        
    Returns:
        The matplotlib Figure object
    """
    # Filter data for the metric
    metric_data = df[df["metric"] == metric]
    
    if metric_data.empty:
        logger.warning(f"No data found for metric {metric}")
        return None
    
    # Set color palette
    palette = {
        "CONTROL": "#FF9999",
        "BASIC": "#99CCFF",
        "FULL": "#99FF99"
    }
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Create violin plots
    sns.violinplot(
        x="strategy",
        y="value",
        data=metric_data,
        palette=palette,
        inner="quartile",
        ax=ax
    )
    
    # Add individual data points
    sns.stripplot(
        x="strategy",
        y="value",
        data=metric_data,
        color="black",
        alpha=0.3,
        size=4,
        jitter=True,
        ax=ax
    )
    
    # Set labels and title
    ax.set_xlabel(xlabel or 'Deprecation Strategy', fontweight='bold')
    ax.set_ylabel(metric.replace('_', ' ').title(), fontweight='bold')
    ax.set_title(title or f'Distribution of {metric.replace("_", " ").title()} by Strategy', fontweight='bold')
    
    # Add grid
    ax.grid(True, linestyle='--', alpha=0.7, axis='y')
    
    # Adjust layout
    plt.tight_layout()
    
    # Save if requested
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        logger.info(f"Saved distribution plot to {save_path}")
    
    return fig

def create_all_visualizations(results: Dict[str, Any], output_dir: str = None) -> Dict[str, str]:
    """
    Create all visualizations from the experiment results.
    
    Args:
        results: Dictionary containing experiment results
        output_dir: Directory to save visualizations
        
    Returns:
        Dictionary mapping figure names to file paths
    """
    if output_dir is None:
        output_dir = FIGURES_DIR
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Prepare data
    user_df = prepare_user_response_data(results)
    system_df = prepare_system_performance_data(results)
    research_df = prepare_research_impact_data(results)
    aggregate_df = prepare_aggregate_metrics_data(results)
    
    # Combine all data for convenience
    all_dfs = []
    if not user_df.empty:
        user_df["category"] = "user_response"
        all_dfs.append(user_df)
    if not system_df.empty:
        system_df["category"] = "system_performance"
        all_dfs.append(system_df)
    if not research_df.empty:
        research_df["category"] = "research_impact"
        all_dfs.append(research_df[research_df["metric"] != "citation_pattern"])
    
    combined_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()
    
    # Dictionary to store figure paths
    figure_paths = {}
    
    # Create strategy comparison plots
    if not combined_df.empty:
        for metric in combined_df["metric"].unique():
            if metric == "citation_pattern":
                continue
            
            fig_path = os.path.join(output_dir, f"strategy_comparison_{metric}.png")
            plot_strategy_comparison(
                combined_df,
                metric,
                save_path=fig_path
            )
            figure_paths[f"strategy_comparison_{metric}"] = fig_path
    
    # Create citation pattern plot
    if not research_df.empty and "citation_pattern" in research_df["metric"].values:
        fig_path = os.path.join(output_dir, "citation_patterns.png")
        plot_citation_patterns_by_strategy(
            research_df,
            save_path=fig_path
        )
        figure_paths["citation_patterns"] = fig_path
    
    # Create heatmaps by dataset
    if not combined_df.empty:
        for metric in combined_df["metric"].unique():
            if metric == "citation_pattern":
                continue
            
            # Check if there are multiple datasets
            metric_data = combined_df[combined_df["metric"] == metric]
            if len(metric_data["dataset"].unique()) > 1:
                fig_path = os.path.join(output_dir, f"heatmap_{metric}.png")
                plot_heatmap_by_dataset(
                    combined_df,
                    metric,
                    save_path=fig_path
                )
                figure_paths[f"heatmap_{metric}"] = fig_path
    
    # Create distribution plots
    if not combined_df.empty:
        for metric in combined_df["metric"].unique():
            if metric == "citation_pattern":
                continue
            
            # Check if there are enough data points
            metric_data = combined_df[combined_df["metric"] == metric]
            if len(metric_data) >= 10:  # At least 10 data points for meaningful distribution
                fig_path = os.path.join(output_dir, f"distribution_{metric}.png")
                plot_distribution_by_strategy(
                    combined_df,
                    metric,
                    save_path=fig_path
                )
                figure_paths[f"distribution_{metric}"] = fig_path
    
    logger.info(f"Created {len(figure_paths)} visualizations in {output_dir}")
    return figure_paths

def main(args):
    """Main function to create visualizations from experiment results."""
    # Load experiment results
    results = load_experiment_results(args.results_file)
    
    if not results:
        logger.error("Failed to load experiment results")
        return 1
    
    # Create visualizations
    figure_paths = create_all_visualizations(results, args.output_dir)
    
    logger.info(f"Created {len(figure_paths)} visualizations")
    for name, path in figure_paths.items():
        logger.info(f"- {name}: {path}")
    
    return 0

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create visualizations for the experiment results")
    parser.add_argument("--results-file", type=str, default=None, help="Path to the experiment results JSON file")
    parser.add_argument("--output-dir", type=str, default=None, help="Directory to save visualizations")
    args = parser.parse_args()
    
    main(args)