# Causal Representation Learning for Mitigating Shortcut Learning in Multi-Modal Models

## Motivation
Large Multi-Modal Models (LMMs) increasingly power critical applications but remain vulnerable to shortcut learning, where models rely on spurious correlations rather than causal relationships. This is especially concerning in high-stakes domains like healthcare and autonomous systems. The problem is particularly acute in multi-modal settings where spurious correlations can exist across modalities (e.g., background textures consistently paired with certain objects). Current robustification methods often require prior knowledge of spurious features or extensive group annotations, limiting practical utility.

## Main Idea
We propose Causal Multi-Modal Representation Learning (CMMRL), a framework to discover and mitigate shortcut learning in LMMs without requiring explicit annotation of spurious features. CMMRL leverages three key innovations: (1) a contrastive invariance mechanism that identifies features that remain stable across intentionally perturbed inputs; (2) a modality disentanglement component that separates shared causal features from modality-specific spurious ones by analyzing cross-modal prediction errors; and (3) an intervention-based fine-tuning approach where the model is trained to maintain predictions when spurious features are manipulated. Our method is designed to work as a fine-tuning layer on existing foundation models, making it practical for deployment. We expect CMMRL to significantly improve out-of-distribution generalization in multi-modal tasks while requiring minimal additional annotation or computational overhead.