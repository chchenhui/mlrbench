**Related Papers**

1. **Title**: The Human-AI Handshake Framework: A Bidirectional Approach to Human-AI Collaboration (arXiv:2502.01493)
   - **Authors**: Aung Pyae
   - **Summary**: This paper introduces the "Human-AI Handshake Model," emphasizing a bidirectional, adaptive framework for human-AI collaboration. It highlights five key attributes: information exchange, mutual learning, validation, feedback, and mutual capability augmentation, aiming to foster balanced interactions where AI acts as a responsive partner evolving with users over time.
   - **Year**: 2025

2. **Title**: Human-AI Coevolution (arXiv:2306.13723)
   - **Authors**: Dino Pedreschi, Luca Pappalardo, Emanuele Ferragina, Ricardo Baeza-Yates, Albert-Laszlo Barabasi, Frank Dignum, Virginia Dignum, Tina Eliassi-Rad, Fosca Giannotti, Janos Kertesz, Alistair Knott, Yannis Ioannidis, Paul Lukowicz, Andrea Passarella, Alex Sandy Pentland, John Shawe-Taylor, Alessandro Vespignani
   - **Summary**: This work defines human-AI coevolution as a continuous mutual influence between humans and AI algorithms, particularly through recommender systems and assistants. It emphasizes the complex feedback loops in human-AI interactions and proposes "Coevolution AI" as a new field to study these dynamics, highlighting the need for interdisciplinary approaches to understand and manage unintended social outcomes.
   - **Year**: 2023

3. **Title**: ValueCompass: A Framework of Fundamental Values for Human-AI Alignment (arXiv:2409.09586)
   - **Authors**: Hua Shen, Tiffany Knearem, Reshmi Ghosh, Yu-Ju Yang, Tanushree Mitra, Yun Huang
   - **Summary**: The authors present "ValueCompass," a framework grounded in psychological theory to identify and evaluate human-AI alignment. By applying this framework across various real-world scenarios, they uncover significant misalignments between human values and language models, underscoring the necessity for context-aware AI alignment strategies.
   - **Year**: 2024

4. **Title**: Beyond Prompts: Learning from Human Communication for Enhanced AI Intent Alignment (arXiv:2405.05678)
   - **Authors**: Yoonsu Kim, Kihoon Son, Seoyoung Kim, Juho Kim
   - **Summary**: This study explores human strategies for intent specification in human-human communication to improve AI intent alignment. By comparing human-human and human-LLM interactions, the authors identify key strategies that can be applied to design AI systems more effective at understanding and aligning with user intent.
   - **Year**: 2024

5. **Title**: Interactive AI Alignment: Specification, Process, and Evaluation Alignment (arXiv:2311.00710)
   - **Authors**: Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, Meredith Ringel Morris
   - **Summary**: This paper revisits the input-output interaction cycle in AI systems, connecting concepts in AI alignment to define three objectives for interactive alignment: specification alignment, process alignment, and evaluation alignment. It uses existing systems as examples to show how these user-centered views can be applied descriptively, prescriptively, and evaluatively.
   - **Year**: 2023

6. **Title**: Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions (arXiv:2406.09264)
   - **Authors**: Hua Shen
   - **Summary**: This systematic review of over 400 papers introduces a conceptual framework of "Bidirectional Human-AI Alignment," encompassing both aligning AI to humans and aligning humans to AI. It articulates key findings, literature gaps, and trends, providing recommendations for future research in this evolving field.
   - **Year**: 2024

7. **Title**: AI Alignment and Social Choice: Fundamental Limitations and Policy Implications (arXiv:2310.16048)
   - **Authors**: Abhilash Mishra
   - **Summary**: Building on impossibility results in social choice theory, this paper investigates challenges in aligning AI agents with human values through democratic processes. It highlights the limitations of reinforcement learning with human feedback (RLHF) and discusses policy implications for the governance of AI systems built using RLHF.
   - **Year**: 2023

8. **Title**: Combining Theory of Mind and Kindness for Self-Supervised Human-AI Alignment (arXiv:2411.04127)
   - **Authors**: Joshua T. S. Hewson
   - **Summary**: This work proposes a human-inspired approach to AI alignment by integrating Theory of Mind and kindness into self-supervised learning. It addresses concerns about current AI models' lack of social intelligence and understanding of human values, aiming to create AI systems capable of safe and responsible decision-making in complex situations.
   - **Year**: 2024

9. **Title**: Improving Human-AI Collaboration With Descriptions of AI Behavior (arXiv:2301.06937)
   - **Authors**: √Ångel Alexander Cabrera, Adam Perer, Jason I. Hong
   - **Summary**: The authors propose using behavior descriptions to enhance human-AI collaboration. Through user studies across various domains, they demonstrate that providing details of AI systems' performance on subgroups of instances can increase human-AI accuracy by helping users identify AI failures and appropriately rely on AI assistance.
   - **Year**: 2023

10. **Title**: In situ bidirectional human-robot value alignment
    - **Authors**: Not specified
    - **Summary**: This study investigates bidirectional communication between humans and robots to achieve value alignment in collaborative tasks. It proposes an explainable AI system where robots predict users' values through in situ feedback while communicating their decision processes, demonstrating that real-time human-robot mutual understanding is achievable.
    - **Year**: 2024

**Key Challenges**

1. **Dynamic Nature of Human Values**: Human values are not static; they evolve over time and across different contexts. Capturing and modeling this dynamism poses a significant challenge for AI systems aiming to maintain alignment.

2. **Bidirectional Communication Complexity**: Establishing effective two-way communication between humans and AI systems requires sophisticated mechanisms for mutual understanding, which can be difficult to design and implement.

3. **Contextual Variability**: Human values and preferences can vary significantly across cultures, situations, and individual experiences, making it challenging to develop AI systems that are universally aligned.

4. **Ethical and Policy Considerations**: Aligning AI with human values involves navigating complex ethical landscapes and policy implications, especially when values conflict or are ambiguous.

5. **Scalability of Alignment Mechanisms**: Developing alignment strategies that are scalable and adaptable to various AI applications and human interactions remains a significant hurdle. 