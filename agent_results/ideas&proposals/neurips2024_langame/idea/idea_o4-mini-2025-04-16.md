Title: PlanCraft – Self-Play Language Games to Enhance LLM Planning

Motivation:  
Large language models excel at pattern completion but often struggle with coherent multi-step planning and real‐world grounding. Traditional supervised finetuning and preference-based losses offer limited feedback on plan feasibility. Introducing interactive, game‐based training can bootstrap strategic reasoning through iterative self-play, overcoming planning deficiencies and fostering robust decision‐making.

Main Idea:  
We propose PlanCraft, a deep reinforcement-learning framework where two LLM agents—Planner and Evaluator—engage in a language game.  
• Planner: given a high-level goal (e.g., “organize a virtual conference”), generates a sequence of actionable steps.  
• Evaluator: simulates environment responses via a lightweight state tracker or symbolic simulator, critiques the Planner’s steps, and assigns rewards based on coherence, efficiency, and safety.  
Agents are jointly finetuned via Proximal Policy Optimization (PPO), iterating until Plans achieve high success rates in simulated tasks.  
Expected outcomes include improved chain-of-thought clarity, reduced hallucinations in multi-step tasks, and emergent planning heuristics.  
Potential impact spans more reliable dialogue agents, autonomous robotics instructions, and a general paradigm for interactive LLM self-improvement.