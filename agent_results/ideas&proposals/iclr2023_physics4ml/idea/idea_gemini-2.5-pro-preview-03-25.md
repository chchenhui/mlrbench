**Title:** Lagrangian-Inspired Recurrent Networks for Sequence Modeling

**Motivation:** Standard recurrent networks (RNNs) and Transformers often lack structured inductive biases reflecting underlying dynamics in sequential data, potentially limiting efficiency and interpretability, especially for time-series with physical origins. Lagrangian mechanics provides a powerful framework for describing dynamical systems based on energy principles.

**Main Idea:** We propose Lagrangian Recurrent Networks (LRNs) where the sequence dynamics are modeled via a learned Lagrangian function L(q, qÌ‡), representing the system's kinetic and potential energy. The hidden state evolves according to the Euler-Lagrange equations derived from L. Inputs can act as external forces or modify the potential energy landscape. This approach embeds physical principles (like energy conservation under certain conditions via Noether's theorem) directly into the architecture. We expect LRNs to offer improved sample efficiency, better generalization for long-range dependencies in structured sequences (e.g., physical simulations, robotics, potentially econometrics), and enhanced interpretability through analysis of the learned Lagrangian L.