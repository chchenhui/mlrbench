Title: Hamiltonian Normalizing Flows for Energy-Preserving Generative Modeling

Motivation:  
Normalizing flows provide invertible mappings for density estimation but often require expensive Jacobian determinants and can distort volume in high-dimensional spaces. Embedding Hamiltonian structure, which by Liouville’s theorem preserves phase-space volume, promises more stable training, exact invertibility, and natural incorporation of physical symmetries.

Main Idea:  
We propose to parameterize a normalizing flow as a learned Hamiltonian system Hθ(x,p), where x is the data coordinate and p is an auxiliary momentum. Starting from a simple Gaussian in (x,p), samples are evolved via a symplectic integrator that exactly preserves volume and a learned “energy.” Because the flow is volume-preserving, the log-density change is zero, eliminating Jacobian evaluations. Hθ is implemented with an SE(n)-equivariant neural network to embed rotational and translational symmetries when known. Training maximizes the exact data log-likelihood by backpropagating through the integrator. We expect this approach to yield stable convergence, improved sample fidelity in image and molecular domains, and a framework for injecting conservation laws and symmetries directly into generative models.