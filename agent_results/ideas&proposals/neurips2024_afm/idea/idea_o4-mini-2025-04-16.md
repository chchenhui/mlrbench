Title: FusionAdapter – Retrieval-Augmented Modular Adapters for Continual Personalized Adaptation

Motivation:  
Personalized foundation models must integrate up-to-date knowledge and user-specific preferences without incurring heavy compute or catastrophic forgetting. Existing personalization techniques often overlook evolving user contexts or demand full-model retraining, limiting scalability and adaptability.

Main Idea:  
We propose FusionAdapter, a collection of lightweight LoRA-based adapter modules attached to a frozen foundation model. Three adapter types—Global-RAG, User-History, and Situational-Context—each specialize in retrieving and encoding salient knowledge from external corpora, personal logs, or the current session. A learned meta-controller dynamically routes input through the most relevant adapters and fuses their outputs via gated attention. Continuous adaptation is achieved by fine-tuning only the adapter weights on streaming user interactions, with an elastic weight consolidation penalty to preserve prior expertise. Prompt-based gating filters retrieved documents for coherence and relevance. FusionAdapter adds <5% parameter overhead, yields up-to-date, preference-aligned generations, and demonstrates robustness against forgetting—enabling efficient, evolving LLM personalization in real-world deployments.