Title: SpatiaSonic – Diffusion-Based Spatial Audio Generation from 360° Video

Motivation:  
Immersive VR/AR applications require realistic spatial audio but manually designing multi-channel soundscapes is time-consuming and costly. Automatically generating spatialized audio from visual input can streamline content creation, enhance presence in virtual environments, and democratize high-quality immersive media.

Main Idea:  
SpatiaSonic is a three-stage pipeline that fuses 360° video analysis with diffusion-based audio synthesis.  
1. Scene Understanding: A multi-task vision model extracts depth, object trajectories, and semantic event labels from equirectangular video frames.  
2. Acoustic Simulation: Using estimated room geometry and object positions, we compute coarse multi-channel room impulse responses (RIRs) and source direction metadata.  
3. Conditional Diffusion Synthesis: A U-Net diffusion model generates spatial audio by conditioning on semantic tokens (e.g., “footsteps,” “waterfall”) and simulated RIRs, producing multi-channel waveforms aligned with visual events.  

We will evaluate spatial fidelity using objective measures (directional accuracy, reverberation consistency) and user studies. SpatiaSonic promises to automate high-quality spatial audio production for VR/AR, gaming, and telepresence.