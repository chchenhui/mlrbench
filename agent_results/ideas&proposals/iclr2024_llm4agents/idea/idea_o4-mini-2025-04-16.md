Title: Episodic Knowledge Graph Memory for LLM Agents

Motivation:  
LLM agents often struggle to maintain consistent context and recall past experiences over extended interactions. Stateless prompting limits long-term coherence in reasoning and planning. Introducing a structured, dynamic memory mechanism can bridge this gap and empower agents to leverage past events when tackling complex, multi-step tasks.

Main Idea:  
We propose augmenting LLM agents with a dynamic episodic knowledge graph that encodes each interaction episode as a node and contextual relationships as edges. During runtime, the agent:  
1. Extracts event representations via transformer-based embeddings.  
2. Updates the graph using a lightweight graph neural network (GNN) that captures temporal and semantic relations.  
3. Retrieves relevant subgraphs through attention-guided queries to construct succinct “memory prompts” for the LLM.  
4. Periodically consolidates semantically overlapping nodes and prunes outdated entries to control graph size.  

We will evaluate this architecture on simulated navigation and multi-step planning benchmarks, measuring improvements in task success, reasoning depth, and prompt efficiency. By instilling structured, long-term memory, this approach aims to enhance LLM agents’ consistency and autonomy in real-world and simulated environments.