**Title:** Characterizing SSL's Practical Edge: Robustness to Data Scarcity and Domain Shift

**Motivation:** While SSL matches supervised performance benchmarks, its practical advantages in challenging real-world scenarios like limited labeled data or distribution shifts remain unclear. Understanding where SSL truly excels beyond standard benchmarks is crucial for its effective deployment and justifies the often significant pre-training cost.

**Main Idea:** We propose a systematic empirical study comparing SSL (pre-training + fine-tuning) versus purely supervised learning. We will evaluate performance across diverse tasks (e.g., image classification, NLP sentiment analysis) while rigorously varying two key factors: (1) the quantity of labeled data available for fine-tuning/supervised training (from few-shot to larger subsets) and (2) the degree of domain shift between the (pre-)training data and the evaluation data. The expected outcome is a quantitative map identifying the specific conditions (task types, scarcity levels, shift magnitudes) where SSL provides significant robustness and generalization advantages over supervised methods, offering practical guidance for model selection.