Title: Sketchable Low-Rank Adapter Tuning with Provable Guarantees

Motivation:  
Fine-tuning large pre-trained models remains computationally and memory intensive, especially in resource-constrained settings. While low-rank adapters reduce parameter overhead, existing methods lack theoretical guarantees on their approximation quality and generalization behavior.  

Main Idea:  
We propose a sketch-based subspace estimation framework to build adaptive low-rank adapters with provable error bounds. At each fine-tuning step, structured random projections (“sketches”) approximate the dominant directions of the gradient or Fisher-information matrix, yielding a compact subspace that captures most weight-update energy. An iterative subspace‐refinement routine refines this sketch, dynamically adjusting the adapter’s rank to satisfy a user‐specified approximation tolerance. We derive theoretical bounds linking sketch dimension and adapter rank to the fine-tuned model’s generalization gap. Empirically, we integrate our Sketchable Low-Rank Adapter into transformer models and show 80% parameter reduction, 4× fine-tuning speedup, and negligible performance loss on NLP benchmarks. This method unites theoretical guarantees with practical efficiency, enabling scalable fine-tuning on constrained hardware.