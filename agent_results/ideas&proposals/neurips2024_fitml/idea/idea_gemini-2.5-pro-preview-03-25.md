**Title:** AdaRank: Adaptive Rank Allocation for Parameter-Efficient Fine-Tuning

**Motivation:** Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) significantly reduce computational costs but typically rely on a fixed, manually chosen rank `r` for update matrices across all layers. This fixed rank might be suboptimal, leading to either under-capacity for complex adaptations or over-parameterization for simpler ones, hindering optimal efficiency and performance trade-offs.

**Main Idea:** We propose AdaRank, a method to dynamically allocate rank budgets during the fine-tuning process. AdaRank monitors metrics indicative of parameter utility (e.g., gradient magnitude, Fisher information, or importance scores derived during training) for the low-rank update matrices (A and B in LoRA). Based on these metrics, it iteratively reallocates the total rank budget, increasing rank for layers/components deemed more important for the specific downstream task and decreasing it for less critical ones, while maintaining the overall parameter budget. This approach aims to learn a task-specific, optimal rank distribution automatically, leading to improved performance for a given parameter count compared to fixed-rank PEFT methods. We will evaluate AdaRank on diverse LLM fine-tuning tasks.