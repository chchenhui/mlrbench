# Title: Sim2Act: Self-Supervised Action Data Generation for Multi-Modal Decision Making Models

## Introduction

Foundation models, pretrained on diverse vision and language datasets, have shown remarkable capabilities in various downstream tasks. However, their effectiveness in sequential decision-making tasks, such as planning and control, is limited by the lack of action-conditioned data. This proposal aims to bridge this gap by automatically generating large-scale, paired (observation, language, action) datasets using simulated environments. The primary goal is to improve sample efficiency and generalization in downstream control, planning, and robotics applications.

### Background

Foundation models excel in vision-language understanding but struggle with tasks that require actions. Sequential decision-making tasks, such as playing board games, navigating environments, and manipulating objects, require models to not only understand the state of the environment but also to predict and execute actions to achieve specific goals. Traditional reinforcement learning (RL) methods, while effective in some cases, often require extensive manual tuning and large amounts of data to achieve good performance.

### Research Objectives

1. **Data Generation**: Develop a framework to automatically generate large-scale, paired (observation, language, action) datasets using simulated environments.
2. **Model Training**: Fine-tune a vision-language model with an action-prediction head using contrastive learning and behavior cloning.
3. **Iterative Improvement**: Iteratively improve the policies generated by the model to bootstrap more complex interactions and long-horizon behaviors.
4. **Evaluation**: Validate the performance of the Sim2Act framework in both simulated and real-world robotic settings.

### Significance

The proposed Sim2Act framework addresses a critical challenge in the integration of foundation models with sequential decision-making tasks. By generating action-annotated data, we aim to significantly enhance the sample efficiency and generalization capabilities of foundation models in control, planning, and robotics applications. This work has the potential to advance the state-of-the-art in multi-modal decision-making models and facilitate the deployment of foundation models in real-world scenarios.

## Methodology

### Data Generation

The data generation process involves three main steps:

1. **Task Description Sampling**: Sample natural language task descriptions from a diverse set of scenarios.
2. **Policy Proposal**: Use a base foundation model to propose exploratory policies in the simulator based on the sampled task descriptions.
3. **Data Logging**: Log (visual frame, task prompt, executed action) triplets as the agent interacts with the simulator.

### Model Architecture

The vision-language model used in this framework is augmented with an action-prediction head. The architecture consists of the following components:

- **Vision Encoder**: Encodes visual frames into a feature space.
- **Language Encoder**: Encodes task prompts into a feature space.
- **Action Decoder**: Predicts actions based on the concatenated visual and language features.

The action-prediction head is trained using a combination of contrastive learning and behavior cloning. The contrastive learning objective encourages the model to predict actions that are consistent with the observed state and task prompt. The behavior cloning objective ensures that the model's actions are consistent with the actions taken during the data generation process.

### Training Procedure

The training procedure involves the following steps:

1. **Pre-training**: Fine-tune the vision-language model using the generated (observation, language, action) datasets.
2. **Iterative Refinement**: Use the pre-trained model to generate new policies in the simulator, log new (observation, language, action) triplets, and retrain the model.
3. **Evaluation**: Periodically evaluate the performance of the model on a held-out test set and adjust the training procedure as necessary.

### Evaluation Metrics

The performance of the Sim2Act framework is evaluated using the following metrics:

- **Action Prediction Accuracy**: Measures the accuracy of the model's action predictions.
- **Task Completion Rate**: Measures the percentage of tasks completed by the model.
- **Sample Efficiency**: Measures the amount of data required to achieve a certain level of performance.
- **Generalization to Real-World Scenarios**: Evaluates the model's performance in real-world robotic settings.

## Expected Outcomes & Impact

### Technical Outcomes

1. **Large-Scale Action-Annotated Datasets**: Generate large-scale, paired (observation, language, action) datasets that can be used to train and evaluate multi-modal decision-making models.
2. **Improved Sample Efficiency**: Enhance the sample efficiency of foundation models in sequential decision-making tasks.
3. **Generalization to Real-World Scenarios**: Improve the generalization capabilities of foundation models in real-world robotic settings.

### Scientific Impact

The Sim2Act framework addresses several key challenges in the integration of foundation models with sequential decision-making tasks. By generating action-annotated data, we aim to significantly advance the state-of-the-art in multi-modal decision-making models. The proposed framework has the potential to facilitate the deployment of foundation models in a wide range of real-world applications, including autonomous driving, robotics, and healthcare.

### Practical Impact

The practical impact of the Sim2Act framework includes:

- **Enhanced Robotic Capabilities**: Improve the decision-making capabilities of robots in complex, dynamic environments.
- **Reduced Data Collection Costs**: Reduce the need for extensive data collection in real-world settings by leveraging simulated environments.
- **Improved Sample Efficiency**: Enhance the sample efficiency of foundation models, making them more practical for real-world deployment.
- **Broad Applicability**: Facilitate the deployment of foundation models in a wide range of applications, including autonomous driving, robotics, and healthcare.

## Conclusion

The Sim2Act framework addresses a critical challenge in the integration of foundation models with sequential decision-making tasks. By automatically generating large-scale, paired (observation, language, action) datasets, we aim to significantly enhance the sample efficiency and generalization capabilities of foundation models in control, planning, and robotics applications. The proposed framework has the potential to advance the state-of-the-art in multi-modal decision-making models and facilitate the deployment of foundation models in real-world scenarios.