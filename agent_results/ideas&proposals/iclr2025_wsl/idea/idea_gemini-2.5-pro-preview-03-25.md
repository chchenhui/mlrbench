**Title:** Graph Neural Networks for Permutation-Invariant Weight Space Analysis

**Motivation:** Analyzing neural network weights can reveal crucial information about model properties, training dynamics, and potential vulnerabilities. However, the inherent permutation symmetry (equivalent networks can have permuted neurons yielding different weight tensors) presents a major challenge for standard analysis techniques that treat weights as fixed vectors or matrices.

**Main Idea:** We propose using Graph Neural Networks (GNNs) to analyze neural network weights, inherently handling permutation symmetries. We represent a network (or layer) as a graph where nodes are neurons and edges represent connections with associated weights. GNNs can learn functions on these graphs that are invariant or equivariant to node permutations. We will train a GNN on a dataset of model weights (a "model zoo") to predict properties like task performance, robustness to adversarial attacks, or the presence of backdoors, directly from the weight graph representation. This approach bypasses the need for canonical weight representations and leverages the structural information within networks for more robust and insightful model analysis.