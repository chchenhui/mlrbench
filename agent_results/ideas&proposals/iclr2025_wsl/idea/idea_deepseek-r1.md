**Title:** Equivariant Contrastive Learning for Invariant Weight Embeddings in Model Zoos  

**Motivation:** With millions of pretrained models available, efficiently analyzing and retrieving models based on functional properties—without retraining or fine-tuning—is critical. Current approaches struggle with weight space symmetries (e.g., permuted neurons) and computational costs, limiting scalability and cross-model comparisons.  

**Main Idea:** Propose a contrastive learning framework to create symmetry-invariant embeddings of neural network weights. Use permutation-equivariant graph neural networks (GNNs) to process weight tensors as graphs, ensuring embeddings are invariant to architecture-specific symmetries. Train via contrastive loss to cluster models with similar functionality (e.g., task performance, robustness) while distancing dissimilar ones. Validate on model zoo datasets by predicting properties (e.g., accuracy, backdoor presence) and enabling tasks like model retrieval or lineage analysis. Expected outcomes include a unified embedding space for cross-architecture comparisons, efficient model triage, and improved interpretability. Potential impact: Democratizes model reuse, accelerates scientific analysis of pretrained networks, and provides a foundation for model editing/merging workflows.