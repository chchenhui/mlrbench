**Title:** Structured Latent Spaces for Permutation-Invariant Neural Weight Compression and Reuse  

**Motivation:** Neural network weights contain rich information about model behavior but are often underutilized due to their high dimensionality, permutation invariance (sensitivity to neuron reordering), and lack of structured compression. Efficient weight representations could enable downstream tasks like model merging, transfer learning, and rapid deployment. However, existing methods (e.g., pruning, distillation) neglect global weight structure, and permutation-invariant representations remain poorly addressed. Developing interpretable, compressed weight spaces could democratize model reuse and reduce training overhead.  

**Main Idea:** We propose a permutation-equivariant autoencoder framework to learn low-dimensional, disentangled latent representations of neural network weights. The encoder will leverage graph neural networks (GNNs) to model neuron-wise symmetries, explicitly encoding invariances (e.g., permutation, scaling) into the latent space. The decoder will reconstruct full-scale models while preserving task performance. Key innovations include (1) a novel loss function balancing weight reconstruction and permutation invariance, and (2) clustering and interpolation experiments in the latent space for zero-shot model adaptation. We expect structured latent spaces to reveal semantic relationships between weights (e.g., "depth" vs. "width" dimensions) and enable practical applications like model compression, ensembling via interpolation, and task-specific weight sampling for efficient hypernetworks. Impact includes simplifying transfer learning pipelines and enabling interpretable model editing.