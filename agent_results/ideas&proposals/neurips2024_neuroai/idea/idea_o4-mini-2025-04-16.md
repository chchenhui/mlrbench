Title: Hierarchical Spiking Predictive Coding with Active Inference for Neuromorphic Vision

Motivation: Biological brains achieve real-time perception and decision-making under tight energy and data constraints by continually predicting sensory input and acting to minimize surprise. Translating this self-supervised predictive coding and active inference loop to neuromorphic hardware promises low-power, robust vision systems that learn from few examples and adapt on the fly.

Main Idea: We propose a multilayer spiking neural network (SNN) architecture on Intel Loihi (or similar) that implements hierarchical predictive coding: top-down pathways generate sensory predictions, bottom-up spikes convey prediction errors, and local Hebbian-style updates adjust synapses without labels. An active inference module uses error signals to drive a pan/tilt event-based camera, selecting viewpoints that optimally reduce uncertainty. Training is purely self-supervised via continuous prediction error minimization. We will evaluate on event-based object tracking and few-shot gesture recognition, measuring energy usage, adaptation speed, and robustness under occlusions. Expected outcomes include ultra-low-power perception, rapid learning in dynamic scenes, and insights into brain-like predictive learning on hardware.