**Title:** Proactive Reliability Shaping via Causal Analysis of Pre-training Data

**Motivation:** Foundation model unreliability often stems from subtle issues within vast pre-training datasets (e.g., spurious correlations, low-quality text inducing hallucinations). Current interventions are typically reactive during fine-tuning. Identifying and mitigating these risks proactively during pre-training is critical for building fundamentally more reliable models.

**Main Idea:** We propose a methodology to causally link pre-training data characteristics to downstream model unreliability. First, segment the pre-training corpus based on metadata or learned representations (e.g., topic, style, source quality). Then, train smaller "probe" FMs on different data segment combinations or use attribution techniques scaled to large models. Analyze how specific data segments causally influence failure modes like hallucination rates or bias metrics on targeted benchmarks. The outcome will be a causal map identifying high-risk data properties. This enables data curation strategies (filtering, down-weighting problematic segments) *before* full-scale pre-training, fostering more inherently reliable FMs.