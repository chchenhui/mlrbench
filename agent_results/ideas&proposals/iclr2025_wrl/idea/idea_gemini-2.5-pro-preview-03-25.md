**Title:** Adaptive Skill Composition using Large Multi-modal Models for Long-Horizon Tasks

**Motivation:** Robots performing complex, long-horizon tasks like "tidy the room" require reasoning over multi-modal inputs (visual scene, language instruction) and composing sequences of primitive skills. Current methods often lack robustness to variations or fail to generalize composition strategies. Large Multi-modal Models (LMMs) excel at understanding context but struggle with robust, closed-loop execution and skill sequencing adaptable to dynamic environmental states.

**Main Idea:** We propose integrating a pre-trained LMM with a library of learned low-level robot skills (e.g., pick, place, wipe). Given a high-level command and visual context, the LMM generates a conditional sequence plan of required skills. Crucially, instead of just outputting a fixed sequence, the LMM outputs a probabilistic policy over skill transitions, conditioned on the current state observed through robot sensors. This allows adapting the plan mid-execution based on real-world feedback (e.g., if an object is unexpectedly missing). We will leverage reinforcement learning or imitation learning from human demonstrations to fine-tune the LMM's skill sequencing policy for specific task domains, improving grounding and execution reliability in unstructured environments. Expected outcome: A robot system demonstrating improved generalization and adaptability in complex, multi-step household tasks.