# Adaptive Invariant Feature Extraction using Synthetic Interventions (AIFS): A Framework for Mitigating Spurious Correlations in Deep Learning Models

## 1. Introduction

### Background
Deep learning models have demonstrated remarkable performance across various domains, yet they often fail to generalize when deployed in real-world settings. A primary reason for this failure is the tendency of these models to rely on spurious correlations â€“ statistical associations between features and labels that are specific to the training distribution but do not reflect causal relationships. These spurious correlations, or "shortcuts," emerge due to the statistical nature of learning algorithms and their inductive biases during training (Ye et al., 2024).

The problem is particularly pronounced when certain feature-label associations are overrepresented in training data. For instance, a model trained to identify cows might rely on the presence of grass in images rather than the animal's actual features. When the model encounters cows in different environments (e.g., on a beach), it fails to make correct predictions. Such reliance on spurious features undermines model reliability, fairness, and safety in real-world applications, especially when dealing with underrepresented groups or minority populations.

Current approaches to address this issue generally fall into two categories: (1) methods that require explicit knowledge or annotation of spurious features or group labels, and (2) architectural or algorithmic modifications that aim to promote invariance. While the first category can be effective, obtaining annotations for spurious correlations is often impractical or impossible, as many spurious relationships remain undetected by human annotators. The second category offers more generalizable solutions but may struggle to target specific spurious correlations effectively.

### Research Objectives
This research proposes Adaptive Invariant Feature Extraction using Synthetic Interventions (AIFS), a novel framework that automatically discovers and neutralizes spurious correlations in deep learning models without requiring explicit annotations of spurious features. The specific objectives of this research are:

1. To develop a mechanism for dynamically identifying latent dimensions in neural networks that correspond to potentially spurious features
2. To design an intervention-based training framework that synthesizes counterfactual examples to promote invariant predictions
3. To implement an adaptive learning process that iteratively refines the model's focus on causal rather than spurious features
4. To validate the effectiveness of AIFS across different data modalities and applications, demonstrating its generalizability

### Significance
The proposed research addresses a fundamental challenge in machine learning that has significant implications for the reliability, fairness, and safety of AI systems. By developing methods that automatically discover and mitigate spurious correlations without requiring explicit annotations, AIFS could enable more robust models across various domains:

1. **Theoretical significance**: AIFS bridges the gap between causal inference and representation learning, providing insights into how deep networks encode both causal and spurious features in their latent spaces.

2. **Practical significance**: The framework offers a generalizable approach to improving model robustness without domain-specific knowledge or costly annotations, making it applicable across diverse applications from healthcare to autonomous systems.

3. **Societal significance**: By reducing reliance on spurious correlations, models trained with AIFS could make fairer predictions for underrepresented groups, addressing concerns about AI bias and discrimination.

Unlike previous approaches that either require explicit group labels (Wen et al., 2024) or focus only on specific network layers (Izmailov et al., 2022; Hameed et al., 2024), AIFS provides an end-to-end solution that adaptively identifies and addresses spurious correlations throughout the learning process, offering a more comprehensive approach to building robust AI systems.

## 2. Methodology

### 2.1 Framework Overview

The AIFS framework consists of four main components:
1. A pretrained encoder that maps inputs to latent representations
2. An intervention module that applies controlled perturbations to latent subspaces
3. A classifier that makes predictions based on the (potentially perturbed) latent representations
4. An attribution mechanism that identifies latent dimensions most sensitive to interventions

These components work together in an iterative process to identify and neutralize spurious correlations during training. Figure 1 illustrates the overall architecture of the AIFS framework.

### 2.2 Latent Representation and Intervention Mechanism

Let $\mathcal{X}$ be the input space and $\mathcal{Y}$ be the output space. We assume a data generating process where there exists a set of causal features $C$ and spurious features $S$ such that $Y$ is causally determined by $C$, while $S$ may be correlated with $Y$ in the training distribution but not causally linked.

We begin with an encoder function $E: \mathcal{X} \rightarrow \mathcal{Z}$ that maps inputs to a latent space $\mathcal{Z}$. The encoder can be pretrained using self-supervised techniques or transferred from a foundation model. For each input $x$, we obtain a latent representation $z = E(x)$.

The intervention module applies controlled perturbations to specific dimensions of the latent representation. We parameterize this module as:

$$z'_i = z_i + m_i \cdot \delta_i$$

where:
- $z'_i$ is the perturbed value for the $i$-th dimension of the latent representation
- $m_i \in [0,1]$ is a learned mask value indicating the degree of intervention for dimension $i$
- $\delta_i \sim \mathcal{N}(0, \sigma^2)$ is a random perturbation sampled from a Gaussian distribution

The mask values $\mathbf{m} = \{m_1, m_2, ..., m_d\}$ are learned parameters that adaptively control which dimensions receive stronger interventions. Initially, all mask values are set to a small constant (e.g., 0.1).

### 2.3 Training Objectives

The AIFS framework employs a multi-objective training approach with three key components:

1. **Classification Loss**: A standard supervised loss for the primary task
   $$\mathcal{L}_{cls} = \mathbb{E}_{(x,y) \sim \mathcal{D}}[\ell(f(E(x)), y)]$$
   where $f$ is the classifier and $\ell$ is an appropriate loss function (e.g., cross-entropy for classification).

2. **Invariance Loss**: Encourages consistent predictions between original and perturbed representations
   $$\mathcal{L}_{inv} = \mathbb{E}_{x \sim \mathcal{D}}[d(f(E(x)), f(E(x) + \mathbf{m} \odot \boldsymbol{\delta}))]$$
   where $d$ is a divergence measure (e.g., KL divergence for classification) and $\odot$ represents element-wise multiplication.

3. **Sensitivity Loss**: Penalizes the model for relying on dimensions that receive strong interventions
   $$\mathcal{L}_{sens} = \mathbb{E}_{(x,y) \sim \mathcal{D}}[\ell(f(E(x) + \mathbf{m} \odot \boldsymbol{\delta}), y) \cdot \|\mathbf{m}\|_1]$$
   where $\|\mathbf{m}\|_1$ is the L1 norm of the mask, encouraging sparsity in interventions.

The overall training objective is a weighted combination of these losses:
$$\mathcal{L} = \mathcal{L}_{cls} + \lambda_{inv}\mathcal{L}_{inv} + \lambda_{sens}\mathcal{L}_{sens}$$
where $\lambda_{inv}$ and $\lambda_{sens}$ are hyperparameters controlling the relative importance of each objective.

### 2.4 Adaptive Mask Update Mechanism

The critical innovation in AIFS is the adaptive mechanism for updating the intervention masks. Every $k$ training steps, we perform the following:

1. **Attribution Analysis**: For each dimension $i$ in the latent space, compute its contribution to the prediction by approximating the gradient of the output with respect to that dimension:
   $$a_i = \mathbb{E}_{(x,y) \sim \mathcal{D}_{val}}\left[\left|\frac{\partial f(E(x))_y}{\partial E(x)_i}\right|\right]$$
   where $\mathcal{D}_{val}$ is a small validation set and $f(E(x))_y$ is the predicted probability for the true class $y$.

2. **Mask Update**: Update mask values based on the attribution scores:
   $$m_i \leftarrow \beta \cdot m_i + (1-\beta) \cdot \text{normalize}(a_i)$$
   where $\beta$ is a momentum coefficient and normalize scales the attribution scores to [0,1].

3. **Intervention Strength Adaptation**: Adjust the overall intervention strength $\sigma$ based on validation performance:
   $$\sigma \leftarrow \sigma \cdot (1 + \gamma \cdot (\text{acc}_{base} - \text{acc}_{robust}))$$
   where $\text{acc}_{base}$ is accuracy on the validation set without interventions, $\text{acc}_{robust}$ is accuracy with interventions, and $\gamma$ is a scaling factor.

This adaptive process ensures that dimensions more strongly associated with predictions (potentially including spurious correlations) receive stronger interventions, forcing the model to rely on more robust features.

### 2.5 Implementation Details

The complete AIFS algorithm is outlined below:

1. Initialize encoder $E$ (pretrained or randomly initialized)
2. Initialize classifier $f$ and intervention masks $\mathbf{m}$ with small values
3. For training epochs $t = 1$ to $T$:
   a. For each mini-batch $(x, y)$:
      i. Compute latent representation $z = E(x)$
      ii. Sample perturbations $\boldsymbol{\delta} \sim \mathcal{N}(0, \sigma^2 \cdot \mathbf{I})$
      iii. Apply masked interventions: $z' = z + \mathbf{m} \odot \boldsymbol{\delta}$
      iv. Compute predictions: $\hat{y} = f(z)$ and $\hat{y}' = f(z')$
      v. Compute total loss $\mathcal{L}$ and update parameters
   b. If $t \mod k = 0$:
      i. Perform attribution analysis on validation set
      ii. Update intervention masks $\mathbf{m}$
      iii. Adjust intervention strength $\sigma$

### 2.6 Experimental Design

To validate the effectiveness of AIFS, we will conduct experiments on multiple datasets with known spurious correlations:

1. **Image classification**: 
   - Waterbirds dataset (Sagawa et al., 2020) where birds are spuriously correlated with backgrounds
   - CelebA dataset with spurious correlations between gender and hair color
   - A modified MNIST dataset with color backgrounds correlated with digits

2. **Tabular data**:
   - Adult Income dataset with potential spurious correlations between gender, race, and income
   - Medical diagnosis datasets where symptoms may be spuriously correlated with conditions

3. **Text classification**:
   - Sentiment analysis datasets where specific words or phrases may be spuriously correlated with sentiment

For each dataset, we will evaluate the following:

1. **Overall accuracy**: Performance on the standard test set
2. **Worst-group accuracy**: Performance on the most challenging subgroup
3. **Robustness to distribution shifts**: Performance on artificially shifted test distributions
4. **Interpretability**: Analysis of which features the model relies on after training

We will compare AIFS against the following baselines:
- Standard empirical risk minimization (ERM)
- Group distributionally robust optimization (GroupDRO) (Sagawa et al., 2020)
- JTT (Liu et al., 2021)
- ElRep (Wen et al., 2024)
- SPUME (Zheng et al., 2024)

### 2.7 Evaluation Metrics

We will use the following metrics to evaluate model performance:

1. **Average accuracy**: Standard classification accuracy on the test set
2. **Worst-group accuracy**: The accuracy on the most challenging subgroup
3. **Accuracy gap**: The difference between average accuracy and worst-group accuracy
4. **AUC-ROC**: Area under the receiver operating characteristic curve
5. **Expected calibration error (ECE)**: Measure of model calibration
6. **Feature attribution alignment**: Correlation between feature attributions and known causal factors

## 3. Expected Outcomes & Impact

### 3.1 Expected Outcomes

The proposed AIFS framework is expected to yield several significant outcomes:

1. **Improved robustness to spurious correlations**: Models trained with AIFS should demonstrate higher worst-group accuracy and smaller gaps between average and worst-group performance compared to standard training methods.

2. **Adaptive discovery of spurious features**: Through the intervention mask learning process, AIFS should automatically identify latent dimensions corresponding to spurious correlations, providing insights into what features the model initially relies on.

3. **Generalizable intervention framework**: The approach should work across different data modalities and model architectures without requiring domain-specific knowledge or annotations of spurious features.

4. **Insights into causal representation learning**: The patterns of mask values and their evolution during training will offer insights into how deep networks encode causal vs. spurious information in their latent spaces.

5. **Better model interpretability**: By encouraging focus on causal rather than spurious features, models trained with AIFS should produce more interpretable and trustworthy predictions.

### 3.2 Broader Impact

The AIFS framework has potential for broad impact across several domains:

1. **Healthcare and medicine**: By reducing reliance on spurious correlations, diagnostic models could make more reliable predictions for underrepresented patient groups, leading to more equitable healthcare outcomes.

2. **Autonomous systems**: Self-driving vehicles and robots equipped with perception systems trained using AIFS could better generalize to novel environments where visual cues differ from training data.

3. **Natural language processing**: Language models could avoid relying on stereotypical associations or stylistic artifacts, producing more accurate and fair content across diverse contexts.

4. **Financial services**: Risk assessment models could focus on genuinely predictive factors rather than spurious correlations with protected attributes, improving fairness in lending and insurance.

5. **Scientific discovery**: In fields like drug discovery or materials science, models trained with AIFS could identify truly causal relationships rather than dataset-specific correlations.

### 3.3 Limitations and Future Work

While AIFS presents a promising approach to addressing spurious correlations, several limitations warrant acknowledgment and future research:

1. **Computational overhead**: The intervention process and adaptive mask updates introduce additional computational complexity compared to standard training.

2. **Hyperparameter sensitivity**: The balance between classification, invariance, and sensitivity losses may require careful tuning for optimal performance.

3. **Foundation for future research**: AIFS establishes a framework that could be extended in several directions:
   - Integration with causal discovery algorithms to explicitly identify causal structures
   - Extension to self-supervised and unsupervised learning scenarios
   - Application to multi-modal data where spurious correlations may exist across modalities
   - Theoretical analysis of the intervention mechanism's impact on the optimization landscape

In conclusion, the proposed AIFS framework addresses a fundamental challenge in machine learning by automatically discovering and mitigating spurious correlations without requiring explicit annotations. By promoting invariance to synthetic interventions in latent space, AIFS encourages models to focus on truly causal features, leading to more robust and generalizable predictions. This research contributes to the broader goal of building AI systems that are reliable, fair, and trustworthy, even in the presence of distribution shifts and underrepresented groups.