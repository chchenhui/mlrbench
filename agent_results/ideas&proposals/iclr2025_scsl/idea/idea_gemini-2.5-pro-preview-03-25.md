**Title:** Proactive Robustification of Foundation Models Against Unknown Spurious Correlations

**Motivation:** Foundation models (LLMs/LMMs) risk learning unknown spurious correlations from vast datasets, hindering robustness and reliability. Identifying all potential spurious attributes *a priori* is impractical. This research proposes a method to proactively enhance robustness without predefined spurious labels.

**Main Idea:** We propose an iterative "probe-and-correct" framework. First, use post-hoc explanation methods (e.g., influence functions, counterfactual generation) to identify input features consistently driving model predictions across diverse data samples. Automatically cluster these influential features to hypothesize potential shortcuts (e.g., simple tokens, background objects). Next, generate minimally modified counterfactual inputs where hypothesized spurious features are altered but the core concept remains. Finally, fine-tune the model using a targeted regularization term or contrastive loss that penalizes reliance on these identified potential shortcuts, explicitly encouraging the use of more invariant, causal features. Expected outcome: Enhanced model robustness against unforeseen spurious correlations, improving real-world generalizability and trustworthiness.