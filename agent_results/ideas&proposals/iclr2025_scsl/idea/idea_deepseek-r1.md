**Title:** AutoSpurious: Automated Detection and Benchmarking of Spurious Correlations via Model Explainability  

**Motivation:** Current benchmarks for spurious correlations rely on human-annotated group labels, which are unscalable and fail to capture unknown or non-human-aligned biases. This limits robust evaluation of AI models, especially in multimodal or real-world scenarios. Automated methods to detect and benchmark spurious features are critical for advancing model reliability and generalization.  

**Main Idea:** This research proposes a framework to automatically identify spurious correlations in datasets using model explainability tools (e.g., feature attribution, attention maps) and synthesize evaluation benchmarks. First, trained models are analyzed to extract features disproportionately influencing predictions. These features are perturbed or masked in the original data to create "stress tests" for robustness. For multimodal data (e.g., image-text pairs), cross-modal feature interactions are analyzed to detect subtle spurious links (e.g., background objects in images tied to text labels). The framework generates a benchmark suite with controlled spurious correlations, enabling systematic evaluation across architectures and modalities. Expected outcomes include a scalable benchmark for unseen spurious patterns and insights into model vulnerability. Impact: Enables efficient robustness assessment without human annotation, applicable to LLMs, LMMs, and reinforcement learning agents.