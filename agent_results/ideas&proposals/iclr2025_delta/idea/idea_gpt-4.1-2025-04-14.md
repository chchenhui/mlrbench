Title: Analyzing and Enhancing the Implicit Bias of Regularization in Deep Generative Models

Motivation:  
Despite their impressive performance, deep generative models (DGMs) are often sensitive to overfitting and unstable training. Regularization techniques mitigate these issues, but their implicit bias—how they shape the learned representations and generated outputs—remains poorly understood. Unveiling this bias is crucial for principled improvement of generalization, robustness, and the interpretability of DGMs.

Main Idea:  
We propose a systematic study of the implicit bias induced by different explicit and implicit regularization methods (e.g., weight decay, dropout, batch/layer norm, spectral norm) in DGMs, including VAEs and GANs. The framework combines theoretical analysis (e.g., trajectory analysis in parameter space and induced priors in function space) with empirical investigations (e.g., probing latent space geometry and output diversity across datasets of varying complexity). Building on this understanding, we will introduce adaptive regularization schemes that adjust their effect dynamically during training, guided by metrics such as latent space coverage, sample diversity, and generalization error. The anticipated outcomes include sharper theoretical insights into regularization-induced biases in deep generative learning, practical adaptive regularizers improving robustness and stability, and guidelines for selecting regularization techniques based on dataset properties, thereby advancing the reliability and efficacy of DGMs in diverse real-world applications.