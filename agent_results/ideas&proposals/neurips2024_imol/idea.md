**Title:**  
Adaptive Contextual Goal Generation for Lifelong Learning via Hierarchical Intrinsic Motivation  

**Motivation:**  
Current intrinsically motivated agents struggle to generalize across dynamic environments and sustain long-term skill development due to rigid pre-defined goal spaces or static reward mechanisms. This limits their ability to autonomously adapt to novel contexts or repurpose learned skills creativelyâ€”a critical capability for real-world deployment. Addressing this gap requires methods that enable agents to dynamically align intrinsic goals with evolving environmental properties while balancing exploration and exploitation over time.  

**Main Idea:**  
We propose a hierarchical framework where an agent learns to generate and switch intrinsic goals *contextually* through a meta-reinforcement learning architecture. At the lower level, skill-specific policies are trained using curiosity-driven rewards (e.g., prediction error or information gain). At the meta-level, a goal-generation module adaptively selects high-level objectives by analyzing environmental statistics (e.g., sensor dimensionality, task complexity, or dynamical predictability) using an attention mechanism. For instance, in food-scarce environments, the agent prioritizes exploration goals (e.g., mapping terrain), while resource-rich settings trigger exploitation-focused goals (e.g., skill refinement). This system integrates incremental learning by retaining past skills in a library and composing them for novel contexts via few-shot transfer. Validation involves testing generalization across procedurally generated tasks (e.g., 3D navigation, multi-object manipulation) and comparing performance against static-goal baselines. Success metrics include task coverage, adaptation speed, and skill reusability. This approach could enable autonomous lifelong learning without external supervision, bridging the gap between curiosity-driven exploration and practical real-world deployment.