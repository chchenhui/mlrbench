Title: MetaSSL – Automated Auxiliary Task Discovery via Meta-Learning

Motivation: Designing auxiliary tasks in self-supervised learning (SSL) is currently manual and often suboptimal, relying heavily on domain expertise. Automating this process could tailor tasks to data characteristics, improve sample efficiency, and provide insights into what makes a pretext task effective.

Main Idea: We introduce a bilevel meta-learning framework that parameterizes a family of auxiliary tasks (e.g., augmentation policies, context-prediction objectives) via a controller network φ.  
1. Inner Loop: Train a base encoder f_θ on unlabeled data using SSL losses generated by φ.  
2. Outer Loop: Evaluate f_θ’s representations using a small labeled or proxy validation set (e.g., linear probe accuracy), and update φ to maximize this metric via gradient-based hyperparameter optimization or policy gradients.  
The controller gradually discovers high-impact pretext tasks tailored to the dataset.  
Theoretically, we derive sample complexity bounds showing that adaptive task selection reduces the number of unlabeled examples needed for a target representation quality, compared to fixed tasks.  
Empirically, MetaSSL yields state-of-the-art performance across vision, text, and time-series benchmarks with minimal manual tuning, advancing both the theory and practice of SSL.