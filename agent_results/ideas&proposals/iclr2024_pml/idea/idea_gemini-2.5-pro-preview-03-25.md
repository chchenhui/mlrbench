**Title:** Differentially Private Parameter-Efficient Fine-Tuning for Large Language Models

**Motivation:** Fine-tuning large language models (LLMs) on sensitive downstream tasks (e.g., medical records, private emails) poses significant privacy risks, as models can memorize and leak training data. Applying traditional differential privacy (DP) methods to all parameters during fine-tuning is computationally expensive and often leads to substantial utility degradation.

**Main Idea:** We propose integrating differential privacy specifically with parameter-efficient fine-tuning (PEFT) methods like LoRA or Adapters. Instead of adding noise to gradients for the entire model, we will develop and analyze techniques to apply calibrated noise only to the gradients of the much smaller set of trainable PEFT parameters. This drastically reduces the dimensionality and computational overhead of DP. We hypothesize this targeted noise injection can achieve strong formal privacy guarantees (ε, δ-DP) with significantly less impact on model utility compared to DP applied during full fine-tuning. We will evaluate this through theoretical analysis and empirical studies on benchmark NLP tasks, measuring privacy (e.g., via membership inference attacks) and task performance trade-offs.