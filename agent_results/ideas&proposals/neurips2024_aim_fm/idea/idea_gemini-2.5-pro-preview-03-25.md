**Title:** Concept Bottleneck Injection for Explainable Medical Foundation Models

**Motivation:** Medical Foundation Models (MFMs) often function as black boxes, hindering clinical trust and adoption. Explainability is crucial for verifying diagnostic reasoning, identifying biases, and facilitating clinician understanding. Current post-hoc explanation methods can be unreliable; we need intrinsically interpretable models.

**Main Idea:** We propose injecting a "Concept Bottleneck" layer into MFMs during fine-tuning for specific medical tasks (e.g., chest X-ray classification). This layer forces the model to predict high-level, clinically relevant concepts (e.g., "cardiomegaly," "pleural effusion," "nodule presence") *before* making the final diagnosis. These intermediate concepts are supervised using annotations derived from medical reports or expert labels. The final prediction is then based solely on these concepts, providing a transparent decision pathway. This approach offers intrinsic explainability by revealing the concepts driving the diagnosis, allows clinicians to verify or correct the model's intermediate reasoning, and potentially improves robustness by focusing the model on clinically meaningful features.