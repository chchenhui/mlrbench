# TactileNet: Hierarchical Touch Representation Learning for Robotic Manipulation

## Motivation
While touch is essential for robotic manipulation, current approaches struggle to effectively process high-resolution tactile data. Unlike vision, tactile sensing captures only local information through active exploration, requiring temporal integration for complete understanding. Most existing models simply adapt vision architectures to tactile data, ignoring touch's unique spatiotemporal properties. This research addresses the fundamental question of "How do we make sense of touch?" by proposing a neural architecture specifically designed for tactile data's intrinsic characteristics, enabling robots to develop rich touch representations for complex manipulation tasks.

## Main Idea
TactileNet is a hierarchical neural architecture that models touch data's unique properties through three integrated components: (1) A local feature extractor using specialized convolutional kernels that capture pressure distributions and surface properties at varying scales; (2) A temporal integration module that processes sequential tactile impressions during active exploration, using attention mechanisms to focus on salient tactile features; and (3) A hierarchical representation builder that organizes tactile information from low-level textures to high-level object properties. The model incorporates inductive biases reflecting how humans interpret touch, including relative intensity perception and temporal continuity constraints. We'll validate TactileNet on manipulation tasks requiring fine tactile feedback (e.g., slip detection, texture classification, and grasp stability prediction), demonstrating superior performance compared to vision-derived architectures.