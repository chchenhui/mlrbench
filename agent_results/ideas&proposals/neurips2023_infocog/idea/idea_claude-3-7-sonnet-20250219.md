# Information-Theoretic Bounds for Interpreting Language Model Representations

## Motivation
Large language models (LLMs) exhibit remarkable cognitive capabilities but remain largely black boxes. Understanding how information is encoded and processed within these systems is crucial for advancing AI alignment and building models that better interface with human cognition. Current approaches to model interpretability lack rigorous quantification of what information is preserved or discarded at different processing stages, limiting our ability to create models that mirror human information processing.

## Main Idea
I propose developing a novel framework that applies information bottleneck theory to analyze representations in language models. By measuring the mutual information between model activations and both input text and downstream cognitive tasks (e.g., reasoning, factual recall, social inference), we can characterize how information flows through these systems compared to human cognitive processing. The research will introduce estimators optimized for high-dimensional neural representations and develop targeted probing tasks aligned with cognitive science theories. This approach will enable us to identify where models compress information differently from humans, revealing the information-theoretic signatures of different cognitive functions. The framework will produce quantifiable bounds on what information is retained at each processing stage, providing principled guidance for developing more interpretable and human-aligned language models that better preserve cognitively relevant information.