Title: HyperPrompt – Dynamic Soft Prompt Generation via Hypernetworks for Robust In-Context Learning

Motivation:  
Standard in-context learning uses fixed or handcrafted prompts that often underperform under distributional shifts or when examples vary widely in style. Adapting prompts on-the-fly to align with novel few-shot contexts can substantially boost reliability and generalization of large language models (LLMs).

Main Idea:  
We introduce HyperPrompt, a two-stage architecture in which a lightweight hypernetwork H ingests k labeled examples and produces a task-specific soft prompt vector. During training, H and the base LLM are jointly optimized across diverse tasks so that H learns to encode example patterns into optimal prompt adjustments. At inference, given unseen few-shot examples, H generates a bespoke prompt that steers the LLM toward accurate predictions. We anticipate (1) improved accuracy under domain shifts, (2) reduced sensitivity to example ordering, and (3) faster task adaptation without full fine-tuning. This approach also facilitates interpretability—by examining H’s weights—and theoretical analysis of prompt transformations. HyperPrompt paves the way for more reliable, scalable in-context learning across applications.