Title: LLM-Informed Bayesian Sequential Experimental Design

Motivation:  
In scientific and engineering applications, traditional Bayesian experimental design often relies on weak or generic priors, slowing convergence when experiments are costly or high-dimensional. Meanwhile, large language models (LLMs) encode rich domain knowledge from scientific literature that remains untapped in active experimental planning. Integrating LLM-derived priors into Bayesian frameworks can accelerate discovery, improve uncertainty quantification, and reduce experimental costs.

Main Idea:  
We propose a two-stage methodology. First, prompt a domain-specialized LLM to generate probabilistic statements (e.g., approximate functional forms, parameter ranges, correlations) about the system under study. These statements are parsed into informative prior distributions over functions or parameters in a Gaussian Process (GP) or Bayesian neural network. Second, we perform sequential experimental design using acquisition functions (e.g., mutual information, expected improvement) computed under the enriched posterior. As new data arrive, priors adapt via continual LLM re-prompting to refine beliefs. We will evaluate this on chemical reaction optimization and materials synthesis benchmarks. Expected outcomes include faster convergence to optimal conditions, calibrated uncertainty estimates, and demonstrable cost savingsâ€”paving the way for knowledge-grounded, uncertainty-aware AI in critical scientific workflows.