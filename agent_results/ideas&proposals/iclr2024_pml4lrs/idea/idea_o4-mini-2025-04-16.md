Title: Edge-Driven Generative Labeling and Distillation for Low-Resource ML

Motivation:  
In many developing-region settings, obtaining diverse, high-quality labeled data is cost-prohibitive and large pre-trained models poorly generalize to local domains. A lightweight, domain-adaptive pipeline that synthesizes and distills data on-device can bridge this gap while respecting strict compute and annotation budgets.

Main Idea:  
We propose a two-stage teacher–student framework. First, a cloud-hosted “teacher” generative model (e.g., a small GAN or diffusion network) is fine-tuned on a handful of seed labels to produce synthetic, domain-specific samples. Next, a compact “student” classifier, pruned and quantized for edge deployment, self-distills from teacher logits and uses uncertainty-driven active sampling to solicit human annotations only on the most informative synthetic instances. Periodic feedback refines the teacher with newly labeled examples, closing the loop. We will validate on applications like crop-disease detection and rural healthcare imaging, targeting a 5× reduction in labeled data requirements, sub-200 ms inference latency, and robust local performance. This approach democratizes ML by combining generative augmentation with on-device efficiency for resource-constrained environments.