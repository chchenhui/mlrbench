Title: Causal Unlearning for Secure and Fair Pre-trained Language Models

Motivation: Large-scale language models often memorize sensitive data and encode biases, risking privacy leaks and unfair outputs. Existing unlearning methods lack principled ways to target specific information pathways, leading to suboptimal removal or degraded performance.

Main Idea: We propose a Causal Unlearning framework that (1) constructs a lightweight structural causal model (SCM) over internal activations to isolate pathways conveying sensitive tokens or protected attributes; (2) quantifies their path-specific effects via influence-function estimators; and (3) applies targeted gradient-based interventions (“do-operation approximations”) to zero out these effects in model weights. To provide formal guarantees, we inject calibrated noise to satisfy differential privacy during unlearning. We evaluate on standard benchmarks for membership inference, toxicity, and bias amplification, demonstrating that our method removes targeted information with provable privacy/fairness assurances while preserving overall language performance. This approach enables more trustworthy deployment of foundation models in sensitive applications.