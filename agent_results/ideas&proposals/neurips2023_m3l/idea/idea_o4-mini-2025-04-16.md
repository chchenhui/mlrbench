Title: Spectral Stochastic Flow for Adaptive Control on the Edge of Stability

Motivation:  
Deep networks routinely operate in the “Edge of Stability” (EoS) regime—large learning rates that destabilize classical convergence guarantees yet empirically accelerate training. A principled theory for controlling this regime is lacking, leading to costly hyperparameter sweeps, especially at billion-parameter scale.

Main Idea:  
We propose modeling discrete-time SGD with momentum as a Spectral Stochastic Flow (SSF), an SDE whose drift and diffusion are projected onto the top-k Hessian eigenspaces. SSF yields closed-form criteria for when step sizes trigger EoS transitions in sharp and flat directions. Building on this, we design an adaptive spectral step-sizer that scales learning rates per eigenmode to maintain each eigendirection at a user-specified stability threshold. Methodology includes:  
1. Estimating leading Hessian eigenpairs on the fly using subspace-iteration.  
2. Calibrating the SSF diffusion term to match observed gradient noise covariance.  
3. Implementing spectral step-sizer and validating on ResNet and Transformer benchmarks.  

Expected outcomes are faster convergence, controlled sharpness growth, and reduced tuning overhead. This bridges discrete SGD practice and continuous theory, offering a scalable recipe for stable large-model training.