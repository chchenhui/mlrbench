**Title:** FineActionBench: A Benchmark for Fine-Grained Temporal Video-Language Alignment

**Motivation:** Evaluating video-language models often relies on coarse-grained tasks like retrieval or captioning. However, many real-world applications demand precise temporal understanding â€“ aligning specific textual phrases to exact moments or action segments within a video. The lack of benchmarks focusing on this fine-grained temporal grounding hinders the development and comparison of models capable of such detailed alignment.

**Main Idea:** We propose FineActionBench, a novel benchmark designed to rigorously evaluate the fine-grained temporal alignment capabilities of video-language models. The methodology involves curating videos depicting complex, multi-step activities (e.g., cooking, assembly) and generating dense annotations where short textual phrases describe specific, temporally localized sub-actions or events. We will develop new evaluation metrics, such as phrase-localized Temporal Intersection over Union (T-IoU), to quantify how accurately models can ground textual descriptions within precise video segments. FineActionBench will provide a much-needed tool for measuring progress in temporal video-language understanding.