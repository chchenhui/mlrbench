**Title:** Context-Aware AutoRL: In-Context Sample-Efficient Hyperparameter Tuning via LLMs  

**Motivation:** Manual hyperparameter tuning is a major bottleneck in applying RL to novel domains, requiring extensive expertise and trials. Existing AutoML methods often rely on pretraining or static search spaces, leading to inefficiency. LLMs offer in-context learning, enabling adaptation without retraining. Leveraging this could democratize RL by automating hyperparameter adjustments dynamically, addressing brittleness and improving accessibility.  

**Main Idea:** We propose a LLM-based controller that dynamically generates RL hyperparameters (e.g., learning rates, batch sizes) using an in-context history of training interactions. By constructing input prompts from real-time RL logs (e.g., loss curves, reward signals) and past hyperparameter configurations, the LLM learns contextual patterns mapping training dynamics to optimal settings. Training leverages offline RL datasets to simulate diverse contexts, teaching the LLM to recommend adjustments tailored to the current learning phase. This reduces sample inefficiency by prioritizing past effective configurations and enables zero-shot transfer via prompt adaptation. The system operates as a plug-and-play module, iteratively refining hyperparameters during training without modifying the RL algorithm. Expected outcomes include a 30-50% reduction in tuning trials across domains like robotics and game AI, with improved performance consistency. This bridges LLMs and AutoML, enabling expertise-free RL pipelines.