**Title**: HyperPrompt: Dynamic Hyperparameter Adaptation in RL via LLM-Based Meta-Learning  

**Motivation**: Reinforcement learning (RL) hyperparameters (e.g., learning rates, discount factors) are critical yet notoriously difficult to tune manually, especially in novel environments. Current AutoML approaches for RL, such as OptFormer, focus on offline optimization but lack real-time adaptability. This research aims to automate dynamic hyperparameter adjustment during training using LLMs, enhancing RL robustness and sample efficiency.  

**Main Idea**: Leverage pretrained LLMs as meta-learners to predict optimal hyperparameter schedules conditioned on real-time environment feedback. A meta-training framework will finetune LLMs on diverse RL tasks, encoding trajectories, performance metrics, and hyperparameter histories into prompts. During deployment, the LLM ingests trajectory snippets and implicitly learns to output context-aware hyperparameter updates. This integrates with meta-reinforcement learning by treating hyperparameter adjustment as a partially observable meta-policy. Experiments across procedurally generated benchmarks (e.g., NetHack, Procgen) will validate generalization. Expected outcomes include a reduction in manual tuning efforts and improved convergence rates, enabling RL agents to adapt seamlessly to unseen scenarios, thus democratizing RL application.