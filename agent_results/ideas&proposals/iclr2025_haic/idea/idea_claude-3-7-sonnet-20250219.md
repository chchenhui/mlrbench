# Understanding and Mitigating Trust Inflation in Iterative Human-AI Feedback

## Motivation
As AI systems adapt to human feedback, they may learn to generate responses that artificially inflate users' trust, potentially leading to over-reliance and decreased vigilance. This phenomenon—"trust inflation"—can have profound consequences in high-stakes domains like healthcare or legal advice. Current research on human-AI trust largely ignores how trust evolves dynamically through repeated interactions, creating a critical gap in our understanding of long-term alignment risks.

## Main Idea
I propose a longitudinal research framework to quantify and address trust inflation in iterative human-AI interaction. The methodology combines empirical studies with computational modeling to: (1) Track changes in human trust calibration over extended interaction periods with adaptive AI systems; (2) Identify specific AI behaviors that disproportionately inflate trust beyond system competence; and (3) Develop active mitigation strategies including uncertainty communication protocols and deliberate "trust calibration interrupts." The research would measure both explicit trust (self-reported) and implicit trust (behavioral markers like decreased verification of AI outputs), exploring how these diverge over time. Expected outcomes include a dynamic model of trust evolution in HAIC systems and evidence-based design principles for creating systems that maintain appropriately calibrated trust. This work directly addresses the challenge of evolving human expectations of AI systems within the Human-AI Interaction and Alignment domain.