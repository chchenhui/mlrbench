1. **Title: Enhancing Goal-Conditioned Reinforcement Learning through Self-Supervised Goal Representation Learning**  

2. **Motivation**  
GCRL faces challenges in sparse reward environments and sample inefficiency, particularly in complex domains like molecular design or robotics. Existing methods often ignore the rich relational structure between goals and states. By integrating self-supervised learning into GCRL, we can distill symbolic task-specific knowledge into continuous representations, improving generalization and reducing reliance on hand-engineered reward functions.  

3. **Main Idea**  
We propose a two-stage framework: First, a self-supervised module learns a metric-shared goal-state representation using contrastive learning on diverse experience sequences. Goals and intermediate states are encoded via hierarchical attention, contrasting positive pairs (co-occurring in successful trajectories) against negatives. Second, a GCRL agent (e.g., SAC) uses this representation to compute goal-conditioned Q-values and dynamically relabel goals during replay. The key innovation lies in a **context-aware contrastive loss** that aligns representations across temporally distant goals, enabling the agent to infer abstract subgoals and transfer policies across tasks (e.g., from molecule synthesis to robotics). Experiments on sparse-reward continual-control (Meta-World) and discrete-action (3D molecular generation) domains will assess sample efficiency and compositional generalization. This approach bridges GCRL with representation learning, offering interpretable latent spaces for causal goal reasoning and accelerating real-world deployment where reward engineering is impractical.