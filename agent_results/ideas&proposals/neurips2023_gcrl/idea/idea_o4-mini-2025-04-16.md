Title: Counterfactual Goal-Augmented World Models for Causal GCRL

Motivation:  
Goal-conditioned RL agents excel at reaching specified observations but struggle when environments exhibit causal dependencies or require intervention planning. Bridging GCRL with causal reasoning would enable agents to plan interventions (e.g., precision medicine treatments) rather than merely react, improving safety and interpretability in high-stakes domains.

Main Idea:  
We propose equipping GCRL with a latent structural causal world model (SCWM) that generates both observational and counterfactual trajectories. First, learn an SCWM via self-supervised objectives: (1) fit observational transitions, (2) predict outcomes under hypothetical interventions on latent variables. Next, condition policy and value networks on both a desired goal observation and an intended intervention set. During training, the agent samples counterfactual “what-if” rollouts from the SCWM to evaluate which interventions drive the system toward the goal. Policy gradients are computed over these counterfactual trajectories, yielding a goal-conditioned intervention planner. We expect this yields (a) improved sample efficiency through model reuse, (b) explicit causal reasoning about interventions, and (c) applicability to precision medicine and molecular design where treatment actions correspond to interventions.