**Title:** Optimization-Centric Scaling Laws: Unraveling the Impact of Algorithms on Model Efficiency  

**Motivation:** As model sizes grow exponentially, understanding how optimization algorithms influence scaling laws is critical. Current scaling laws focus on model size, data, and compute but neglect the role of optimization methods. This gap leads to inefficient hyperparameter tuning, wasted resources, and suboptimal large-scale training. By formalizing the interplay between optimizers and scaling, we can predict optimal training configurations for billion-parameter models using smaller proxies, drastically reducing costs and environmental impact.  

**Main Idea:** This research proposes a framework to quantify how optimization algorithms (e.g., Adam, SGD) affect neural scaling laws. First, conduct systematic experiments across model sizes, optimizers, and hyperparameters (learning rates, momentum) to derive algorithm-dependent scaling exponents. Next, develop a theoretical model linking optimizer dynamics (e.g., gradient noise, adaptive step sizes) to scaling behavior. Finally, use these insights to create a *scaling-aware optimizer selection protocol* that recommends optimization strategies for target model sizes and compute budgets. Expected outcomes include open-sourced scaling predictors and practical guidelines for aligning optimizer choices with scaling objectives, potentially reducing hyperparameter search costs by 50%+ in large-scale training.