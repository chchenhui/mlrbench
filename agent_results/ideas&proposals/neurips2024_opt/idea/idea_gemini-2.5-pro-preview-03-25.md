**Title:** Budget-Aware Optimization Scaling: Predicting Optimal Model Configuration via Trajectory Scaling Laws

**Motivation:** Training large models incurs significant computational cost. Choosing optimal model hyperparameters (width, depth, batch size) and optimizer settings for a *fixed compute budget* remains a major challenge, often relying on expensive trial-and-error. This research aims to develop a methodology to predict the best configuration beforehand, saving compute resources and time.

**Main Idea:** We propose a framework to predict the optimal model configuration (architecture parameters, batch size) and initial learning rate for a specific optimizer (e.g., AdamW) under a fixed compute budget. We will model and derive scaling laws not just for final loss, but for the *loss trajectory* as a function of compute, parameterized by model size, batch size, and key optimizer settings. By conducting systematic, cheaper experiments on smaller models and extrapolating these trajectory scaling laws, our framework will predict the configuration expected to achieve the lowest loss within the specified budget. Expected outcomes include a practical tool reducing hyperparameter tuning costs and energy consumption for large-scale model training.