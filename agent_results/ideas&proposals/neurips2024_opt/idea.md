**Title:** Optimization-Aware Scaling Laws for Efficient Hyperparameter Transfer in Large Model Training  

**Motivation:** Current scaling laws focus on model and data size but ignore the role of optimization algorithms, leading to suboptimal hyperparameter choices when scaling models. This gap results in costly trial-and-error tuning for large models, wasting computational resources and time. Integrating optimization dynamics into scaling laws is critical to enable efficient hyperparameter extrapolation across model sizes.  

**Main Idea:** This research proposes deriving *optimization-aware scaling laws* that explicitly model interactions between optimizer hyperparameters (e.g., learning rate, batch size), model size, and optimizer choice (e.g., Adam, SGD). Through systematic experiments on varied model sizes and optimizers, we will quantify how optimal hyperparameters scale with model dimensions and optimizer properties. For example, we aim to identify whether learning rate decay rates or momentum terms follow predictable scaling patterns. The derived laws will be formalized into a lightweight framework that recommends hyperparameters for target model sizes, validated on LLM fine-tuning tasks. Outcomes include reduced hyperparameter search costs and energy-efficient training, directly addressing the compute-optimal scaling challenge.