**Title:** Adaptive Expert Pruning and Merging for Modular Continual Learning

**Motivation:** Modular architectures, like Mixture-of-Experts (MoE), offer a promising avenue for continual learning by adding new experts for new tasks. However, this often leads to unbounded model growth over time, increasing computational and memory costs. Existing methods lack robust mechanisms to consolidate knowledge and manage model complexity efficiently during long learning sequences.

**Main Idea:** We propose a continual learning framework using MoE that dynamically manages its expert pool. When encountering new tasks, new experts are initialized. Concurrently, we introduce periodic processes for: 1) **Expert Similarity Assessment:** Quantifying functional overlap between experts using shared data representations or correlation of predictions on buffered examples. 2) **Knowledge Consolidation:** Automatically merging highly similar experts (e.g., via parameter averaging or task-vector merging) and pruning underutilized or redundant experts based on usage frequency and performance contribution. This approach aims to mitigate catastrophic forgetting while actively controlling model size and inference cost, enabling more scalable and efficient lifelong learning systems.