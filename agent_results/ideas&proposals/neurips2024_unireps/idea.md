**Title:** Task-Conditioned Functional Alignment for Cross-Architecture Model Merging

**Motivation:** Merging pre-trained models can save significant computational resources. However, current methods often struggle when models have different architectures or are trained on slightly varied task distributions. Understanding *when* representations functionally align despite these differences is key to effective merging.

**Main Idea:** We propose developing a "Task-Conditioned Functional Alignment" (TCFA) technique. Instead of directly aligning parameter spaces, TCFA focuses on aligning activation spaces based on functional similarity *conditioned on specific downstream task properties*. We will first probe different layers of source models using task-specific input variations (e.g., different classes, styles, or transformations). We then use Optimal Transport or subspace alignment methods (like CCA variants) to find minimal transformations that align activation manifolds corresponding to the *same task condition* across models. These learned transformations act as lightweight "stitching" layers to merge models, potentially requiring far fewer trainable parameters than full fine-tuning or naive parameter averaging, thereby enabling efficient merging across diverse architectures tackling related problems.