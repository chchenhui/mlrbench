**Title:** Interpretability-Aware Scaling for Scientific Discovery

**Motivation:** Scaling AI models often improves predictive performance in science but typically decreases interpretability, hindering the extraction of actionable scientific insights and trust in the model. Addressing this trade-off is crucial for leveraging large models not just for prediction, but for genuine understanding and hypothesis generation.

**Main Idea:** We propose developing techniques for "Interpretability-Aware Scaling". This involves creating model architectures and training protocols where interpretability is a quantifiable objective alongside performance during the scaling process. Methodologically, we could explore incorporating physics-informed constraints, causal discovery modules, or concept bottleneck layers representing scientific variables directly into large models. We will develop metrics to jointly evaluate discovery potential (e.g., accuracy, novelty) and interpretability (e.g., faithfulness of explanations, complexity of extracted rules). The expected outcome is a framework demonstrating how to navigate the Pareto frontier between model scale/performance and scientific interpretability, allowing researchers to choose optimal models based on their specific discovery goals.