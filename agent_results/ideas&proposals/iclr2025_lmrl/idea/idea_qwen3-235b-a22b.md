**Title:** *Hierarchical Contrastive Learning with Uncertainty-aware Multimodal Alignment for Cross-modal Generalization in Biological Data*  

**Motivation:** Biological data are inherently multimodal and sparse, with missing or noisy observations across genomics, proteomics, and imaging. Existing multimodal representation learning methods often assumes perfect alignment across modalities and prioritize shared latent spaces over modality-specific nuances, leading to brittle generalization. This hampers downstream applications like drug discovery or disease subtyping, where cross-modal reasoning (e.g., predicting gene expression from imaging) is critical. To address this, we need frameworks that (1) learn robust representations without requiring fully paired data and (2) explicitly model uncertainty from missing modalities during alignment.  

**Main Idea:** We propose a hierarchical contrastive learning framework that decouples shared and modality-specific representations while dynamically calibrating alignment based on data quality. First, modality-specific encoders learn noise-invariant features using self-supervised denoising objectives tailored to each data type. Second, a cross-modal contrastive module enforces alignment between positive pairs (e.g., cells across single-cell RNA-seq and ATAC-seq) using a distributional contrastive loss that models uncertainty via Monte Carlo sampling. Finally, a hierarchical fusion mechanism adaptively weights modality-specific and shared features during downstream tasks. For evaluation, we introduce a novel benchmark with synthetic and real-world biological multimodal datasets (e.g., JUMP-CP, Human Cell Atlas) with missing modalities, measuring cross-modal retrieval accuracy and uncertainty calibration. Successful outcomes would enable models to generalize across domains (e.g., predicting proteomics from imaging) even with sparse or corrupted data, accelerating integrative biomedical analysis.