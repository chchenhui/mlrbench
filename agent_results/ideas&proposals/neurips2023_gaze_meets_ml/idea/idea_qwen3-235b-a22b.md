**Title:** Gaze-Informed Unsupervised Feature Selection via Neuro-Symbolic Attention  

**Motivation:**  
Manual data annotation is costly and time-intensive, hindering scalable machine learning. Eye-gaze provides a natural, low-cost supervision signal reflecting human visual prioritization, but existing approaches underutilize its potential for feature selection. Bridging this gap can reduce reliance on labeled data and enhance model interpretability through alignment with human cognition, addressing critical needs in domains like medical imaging and autonomous systems.  

**Main Idea:**  
We propose a neuro-symbolic framework where eye-gaze metrics (e.g., fixation duration, saccadic paths) guide unsupervised feature selection. First, symbolic reasoning modules encode domain-specific rules (e.g., longer fixations imply higher feature importance) to generate weak gaze-derived labels. These labels train a neural attention module to identify task-relevant features by correlating gaze patterns with input data. Next, a reinforcement learning agent dynamically adjusts feature selection during model training, balancing prediction accuracy and gaze alignment. Finally, the method is validated on tasks like image classification (using medical or scene datasets) and compared to state-of-the-art weakly supervised baselines. We expect improvements in model efficiency, accuracy (with 40â€“50% fewer labels), and transparency, enabling insights into how attention mirrors human cognition. This approach unifies neuroscience principles with ML, offering a scalable path toward interpretable, human-aligned AI.