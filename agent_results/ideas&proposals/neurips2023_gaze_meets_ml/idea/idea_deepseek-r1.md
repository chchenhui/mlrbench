**Title:** Self-Supervised Feature Prioritization in Medical Imaging via Eye Gaze Patterns  

**Motivation:** In medical imaging (e.g., radiology), unsupervised feature learning often struggles to prioritize clinically relevant regions without costly labeled data. Radiologists’ gaze patterns naturally highlight diagnostically critical areas, offering a novel, annotation-free signal to bridge this gap and align AI with expert reasoning.  

**Main Idea:** Develop a self-supervised framework that uses radiologists’ eye-tracking data to guide feature importance in convolutional or transformer-based networks. The model learns to contrast gaze-attended image regions against non-attended ones, enforcing similarity in embeddings for regions fixated during diagnoses. For example, gaze heatmaps from large-scale eye-tracking datasets (e.g., chest X-rays) would train the network to prioritize salient features through auxiliary contrastive losses. The approach eliminates manual annotations, leveraging inherent human attention as weak supervision. Expected outcomes include improved anomaly detection accuracy in unsupervised settings and interpretable attention maps mirroring expert focus. This could enable AI systems to generalize better in low-data regimes and enhance trust in medical diagnostics by aligning model behavior with clinician workflows.