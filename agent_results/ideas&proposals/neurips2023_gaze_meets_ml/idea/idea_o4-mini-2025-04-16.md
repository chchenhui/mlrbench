Title: Gaze-Driven Contrastive Representation Learning (GCRL)

Motivation:  
Human visual attention encodes rich semantic information that standard self-supervised methods overlook by treating all image regions equally. Leveraging inexpensive eye-tracking data can guide representation learning toward truly salient features, improving downstream performance and interpretability while reducing annotation costs.

Main Idea:  
We propose a contrastive learning framework that uses gaze fixations as weak supervision. From egocentric video paired with gaze logs, we extract fixation-centered crops as “anchors” and their temporally adjacent fixations as “positives,” while sampling spatially distant patches as “negatives.” A Siamese encoder is trained with a saliency-weighted contrastive loss (e.g., modified SimCLR/MoCo) that emphasizes gaze-guided pairs. During training, saliency maps derived from fixation heatmaps further reweight feature similarity to focus on human-attended regions. We evaluate the learned encoder on object classification, action recognition, and saliency prediction tasks, expecting higher accuracy, better sample efficiency, and alignment with human attention patterns. This approach bridges cognitive neuroscience and machine learning, yielding human-centric visual representations and democratizing cost-efficient supervisory signals for diverse vision applications.