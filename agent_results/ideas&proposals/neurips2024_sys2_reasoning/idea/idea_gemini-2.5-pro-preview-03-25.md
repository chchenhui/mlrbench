**Title:** Training Language Models to Self-Correct Reasoning via Explicit Verification Calls

**Motivation:** Large Language Models (LLMs) often produce plausible but incorrect reasoning chains (System-1 shortcuts). Explicitly verifying each reasoning step is computationally expensive. We need a mechanism that allows LLMs to internally identify potential reasoning failures and selectively invoke external verification/computation tools only when necessary, mimicking efficient System-2 self-correction.

**Main Idea:** We propose training an LLM with an auxiliary objective: predicting the correctness of its own intermediate reasoning steps. During training, the model processes reasoning problems alongside ground-truth intermediate steps and correctness labels (or uses a verifier model). The model learns a "confidence score" for each generated step. During inference, if this confidence score falls below a learned threshold, the model pauses its internal generation and outputs a request to an external tool (e.g., a symbolic calculator, a knowledge base query, or a separate verifier model) with the specific sub-problem. The tool's verified output is then fed back into the LLM to continue the reasoning process. This approach aims to create models that proactively self-correct complex reasoning by efficiently integrating internal intuition with external, reliable verification, improving overall accuracy and trustworthiness for System-2 tasks.