1. **Title**: SyncheGen: Compositional Grammar-Guided Benchmarking for System-2 Reasoning with Strict Data Separation  

2. **Motivation**:  
   Current benchmarks for System-2 reasoning often suffer from accidental data contamination or lack compositional structure, conflating memorization with systematic generalization. This hampers progress in building models that truly reason rather than exploit statistical patterns. A rigorous, contamination-free benchmark is critical to evaluate models' ability to compose rules and generalize across unseen task combinations, ensuring robustness in real-world applications (e.g., symbolic math, program synthesis, or complex planning).  

3. **Main Idea**:  
   SyntheGen synthesizes tasks via domain-specific formal grammars (e.g., arithmetic, logic, or syntax trees) to create procedurally generated datasets with strict train-test splits: train examples use unique atomic components (e.g., tokens, graph structures), while test examples combine these atoms in novel ways. For example, training on "simple arithmetic" and testing on nested expressions. Data contamination is prevented by design. The benchmark evaluates accuracy, attention-path consistency for symbolic rules, and transfer to domain-specific downstream tasks. Initially, it will compare transformer variants (e.g., standard, hybrid symbolic-attention) trained in diverse ways (e.g., chain-of-thought prompting, curriculum learning) to identify architectures and strategies most amenable to System-2 reasoning. Success would provide a blueprint for contamination-free evaluation and guide scalable model design.