**Title:** Dynamic Modality Reliability Estimation for Trustworthy Multi-modal Medical Fusion  

**Motivation:** Multi-modal medical data fusion (e.g., CT, MRI, EHRs) is critical for holistic diagnosis, but real-world deployment faces trust gaps. Current methods often assume equal reliability across modalities, ignoring noise, missing data, or domain shifts. This leads to overconfident, unreliable predictions, especially when some modalities are corrupted or biased. Addressing modality reliability is essential to build robust, trustworthy fusion models for clinical use.  

**Main Idea:** Propose a multi-modal fusion framework that dynamically estimates modality-specific reliability during inference. Leverage Bayesian neural networks to quantify uncertainty per modality and integrate these estimates via attention mechanisms to weight modality contributions. For training, introduce a self-supervised auxiliary task predicting modality corruption (e.g., synthetic noise, missing data) to teach the model to assess reliability. Validate on benchmarks with simulated and real-world modality degradation (e.g., low-quality imaging, incomplete EHRs). Expected outcomes: (1) Improved robustness to unreliable modalities, (2) Uncertainty-aware predictions flagging low-confidence cases, and (3) Interpretable attention maps highlighting trusted modalities. Impact: Enables safer deployment of multi-modal ML in clinics by reducing overconfidence and enhancing transparency, while providing a benchmark for reliability-aware fusion.