**Title:** Calibrated Uncertainty Quantification for Multi-modal Medical Diagnosis using Conformal Prediction

**Motivation:** For safe clinical integration, ML models must accurately report their confidence. Standard uncertainty methods often lack formal guarantees, especially when fusing diverse data types (images, EHR). This research aims to provide statistically valid uncertainty estimates for multi-modal diagnostic models, enhancing trustworthiness for clinicians.

**Main Idea:** We propose integrating deep evidential learning with conformal prediction for multi-modal healthcare tasks. Evidential learning will explicitly model distinct uncertainty sources (data vs. model uncertainty) from fused modalities (e.g., CT scans and EHR data). Conformal prediction will then be applied atop the evidential outputs to generate prediction sets (e.g., a set of possible diagnoses) with rigorous, finite-sample coverage guarantees (e.g., 95% confidence). This approach ensures the model's uncertainty quantification is well-calibrated regardless of the underlying model or data distribution. Expected outcomes include improved reliability in identifying uncertain predictions needing expert review, thereby enhancing clinical decision support safety.