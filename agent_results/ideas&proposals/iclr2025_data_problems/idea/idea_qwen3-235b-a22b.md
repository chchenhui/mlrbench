**Title**: Uncertainty-Aware Synthetic Data Generation to Mitigate Model Collapse in Foundation Models  

**Motivation**: Synthetic data is critical for scaling foundation models (FMs), yet unrestrained use risks model collapseâ€”where iterative training on synthetic outputs degrades fidelity and diversity. Existing methods often prioritize data quality over diversity or vice versa, leading to unsafe or brittle models. This problem is exacerbated in multimodal and real-time applications, where compounding errors threaten robustness and safety. Addressing this requires principled approaches to balance synthetic data utility with the preservation of real data characteristics.  

**Main Idea**: Propose a framework that integrates uncertainty quantification into synthetic data generation (SDG). The method trains generators to produce data alongside uncertainty estimates (e.g., entropy, disagreement metrics). A differentiable reward function penalizes low-diversity regions in synthetic data while rewarding alignment with real data distributions. This reward is optimized via adversarial training, where discriminators specifically target modes of failure (e.g., redundancy, overfitting). Empirical evaluations will test the framework against model collapse benchmarks by measuring degradation in downstream task accuracy, distributional drift, and emergence of artifacts. Theoretical analysis will explore connections between generator uncertainty and training dynamics. Success will yield scalable SDG techniques that maintain data quality *and* diversity, enabling safer FM deployment in high-stakes domains like medicine or education, while reducing reliance on contentious real-world data sources.  

(Word count: 198)