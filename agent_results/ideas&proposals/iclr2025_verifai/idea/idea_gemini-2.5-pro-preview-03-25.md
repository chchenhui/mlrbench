**Title:** Runtime Formal Monitoring Guided Steering for Logically Consistent LLM Outputs

**Motivation:** Large Language Models often produce outputs that, while fluent, contain logical inconsistencies or violate specified constraints, especially in complex reasoning tasks. Ensuring the reliability and trustworthiness of LLMs requires mechanisms to guarantee adherence to logical rules or domain-specific constraints during generation.

**Main Idea:** This research proposes integrating runtime monitoring based on formal specifications (e.g., temporal logic, finite automata) *during* the LLM's generation process. Instead of post-hoc checking, the monitor observes the partially generated output sequence. If a potential violation of a pre-defined formal property is detected, the monitor provides feedback (e.g., identifying the violating token sequence, suggesting constraints for the next token) to steer the LLM's sampling strategy away from invalid paths. This involves developing efficient monitoring techniques compatible with token-by-token generation and exploring methods to translate monitor feedback into adjustments in the LLM's output probabilities. The expected outcome is LLMs capable of generating more logically consistent and constraint-adherent text, enhancing their reliability in verification-sensitive domains.