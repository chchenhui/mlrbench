**Title:** Cross-Modal Contrastive Learning for Table-Text Representations  

**Motivation:** Tables are often accompanied by textual metadata (e.g., descriptions, headers, or queries like SQL), but existing models treat these modalities in isolation, limiting their ability to perform tasks requiring joint understanding. Aligning table structures with natural language can improve semantic parsing, data discovery, and hybrid question answering.  

**Main Idea:** Develop a contrastive learning framework that jointly embeds tabular data and their textual context into a shared latent space. Tables are encoded using a transformer-based architecture with positional embeddings for rows/columns, while text is processed via a standard language model. The model is pre-trained on web-scale table-text pairs (e.g., from Wikipedia, Kaggle, or GitHub) using contrastive loss to maximize similarity between matched pairs. For downstream tasks, the aligned embeddings enable cross-modal retrieval (e.g., text-to-SQL, table QA) and multimodal fusion for generative applications. Experiments will evaluate the approach against single-modality baselines on tasks like schema linking, data cataloging, and text-to-SQL parsing, with expected improvements in accuracy and robustness. This method bridges the semantic gap between structured data and unstructured context, enhancing usability in enterprise and scientific domains.