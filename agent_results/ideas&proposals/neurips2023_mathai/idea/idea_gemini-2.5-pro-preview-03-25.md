**Title:** Adversarial Benchmark Generation for Robust Mathematical Reasoning Evaluation

**Motivation:** Existing mathematical reasoning benchmarks risk becoming saturated or compromised as LLMs are increasingly trained on web-scale data, potentially including benchmark problems/solutions. Evaluating true reasoning ability, rather than pattern matching or memorization, requires novel problems resistant to simple retrieval.

**Main Idea:** We propose a methodology to automatically generate adversarial mathematical reasoning problems. This involves using one LLM (the "generator") to create variants of existing problems or synthesize new ones based on specific mathematical concepts, while another LLM (the "solver/critic") attempts to solve them and identify potential flaws or ambiguities. The generator is iteratively refined to produce problems that are solvable by humans but challenging for current models, specifically targeting known weaknesses like sensitivity to phrasing or reliance on spurious correlations. This approach aims to create a dynamic, evolving benchmark that better measures the robustness and generalization capabilities of AI mathematical reasoning, pushing models beyond memorization towards deeper understanding.