# Self-attention and Temporal Awareness for Dynamic Graph Foundation Models

## Motivation
Current graph foundation models often struggle with dynamic graphs where edges and nodes change over time. This limitation severely restricts their application in domains like financial transaction networks, social media interactions, and infrastructure systems which are inherently temporal. While static graph neural networks have advanced significantly, and Transformer architectures have shown promise for graphs, they typically lack mechanisms to effectively model the temporal evolution of graph structures. Addressing this gap could unlock foundation models for dynamic graphs with capabilities comparable to what LLMs have achieved for language.

## Main Idea
We propose a novel architecture that combines self-attention mechanisms with temporal embedding strategies to create foundation models specifically designed for dynamic graphs. Our approach introduces "temporal graph tokens" that encode both structural and temporal information about how graph elements evolve. These tokens are processed through a dual-attention mechanism: one that captures spatial relationships across the graph and another that models temporal dependencies. The model incorporates a contrastive pre-training objective that predicts future graph states based on historical evolution patterns. This architecture allows the model to learn generalizable patterns of graph dynamics that can transfer across domains, potentially becoming a foundation model capable of understanding temporal graph evolution in scientific, financial, and social domains while supporting zero-shot learning for new temporal graph tasks.