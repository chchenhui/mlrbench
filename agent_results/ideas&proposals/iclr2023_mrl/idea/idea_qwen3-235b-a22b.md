**1. Title:** Quantifying Modality (Dis)Similarity for Adaptive Multimodal Fusion  
**2. Motivation:**  
A critical challenge in multimodal learning is understanding how modalities contribute to representation quality, especially when some are redundant, noisy, or semantically misaligned. Existing methods often fuse modalities indiscriminately, leading to suboptimal performance. This research addresses how to quantify modality (dis)similarity to guide adaptive fusion strategies, enabling models to dynamically prioritize informative modalities and suppress harmful ones, which is vital for robust and efficient multimodal reasoning in real-world applications.  

**3. Main Idea:**  
We propose a framework to quantify modality (dis)similarity through contrastive learning of modality-specific and joint representation spaces. First, modality-specific encoders learn to align with a shared semantic space while discouraging divergence. Second, we introduce a "discrepancy score" measuring how well modalities agree on pairwise sample relationships (e.g., via cosine similarity or optimal transport distances in the shared space). This score is regularized during training to enforce theoretical guarantees (e.g., triangle inequality). Finally, the discrepancy score informs a dynamic fusion mechanism that adjusts modality weights during inference, favoring aligned modalities and suppressing noisy ones. We evaluate on multimodal benchmarks with missing modalities and adversarial noise, showing improvements in robustness and efficiency compared to static fusion baselines. Theoretical analysis links discrepancy scores to downstream task performance, providing actionable insights for modality selection in applications like medical imaging or autonomous driving.