**Title:** Mechanistic Understanding of In-Context Learning in Autoregressive Transformers  

**Motivation:** In-context learning (ICL)—the ability of large language models (LLMs) to adapt to new tasks via prompted examples—is practically transformative but theoretically enigmatic. Its emergence lacks rigorous explanations, hindering systematic improvements. Bridging this gap is critical for designing efficient models and harnessing ICL’s full potential.  

**Main Idea:** This research aims to uncover the mechanistic pathways enabling ICL in autoregressive Transformers. Using *circuit analysis* and *probing*, we will identify subnetworks responsible for task inference, example retrieval, and parameter-free adaptation. A theoretical framework will link architectural components (e.g., attention heads, MLP layers) to computational roles, testing hypotheses like attention heads approximating gradient-based meta-learning or forming task-specific data associations. Experiments on synthetic tasks (e.g., linear regression, algorithmic operations) and ablation studies will validate theoretical predictions. Expected outcomes include: (1) a granular map of ICL’s neural correlates, (2) principles for optimizing architectures to amplify ICL, and (3) formal conditions (e.g., data complexity, attention mechanisms) for ICL emergence. The findings would guide efficient LLM designs and advance theory for in-context generalization.