Title: Self-Supervised Latent Reward Learning from Implicit Multimodal Feedback

Motivation: End-users seldom provide explicit rewards in real-world interactive systems, yet rich signals—eye gaze, facial expressions, gestures, prosody—encode valuable intent. Existing interactive RL methods ignore or hand-craft mappings for these implicit cues, limiting adaptability, personalization, and ease of deployment.

Main Idea: We propose an online framework that (1) collects multimodal streams during exploratory interactions, (2) uses self-supervised cross-modal contrastive learning to embed gaze, facial, speech, and gesture inputs into a shared latent space, and (3) fits a Bayesian reward model mapping latent clusters to scalar feedback. The agent’s policy is trained via RL guided by this evolving reward predictor. To resolve ambiguity, the system issues sparse active queries (“Was that helpful?”), refining cluster–reward associations with minimal interruption. As user preferences shift, the Bayesian model adapts, enabling non-stationary reward learning. Expected outcomes include accelerated personalization, robust adaptation to individual signaling styles, and reduced reliance on hand-crafted reward channels. This approach paves the way for truly naturalistic, scalable interactive agents in robotics, tutoring, and assistive interfaces.