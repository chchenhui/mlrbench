**Title:** Cross-Modal Self-Supervised Grounding for Interaction-Centric Reinforcement Learning  

**Motivation:** Traditional reinforcement learning (RL) relies on explicit, hand-crafted rewards, which are costly to design and limit natural human-machine interactions. Humans, however, communicate rich intent through implicit, multimodal signals (e.g., gestures, tone shifts) without explicit labels. Learning to interpret these ambiguous signals could unlock adaptive, intuitive systems that align with dynamic human preferences and real-world contexts.  

**Main Idea:** We propose a self-supervised framework that grounds implicit feedback signals—such as language, gaze, or gestures—by leveraging cross-modal consistency and sequential structure in human-AI interactions. First, we model the agent’s policy and feedback signals (e.g., audiovisual cues) as multivariate time-series, using mutual information maximization to align sequential patterns. Second, we train a reward function via contrastive learning: trajectories with similar feedback embeddings are contrasted against mismatched pairs, inferring implicit rewards without explicit labels. Finally, we integrate this reward into RL, enabling the agent to adapt policies to uncertain feedback sources and evolving user preferences. Testing in simulated environments (e.g., collaborative robotics), we evaluate the agent’s ability to learn from proxy implicit signals (e.g., hand waves) without predefined mappings. This approach reduces reliance on explicit rewards and provides a scalable, generalizable solution for interaction-grounded learning in non-stationary settings, directly addressing gaps in current HCI and RL paradigms.