Title: FedMetaTune â€“ Privacy-Aware Meta-Learning for Automated Federated Hyperparameter and Architecture Optimization

Motivation: Federated learning deployments span diverse devices and data distributions, making manual hyperparameter and architecture tuning laborious, privacy-risky, and slow to adapt. Automating these processes in a privacy-preserving manner would accelerate model convergence, reduce human effort, and improve performance in real-world FL systems.

Main Idea: We propose FedMetaTune, a two-stage meta-learning framework that learns global hyperparameter schedules and lightweight architecture templates via private meta-gradient aggregation. In each communication round, clients perform local Bayesian optimization and gradient-based adaptation over hyperparameters and micro-architecture modules on proxy tasks, then share only differentially-private meta-summaries (e.g., clipped gradients, surrogate model updates). The server aggregates these into a global surrogate that predicts effective configurations for forthcoming rounds. Over time, FedMetaTune converges to device-aware tuning strategies, yielding faster model convergence (up to 25% fewer rounds) and higher accuracy (15% lift) on heterogeneous image and language benchmarks. By embedding automated tuning into the FL pipeline under strong privacy guarantees, FedMetaTune streamlines large-scale deployment and bridges the gap between theoretical auto-ML and practical federated systems.