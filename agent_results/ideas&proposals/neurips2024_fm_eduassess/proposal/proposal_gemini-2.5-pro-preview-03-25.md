**1. Title:**

SecureED: A Contrastive Learning Framework for Detecting AI-Generated Responses in Multimodal Educational Assessments

**2. Introduction**

*   **Background:** The landscape of education is rapidly evolving with the integration of advanced generative artificial intelligence (AI), particularly large foundation models (LFMs) like GPT-4, Llama, and Gemini. These models offer unprecedented potential to transform educational assessment by assisting in test construct identification, automatic item generation (AIG), multimodal item design, automated scoring, and adaptive testing (Workshop Call for Papers). However, the widespread accessibility and sophistication of LFMs also introduce significant challenges to assessment integrity. A primary concern is the potential for students to misuse these tools to generate responses to assessment tasks, thereby undermining the validity of evaluations designed to measure genuine student understanding and capabilities (Idea Motivation). Current methods for detecting AI-generated text, such as those evaluated by Elkhatat et al. (2023) and Weber-Wulff et al. (2023), often lack the necessary robustness, reliability, and generalizability across diverse subjects and question types, especially those requiring high-order reasoning or creativity. Existing tools like GPTZero, while innovative, face challenges with accuracy and evasion tactics like paraphrasing (Tribune.com.pk, 2023; Beam, 2023; Axios, 2024). This inadequacy risks false accusations against students and erodes trust in both the assessment process and the potential benefits of AI in education (Key Challenge #1). Furthermore, the lack of transparency and explainability in many detection tools hinders their acceptance by educational stakeholders (Key Challenge #4). The development of reliable, fair, and interpretable methods for identifying AI-generated assessment responses is therefore critical for maintaining academic integrity and enabling the responsible adoption of generative AI in education.

*   **Research Objectives:** This research proposes the development and evaluation of *SecureED*, a novel framework designed to accurately distinguish between human-written and AI-generated responses in educational assessments, even for complex, multimodal tasks. The primary objectives are:
    1.  To design and implement SecureED, a contrastive learning framework capable of learning discriminative representations that differentiate human versus AI generation patterns across various modalities (text, code, mathematical reasoning).
    2.  To curate a comprehensive, multimodal dataset of paired human and AI-generated responses to representative educational assessment prompts, covering diverse subjects and requiring different cognitive skills (e.g., analysis, synthesis, evaluation, creative problem-solving).
    3.  To enhance the robustness of SecureED against common evasion tactics (e.g., paraphrasing, style mimicry, model mixing) through adversarial fine-tuning and the incorporation of domain-specific linguistic and semantic features.
    4.  To rigorously evaluate SecureED's performance, generalizability across different subjects and LFM generators, and robustness against adversarial attacks, comparing it against state-of-the-art baseline detection methods.
    5.  To investigate the explainability of SecureED's predictions, providing insights into the features that distinguish human from AI responses in educational contexts.
    6.  To develop guidelines for the practical integration of SecureED into educational assessment platforms, ensuring usability and promoting fairness.

*   **Significance:** This research directly addresses the critical challenge of "Generative AI for assessment security and accountability" highlighted in the workshop call. By developing a robust and reliable detection mechanism, SecureED aims to safeguard the integrity of educational assessments in the age of powerful LFMs. This work moves beyond general text detection by focusing specifically on the nuances of assessment responses across multiple domains (text, code, math) and cognitive levels. Its emphasis on contrastive learning leverages recent advancements shown to be effective in differentiating subtle stylistic and semantic differences (Bhattacharjee et al., 2023; Guo et al., 2024; La Cava et al., 2024). Furthermore, by addressing robustness against evasion (a known vulnerability, Kirchenbauer et al., 2023) and incorporating explainability, SecureED contributes to the development of "Trustworthy AI for educational assessment." Successfully achieving the research objectives will provide educators and institutions with a valuable tool to uphold academic standards, foster a fair assessment environment, and build confidence in integrating AI technologies responsibly into the educational ecosystem. The resulting framework and insights will be significant contributions to both the AI and educational assessment research communities.

**3. Methodology**

*   **Research Design:** The core of this research is the design, training, and evaluation of the SecureED framework based on contrastive learning. The overall approach involves: (i) systematic data collection and generation to build a representative dataset; (ii) development of a contrastive learning model architecture tailored for multimodal assessment responses; (iii) incorporation of domain-specific features and adversarial training for robustness; and (iv) comprehensive experimental evaluation against baselines using rigorous metrics.

*   **Data Collection and Generation:**
    1.  **Human Responses:** We will collect anonymized human responses to a curated set of assessment prompts. These prompts will span multiple subjects (e.g., literature, history, physics, computer science, mathematics) and require diverse response types (e.g., essays, short answers, code implementations, mathematical proofs/solutions). Prompts will be designed to elicit higher-order thinking. Data will be sourced ethically, potentially through collaborations with educational institutions or simulated settings with paid participants (e.g., university students), ensuring informed consent and anonymization protocols adhering to strict privacy standards (e.g., IRB approval).
    2.  **AI Responses:** For each prompt and corresponding human response(s), we will generate multiple AI responses using a variety of publicly available and proprietary LFMs (e.g., GPT-3.5, GPT-4, Llama 2/3, Gemini, Claude). To simulate realistic misuse scenarios, we will employ diverse prompting strategies, including zero-shot, few-shot (potentially using snippets of human responses as examples, being careful about contamination), and role-playing prompts (e.g., "respond as a high school student").
    3.  **Dataset Structure:** The final dataset will consist of triplets: `{Prompt, Human Response, AI Response}`. We will also generate LFM responses designed to evade detection (adversarial samples) using techniques like automated paraphrasing (e.g., using tools like QuillBot or dedicated paraphrasing models), instruction-based style alteration, and mixing outputs from different models.
    4.  **Multimodality Handling:** Text responses will be processed directly. Code responses (e.g., Python, Java) will be treated as structured text. Mathematical responses might include LaTeX notation or step-by-step reasoning, which will be parsed and represented appropriately (potentially as structured text sequences or leveraging specialized mathematical language models/embeddings if necessary).

*   **SecureED Framework: Algorithmic Steps:**
    1.  **Model Architecture:** SecureED will employ a Siamese network architecture, or a similar structure suitable for contrastive learning. It will consist of:
        *   **Encoder:** A shared, pre-trained transformer-based model (e.g., RoBERTa for text, CodeBERT for code, potentially adapted or combined for multimodal inputs) will serve as the backbone encoder. This encoder, $E(\cdot)$, will map an input response $x$ (human or AI) to a high-dimensional embedding vector $z = E(x)$.
        *   **Projection Head:** A small multi-layer perceptron (MLP), $P(\cdot)$, will project the encoder output $z$ into a lower-dimensional space where the contrastive loss is applied, $h = P(z)$. This is common practice in contrastive learning to improve representation quality.
        *   **Contrastive Learning Module:** This module computes the loss based on pairs or triplets of embeddings.

    2.  **Contrastive Learning Objective:** We will primarily utilize the InfoNCE (Noise Contrastive Estimation) loss, a widely used objective in self-supervised and contrastive learning. Given a batch of $N$ input pairs (e.g., $N$ human responses $x_i^h$ and their corresponding AI counterparts $x_i^a$ for the same prompt), we treat each response type as a potential anchor. For an anchor embedding $h_i$ (e.g., from a human response $x_i^h$), we want to pull its "positive" counterpart (e.g., an augmented version of $x_i^h$, or potentially the embedding of another human response $x_j^h$ to the same prompt) closer, while pushing away "negative" counterparts (e.g., embeddings $h_k^a$ from AI responses). A common formulation considers pairs $(x_i^h, x_i^a)$. For the embedding $h_i^h = P(E(x_i^h))$, the positive example could be an augmented version $h_i^{h+}$ and negatives would be all $h_j^a$ in the batch. A simpler formulation for this *supervised* contrastive task (where labels human/AI are known) might treat all human responses to a prompt as positive to each other, and all AI responses as negative, and vice versa. Let's formulate it to push human and AI responses apart. For a pair $(x_i^h, x_i^a)$, we obtain embeddings $(h_i^h, h_i^a)$. A simplified contrastive objective could aim to maximize the distance between $h_i^h$ and $h_i^a$. Alternatively, using InfoNCE within a batch $\{ (x_i^h, x_i^a) \}_{i=1}^N$:
        Let $h_i = h_i^h$. The positive sample is $h_i^+ = h_i^a$. The negative samples are $\{ h_j^h \}_{j \neq i} \cup \{ h_j^a \}_{j \neq i}$. The loss for $h_i^h$ is:
        $$
        \mathcal{L}_i = -\log \frac{\exp(\text{sim}(h_i^h, h_i^a) / \tau)}{\sum_{k=1, k \neq i}^N \exp(\text{sim}(h_i^h, h_k^h) / \tau) + \sum_{k=1}^N \exp(\text{sim}(h_i^h, h_k^a) / \tau)}
        $$
        where $\text{sim}(u, v) = u^T v / (||u|| ||v||)$ is cosine similarity and $\tau$ is a temperature hyperparameter. A symmetric loss would be calculated anchored on $h_i^a$. The total loss is the average over all anchors. *Correction:* The formulation above seems incorrect for supervised contrastive learning aim. A better approach adapted from supervised contrastive learning (Khosla et al., 2020): For an anchor $h_i$ (from response $x_i$), the set of positives $P(i)$ includes all embeddings in the batch from responses of the *same class* (human or AI), and negatives $A(i)$ are all embeddings from the *different class*.
        $$
        \mathcal{L}_{\text{SecureED}} = \sum_{i \in I} \frac{-1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp(\text{sim}(h_i, h_p) / \tau)}{\sum_{k \in A(i) \cup P(i), k \neq i} \exp(\text{sim}(h_i, h_k) / \tau)}
        $$
        This encourages embeddings from the same source class (human/AI) to cluster together, while pushing embeddings from different classes apart.

    3.  **Domain-Specific Features & Robustness:**
        *   **Feature Engineering:** We will explore incorporating features known to sometimes distinguish human vs. AI text, such as perplexity, burstiness (variability in sentence length/complexity), measures of logical coherence, or specific stylistic markers relevant to code (e.g., comment frequency, variable naming conventions) or mathematical reasoning (e.g., step justification patterns). These could be computed by auxiliary modules and concatenated to the encoder output $z$ before the projection head, or used in auxiliary prediction tasks during training.
        *   **Adversarial Fine-tuning:** After initial contrastive training, the model will be fine-tuned on a dataset augmented with adversarial examples (paraphrased AI responses, style-transferred responses). This involves iteratively generating challenging AI responses that the current model misclassifies and adding them to the training set, making the model more resilient to evasion tactics (Key Challenge #3).

    4.  **Classification:** Once the contrastive learning phase yields robust embeddings, a simple classifier (e.g., a linear layer or a small MLP) can be trained on top of the frozen or fine-tuned embeddings $z = E(x)$ to predict the probability that a given response $x$ is AI-generated: $P(\text{AI} | x) = \sigma(W^T z + b)$. Alternatively, classification can be done by comparing the embedding of a test response to prototype embeddings of human and AI classes learned during training.

*   **Experimental Design:**
    1.  **Datasets:** We will split our curated multimodal dataset into training (70%), validation (15%), and testing (15%) sets. Splits will be stratified by subject, response type, and generating LFM to ensure representative evaluation.
    2.  **Baselines:** We will compare SecureED against:
        *   Existing commercial/public AI detectors (e.g., GPTZero, OpenAI's Text Classifier, Copyleaks API - subject to availability and usage terms).
        *   Academic methods: Re-implementations or adaptations of relevant approaches like ConDA (Bhattacharjee et al., 2023) and DeTeCtive (Guo et al., 2024) if applicable to the assessment response domain. Also, traditional ML models (XGBoost, Random Forest) trained on stylometric features (Najjar et al., 2025).
        *   Ablation studies: Evaluating SecureED variants without contrastive learning, without domain-specific features, or without adversarial fine-tuning to quantify the contribution of each component.
    3.  **Evaluation Metrics:**
        *   **Core Performance:** Accuracy, Precision, Recall, F1-score (especially for the AI-generated class), AUC-ROC.
        *   **Robustness:** Performance degradation (e.g., drop in F1-score) when evaluated on the adversarial test set (paraphrased, style-mimicked responses).
        *   **Generalizability:** Cross-domain performance (train on subset of subjects, test on unseen subjects), Cross-LFM performance (train on responses from $k$ LFMs, test on responses from an unseen $(k+1)^{th}$ LFM).
        *   **Fairness:** Preliminary checks for bias, e.g., comparing performance across different demographic groups if such data is ethically available and relevant, or across different types of prompts (e.g., prompts related to sensitive topics vs. neutral topics).
    4.  **Explainability:** We will apply techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) to the final classifier to understand which input features (e.g., specific words, phrases, structural elements) contribute most to SecureED's decision for individual predictions. This addresses stakeholder concerns about transparency (Key Challenge #4).

**4. Expected Outcomes & Impact**

*   **Expected Outcomes:**
    1.  **SecureED Framework:** A well-documented, open-source implementation of the SecureED contrastive learning framework for detecting AI-generated educational assessment responses (subject to data privacy constraints).
    2.  **Multimodal Assessment Dataset:** A curated (potentially benchmark) dataset of paired human/AI responses across various domains (text, code, math), including adversarial examples. We will aim to make a subset of this dataset publicly available for research purposes, respecting privacy and ethical considerations.
    3.  **Performance Evaluation:** Comprehensive results demonstrating SecureED's effectiveness, robustness, and generalizability compared to existing state-of-the-art methods and baselines.
    4.  **Explainability Insights:** Analysis and visualization of features identified by explainability methods as key differentiators between human and AI responses in educational contexts.
    5.  **Integration Guidelines:** Practical recommendations and guidelines for integrating SecureED or similar detection tools into Learning Management Systems (LMS) and assessment platforms, addressing usability and ethical considerations (Key Challenge #5).
    6.  **Publications:** Peer-reviewed publications detailing the methodology, findings, and implications of this research, starting with a submission to the Workshop on Large Foundation Models for Educational Assessment.

*   **Impact:**
    1.  **Enhanced Assessment Integrity:** SecureED aims to provide a significantly more reliable tool for educators and institutions to detect AI-generated submissions, thereby helping to preserve the validity and fairness of educational assessments.
    2.  **Increased Trust:** By offering a robust, explainable, and rigorously evaluated solution, this research seeks to build trust among stakeholders (students, educators, administrators) regarding the management of AI misuse in assessments.
    3.  **Responsible AI Adoption:** Providing effective detection tools can mitigate risks associated with LFMs, thereby facilitating their responsible adoption for legitimate educational purposes (e.g., personalized feedback, learning aids) without excessively compromising assessment security.
    4.  **Contribution to Research:** This work will advance the state-of-the-art in AI-generated content detection, particularly within the challenging and high-stakes domain of education. The focus on multimodality, higher-order thinking tasks, robustness, and contrastive learning offers novel contributions to the fields of AI, Natural Language Processing, and Educational Technology.
    5.  **Practical Tools for Educators:** The development of an open-source framework and integration guidelines provides tangible resources for the educational community to address a pressing, real-world challenge.

By addressing the critical need for reliable AI detection in educational assessments, the SecureED project promises to make a substantial contribution to maintaining academic integrity while navigating the transformative potential of large foundation models in education. This research directly aligns with the workshop's focus on generative AI for assessment security, accountability, and trustworthy AI.