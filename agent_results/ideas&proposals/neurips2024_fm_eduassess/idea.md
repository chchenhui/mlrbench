**Title**: *SecureED: Generative AI for Detecting and Preventing AI-Generated Responses in Educational Assessments*  

**Motivation**: As large foundation models (LFMs) become ubiquitous, students may misuse them to generate answers, undermining assessment integrity. Current detection tools lack robustness across diverse subjects and question types, risking false accusations and eroded trust. This research addresses the critical need for reliable, adaptable methods to ensure AI accountability in high-stakes educational evaluations.  

**Main Idea**: Develop *SecureED*, a contrastive learning framework leveraging LFMs to distinguish AI-generated from human responses. The model will be trained on a multimodal dataset (text, code, math) with human/AI-generated pairs, emphasizing high-order thinking tasks. By fine-tuning with adversarial samples and domain-specific features (e.g., reasoning coherence, creativity patterns), *SecureED* will achieve cross-domain generalizability. Evaluation will include robustness tests against evasion tactics (e.g., paraphrasing) and comparisons to existing detectors (e.g., GPTZero). Expected outcomes include an open-source detection API and guidelines for integrating *SecureED* into assessment platforms, ensuring scalable, equitable AI accountability. Impact: Preserves assessment validity while enabling safe adoption of generative AI in education.