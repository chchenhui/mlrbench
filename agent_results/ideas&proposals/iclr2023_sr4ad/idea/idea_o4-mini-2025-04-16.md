Title: PhyGraph – Physics-Integrated Graph Scene Representation for Joint Perception and Prediction  

Motivation:  
Autonomous driving pipelines often decouple perception and prediction, leading to error accumulation, poor interpretability, and physically implausible forecasts. A unified intermediate representation that embeds both interaction semantics and kinematic constraints can boost robustness, generalization to novel scenarios, and downstream planner confidence.  

Main Idea:  
We propose PhyGraph, a dynamic, heterogeneous graph representation that models scene entities (vehicles, pedestrians, lanes, signals) as nodes and their spatio-temporal interactions (proximity, right-of-way, lane adjacency) as typed edges. Each node’s embedding fuses raw sensor features with explicit physical states (position, velocity, turn radius). A physics-informed GNN iteratively propagates messages while enforcing kinematic consistency (e.g., maximum acceleration, collision avoidance) via differentiable constraint layers. We jointly train PhyGraph on multi-task objectives—object detection, tracking, and trajectory prediction—augmented with physics regularizers that penalize implausible motions. Expected outcomes include:  
1. Improved joint perception-prediction accuracy on urban benchmarks.  
2. Physically coherent trajectory forecasts under novel topologies.  
3. Enhanced interpretability via attention maps over interaction edges.  

This representation naturally interfaces with planning modules and bolsters safety by preventing physically impossible predictions.