**Title:** Interpretable Interaction Representations for Joint Perception and Prediction

**Motivation:** Accurate prediction of traffic participants' future behavior is critical for safe autonomous driving, but requires understanding complex interactions. Current joint perception and prediction models often act as black boxes, making it difficult to verify their reasoning or debug failures, especially regarding agent interactions.

**Main Idea:** We propose learning an explicit, interpretable representation focused on interactions within the driving scene. This intermediate representation, potentially based on graph neural networks or attention mechanisms, will explicitly model pairwise or groupwise influences between agents (vehicles, pedestrians, cyclists). Attention weights or graph edge features will be designed to be interpretable, signifying interaction strength or type (e.g., yielding, competing). This interaction module will bridge perception outputs (detected objects, states) and prediction inputs. The expected outcome is a joint model that not only improves prediction accuracy by explicitly reasoning about interactions but also provides interpretable insights into *why* a certain prediction was made, enhancing trustworthiness and facilitating debugging.