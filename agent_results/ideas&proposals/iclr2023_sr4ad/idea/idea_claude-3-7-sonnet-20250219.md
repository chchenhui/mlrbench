# Neural Scene Graphs for Understanding Complex Traffic Scenarios

## Motivation
Autonomous driving in complex urban environments requires deep understanding of dynamic spatial-temporal relationships between traffic participants. Current perception and prediction systems often struggle with complex interactions among vehicles, pedestrians, and infrastructure, leading to safety concerns. A unified representation that captures both spatial configuration and relational dynamics could enhance decision-making capabilities and improve safety in challenging traffic scenarios.

## Main Idea
We propose Neural Scene Graphs (NSG), a structured representation that explicitly models relational dynamics in traffic scenes as attributed graphs with learnable spatial-temporal edges. NSGs represent traffic participants as nodes with their physical attributes, while edges capture interaction dynamics through learned relation embeddings. Our approach leverages a transformer-based architecture to update the scene graph dynamically, enabling joint reasoning about both spatial configuration and social interactions. The framework includes: (1) a perception module that constructs initial scene graphs from sensor data, (2) a graph evolution module that predicts how relationships evolve over time, and (3) a planning module that reasons over possible future scene graph states. This representation facilitates interpretability through graph visualization, enables efficient transfer learning across different driving domains, and provides a unified interface for downstream tasks including prediction, planning, and simulation.