1. **Title**: Hierarchical Spatiotemporal Graphs for Unified Scene Representation in Autonomous Driving  
2. **Motivation**: Autonomous vehicles face challenges in integrating fragmented perception and prediction systems, leading to error propagation and inefficiency in dynamic environments. Existing methods often silo representations for static objects and moving entities, limiting the system's ability to generalize across complex interactions.  
3. **Main Idea**: Propose a hierarchical spatiotemporal graph as a unified scene representation, combining static physical elements (roads, infrastructure) and dynamic actors (vehicles, pedestrians) in a shared geometric space. The graphâ€™s topology will encode interactions via adaptive edge weights, enabling joint perception-prediction-learning via dynamic graph neural networks. Temporal layers will model trajectory evolution using temporal convolutional networks, while self-supervised contrastive learning will enhance generalization across unseen scenarios. By integrating 3D LiDAR, camera, and motion data into this structure, the system could jointly optimize object detection, scene flow estimation, and multi-horizon trajectory prediction. Expected outcomes include improved accuracy in complex urban scenes, reduced dependency on labeled datasets, and better robustness to edge cases. This representation could also aid safety-critical planning by explicitly modeling actor interactions, advancing end-to-end driving systems.