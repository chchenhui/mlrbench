**Title:** Framework for Auditing Algorithmic Compliance with the 'Right to Explanation'

**Motivation:** Regulations like GDPR grant individuals the 'right to explanation' for automated decisions significantly affecting them. However, translating this legal right into verifiable technical requirements for complex ML models (especially deep learning) is challenging. Existing explanation methods (e.g., LIME, SHAP) may not satisfy legal standards of clarity or sufficiency. This research aims to develop a practical framework for auditing whether an ML system adequately fulfills the right to explanation.

**Main Idea:** We propose a semi-automated auditing framework combining technical metrics and human evaluation. The framework will assess explanations based on regulatory criteria like fidelity (accuracy to the model's reasoning), intelligibility (understandability by the target user), and actionability (providing insights for recourse). Methodologically, it involves: 1) Defining quantitative metrics for fidelity and stability of explanations. 2) Developing protocols for user studies to evaluate intelligibility and actionability with diverse user groups. 3) Creating a standardized scoring system integrating technical and user feedback. Expected outcomes include a practical toolkit and methodology for regulators and organizations to audit explanation capabilities, ensuring ML systems meet regulatory requirements beyond mere technical existence of an explanation method.