**Title:** Adaptive Mathematical Reasoning Assessment via Procedural Problem Generation

**Motivation:** Current static benchmarks for mathematical reasoning (e.g., MATH, GSM8k) risk becoming outdated or compromised as LLMs are trained on vast web data possibly containing benchmark problems. Evaluating true reasoning versus pattern matching requires benchmarks that assess generalization and problem-solving processes, not just final answers on familiar distributions.

**Main Idea:** We propose developing a system for Procedural Content Generation (PCG) of mathematical problems tailored to evaluate specific reasoning skills (e.g., algebraic manipulation, geometric intuition, logical deduction). This system would generate novel problem instances based on templates and constraints, ensuring variance and controlling for difficulty. Crucially, the system would adapt problem generation based on an LLM's performance, creating harder or stylistically different problems in areas where the model succeeds and simpler variations where it fails. This allows for dynamic, fine-grained assessment of model capabilities and weaknesses, moving beyond static leaderboards towards a deeper understanding of LLM mathematical comprehension. Expected outcomes include robust, contamination-resistant evaluation and detailed diagnostic profiles of model reasoning abilities.