# Neural Geometry Preservation: A Framework for Understanding Human and Machine Cognition  

## Introduction  

In both biological and artificial neural systems, emerging evidence suggests that preserving the geometric and topological structure of input data is a central computational principle for efficient and robust information processing. In neuroscience, studies have revealed that neural circuits in sensory and motor regions encode spatial and directional information using structured representations, such as grid cells, head direction cells, and low-dimensional manifolds observed in motor cortex activity. Similarly, in deep learning, the field of geometric deep learning has leveraged symmetry, group structure, and manifold-based approaches to improve the performance of neural networks by ensuring that transformations of input data are consistently reflected in hidden representations. Despite these parallel advancements, a comprehensive theoretical framework that explains the underlying mechanisms of geometric preservation across different substrates is still lacking. Developing such a framework would not only unify key findings in neuroscience and machine learning but also provide insights into why certain geometric structures are evolutionarily favored in biological brains while being advantageous in artificial intelligence design.  

This research proposal introduces a novel framework called **Neural Geometry Preservation (NGP)**, aimed at formalizing the principles that govern how neural systems maintain the geometric structure of their inputs throughout processing layers. The framework will consist of three core components: (1) Distortion metrics to quantify how geometry is transformed across neural representations; (2) Theoretical analysis of optimal strategies for geometric preservation under various computational constraints; and (3) Experimental validation of these strategies in both biological and artificial neural systems. By analyzing examples such as grid cells and head direction cells in neuroscience alongside equivariant neural networks in deep learning, this work will uncover general principles that transcend specific implementations, shedding light on why geometric structures emerge naturally in both domains and how they can be harnessed to enhance AI efficiency and biological understanding.  

The significance of this research lies in its potential to bridge neuroscience and machine learning by formalizing a common mathematical foundation for neural representations. Identifying principles of geometric preservation can lead to more interpretable and efficient deep learning architectures, as well as provide neuroscientists with computational tools to analyze neural circuits in structured spaces. Additionally, this framework could inform the design of artificial agents with better generalization and sample efficiency, mirroring the cognitive abilities of biological organisms to adapt to geometric transformations in their environment. By addressing these objectives, NGP will contribute to both fields, aligning with the thematic focus of the NeurReps workshop on geometric and topological representations in neural systems.

## Methodology  

### 1. Distortion Metrics for Geometric Information  

To quantify how neural systems preserve or deform the geometric structure of their inputs across processing layers, we propose developing a set of mathematically rigorous distortion metrics. These metrics will be based on differential geometry and topological data analysis (TDA), allowing us to measure both local and global distortions in the representational space. The core idea is to track how distances, angles, and curvatures in the input space evolve as data passes through successive neural transformations.  

In mathematical terms, let $( \mathcal{M}, g )$ represent the input manifold associated with a data structure equipped with a Riemannian metric $ g $. The neural processing pipeline can be modeled as a sequence of mappings $ f_i: \mathcal{M} \to \mathcal{N}_i $, transforming the input space $ \mathcal{M} $ into a series of latent spaces $ \mathcal{N}_1, \dots, \mathcal{N}_t $. The distortion incurred by each such mapping $ f_i $ can be assessed by comparing the pullback metric $ f_i^* g_{\mathcal{N}_i} $ to the original metric $ g_{\mathcal{M}} $. More specifically, we define a distortion measure $ \mathcal{D}(g_{\mathcal{M}}, f_i^* g_{\mathcal{N}_i}) $ that quantifies the discrepancy between the two metrics, potentially using concepts such as the Gromov–Hausdorff distance (which compares metric spaces) or the Fisher information metric (which captures information-theoretic distortions). As noted in previous studies on geometric deep learning (Bronstein et al., 2023), equivariant architectures preserve metric structures under certain transformations, such as rotations and translations. By extending this formalism to general manifolds, we can define a distortion function that captures both invariant and equivariant transformations across layers.  

To compute these metrics in practice, we will use methods from TDA to estimate the topological persistence of features through each neural transformation. Given a set of input data points $ \{ x_j \} \subset \mathcal{M} $, we can form a Vietoris–Rips complex from these points and track changes in persistent homology after mapping through each $ f_i $. The Wasserstein distance between persistence diagrams at different layers would provide a topological distortion measure, aligning with prior work on TDA-integrated neural networks (Pedersen et al., 2023). Additionally, recent advances in geometric deep learning on simplices, such as neural k-forms (Maggs et al., 2023), provide a framework to capture intrinsic geometric transformations in neural layers, allowing for precise quantification of curvature and differential structure preservation.  

### 2. Provable Guarantees of Structure Preservation  

Beyond empirical distortion metrics, it is essential to identify theoretical guarantees that define optimal geometric preservation strategies. Drawing from the mathematical foundations of equivariance in deep learning (Monti et al., 2023), we can derive conditions under which neural representations remain faithful to the input space. One approach is to examine how neural transformations interact with Lie groups that represent data symmetries. Let $ G $ be a Lie group defining a set of transformations (such as $ SO(3) $ for 3D orientation-based data or $ E(2) $ for planar transformations), and let $ \phi: G \to \text{Aut}(\mathcal{N}) $ be a group representation in the latent space. A transformation $ g \in G $ acting on the input will induce an action on the latent space, and a neural network is **equivariant** if $ \phi(g) \circ f = f \circ g $. This condition ensures that geometric transformations in the input space correspond to structurally consistent transformations in the latent space.  

To derive optimal strategies, we will examine the information bottleneck principle (Tishby & Zaslavsky, 2015) applied to geometric transformations. Let $ x \in \mathcal{M} $ be an input, and $ z \in \mathcal{N} $ be a latent representation. The goal is to minimize the trade-off between the mutual information $ I(x; z) $ and the distortion $ \mathcal{D}(g_{\mathcal{M}}, g_{\mathcal{N}}) $. The optimization problem can be formalized as minimizing the Lagrangian:  

$$
\mathcal{L} = I(x; z) + \beta \cdot \mathcal{D}(g_{\mathcal{M}}, g_{\mathcal{N}})
$$

where $ \beta $ controls the relative influence of distortion minimization. By leveraging results from geometric flows in deep learning (Lei & Baehr, 2025), we can use Ricci flow techniques to analyze how curvature evolves in the latent space, and determine optimal training dynamics that control geometric distortion rates. Additionally, we can use tools from information geometry (Amari, 2016) to characterize the geometric properties of neural parameter spaces and relate them to representation preservation.  

### 3. Experimental Design and Validation  

To validate our theoretical framework, we will conduct a series of experiments across both biological and artificial neural systems. For biological data, we will analyze neural representations from brain recordings in tasks that involve spatial navigation and rotational perception. In particular, we will examine grid cells and head direction cells in the hippocampal formation, where neurons fire in geometrically structured patterns that form low-dimensional manifolds. Using persistent homology and k-form analysis, we will quantify distortions in these manifolds under different cognitive loads or environmental changes.  

For artificial systems, we will test our framework on a selection of neural architectures, including convolutional neural networks (CNNs), graph neural networks (GNNs), and emerging equivariant neural architectures such as geometric neural operators (Quackenbush & Atzberger, 2025) and equivariant neural fields (Wessels et al., 2024). Each network will be trained on structured datasets, such as those involving 3D object transformations, language geometry, or dynamic physical systems. After training, we will extract the latent space of each model and compute geometric distortion measures using the proposed metrics.  

Our experimental validation will also involve ablation studies to assess how different network components (e.g., equivariant layers, attention mechanisms, or geometric loss functions) impact distortion metrics. We will evaluate this by systematically modifying standard CNN and GNN architectures to include various forms of inductive bias for geometric preservation and analyze the resulting distortion rates. Comparative benchmarks across multiple tasks will include performance metrics such as classification accuracy, reconstruction accuracy, and generalization to unseen geometric transformations.  

### 4. Implementation and Computational Framework  

To implement our proposed framework, we will utilize geometric deep learning libraries such as GeometricFlows (Lei & Baehr, 2025) and TopologicalNetworks (Pedersen et al., 2023) to compute distortion metrics and track geometric transformations. The core computations will be performed using PyTorch and geometric deep learning extensions such as PyTorch Geometric. To estimate the latent metric spaces and compute distortion measures, we will employ tools from computational geometry, including parallel transport, covariant derivatives, and curvature estimation in latent representations.  

Our implementation will also integrate concepts from geometric neural operators (Quackenbush & Atzberger, 2025) to enhance the efficiency of geometry-aware deep learning models. Additionally, we will develop custom loss functions that incorporate distortion metrics into a gradient-based learning framework, allowing models to optimize for both task performance and geometric consistency. These methods will be extended to include physics-informed constraints, drawing inspiration from Physics-informed PointNet (Kashefi & Mukerji, 2022), which has successfully applied geometric inductive biases in fluid dynamics and heat transfer simulations.  

By leveraging this computational toolkit, we will establish a comprehensive framework capable of quantifying geometric preservation across different domains, enabling rigorous comparison between biological and artificial systems while ensuring compatibility with existing neural network architectures.  

## Expected Outcomes and Impact  

The proposed research will yield several key contributions across theoretical neuroscience, deep learning, and applied geometry. Firstly, we will develop a formal framework for geometric structure preservation in neural representations, providing distortion metrics that apply to both biological and artificial neural networks. These metrics can be used to analyze how neural circuits and deep learning models transform geometric information across layers, facilitating comparisons between natural and synthetic systems in domains such as language, vision, and motor control.  

Secondly, through theoretical analysis, we aim to derive mathematical guarantees that define optimal strategies for structure preservation under different computational constraints. Using information bottleneck theory, Lie-group-equivariant optimization, and geometric flow-based learning, we will characterize how neural networks can maintain geometric consistency while minimizing distortion and maximizing expressiveness. These theoretical insights will be instrumental in designing neural architectures that preserve data geometry, leading to improved generalization and efficiency in learning tasks where geometric transformations are fundamental.  

Thirdly, our experimental validation will demonstrate the applicability of our framework across diverse domains. We expect to uncover novel principles of geometric preservation in biological neural circuits (e.g., grid cells and head direction cells) and show how deep learning models can emulate these principles through appropriate architectural choices. Additionally, we will develop benchmark datasets and models for evaluating geometric preservation, fostering reproducibility and further exploration in the broader research community.  

Ultimately, this research will advance our understanding of why neural systems, whether biological or artificial, naturally align with geometric priors, and how these structures emerge during training and learning. By formalizing a theoretical foundation for geometry-preserving networks, we will open new avenues for improving learning efficiency and interpretability in deep learning while offering novel computational tools for characterizing neural representations in the brain.

## Broader Implications and Future Applications  

The proposed Neural Geometry Preservation (NGP) framework has wide-ranging applications that extend beyond theoretical insights. In neuroscience, understanding the geometric and topological structure of neural activity can lead to improved models of brain function, particularly in sensory and motor systems. Recent studies on neural manifolds (Lu & Zhang, 2024) have demonstrated that the activity in motor cortex and visual pathways lies on low-dimensional manifolds, which suggests that geometric constraints play a role in neural computation. By formalizing these constraints through NGP, we can provide a principled approach for analyzing how biological neural circuits maintain stable representations despite noisy and changing environments. This understanding could lead to the development of neural data analysis techniques that exploit geometric structures to infer cognitive states or predict behavioral responses more accurately.  

In the context of deep learning, leveraging geometric priors through NGP could significantly enhance the performance of various tasks, particularly in vision, language, and physics-based modeling. For instance, in vision, preserving the geometric structure of images across neural layers can improve object recognition under viewpoint changes, as shown by equivariant neural networks (Ben-Hamu et al., 2023). In language, geometric representations have been employed to capture the hierarchical structure of linguistic data, and an NGP-based approach could refine these representations to maintain syntactic and semantic invariances under rephrasing or translation. Additionally, in robotics and physical systems, where data often resides on curved spaces such as SO(3) for orientation or SE(3) for rigid transformations, ensuring geometric consistency in deep learning models can improve trajectory prediction, control, and generalization from limited training data.  

These applications align with the topics highlighted in the NeurReps workshop, including geometric deep learning, world models, and representational geometry in neural data. Our framework will provide a unified perspective to study how these principles emerge naturally in both biological and artificial cognition, contributing to the broader discussion on geometric and topological approaches to understanding intelligence and learning.

## Research Contributions and Dissemination  

The proposed framework will advance multiple research areas, fostering new interdisciplinary connections between theoretical neuroscience, geometric deep learning, and topological data analysis. First, we will develop a unified mathematical theory of geometric distortion in neural processing, which can be used to assess how well networks retain the intrinsic structure of transformations in their latent spaces. A key contribution will be the derivation of theoretical guarantees that define optimal preservation strategies under constraints such as finite training data, computational complexity, and noise robustness. This includes extending results in neural field equivariance (Wessels et al., 2024) to derive bounds on approximation accuracy when geometric priors constrain representational learning.  

Second, we will implement novel distortion metrics that are both differentiable and interpretable, making them applicable as regularization techniques in deep learning. These metrics will be integrated into the training pipeline as loss functions that penalize deviations from expected geometric structures, following in the footsteps of Physics-informed PointNet (Kashefi & Mukerji, 2022) and geometric neural operators (Quackenbush & Atzberger, 2025). Through this implementation, we aim to create modular geometric deep learning tools that can be incorporated into existing neural frameworks, including transformer architectures, manifold learning models, and physics-based learning systems.  

Finally, our experimental validation on both biological and artificial neural systems will provide benchmark results that can be shared with the research community to foster further exploration. These datasets, models, and distortion metrics will be made publicly available to ensure reproducibility and encourage future investigations into geometric representation learning. By contributing these resources, we will establish a foundation for cross-domain studies of neural computation, supporting the broader NeurReps workshop goal of integrating geometric and topological approaches across neuroscience and machine learning.