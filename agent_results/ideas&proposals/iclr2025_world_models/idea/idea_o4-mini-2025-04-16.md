Title: CausalDiffWorld: Diffusion-Driven Causal Multimodal World Models for Embodied AI

Motivation: Realistic, causally coherent simulation of environment dynamics across vision, language, and control is critical for robust embodied agent planning but remains challenging due to high-dimensional complexity and missing causal structure.

Main Idea: We propose CausalDiffWorld, which integrates latent diffusion models with explicitly learned causal graphs in a state-space transformer backbone. First, a structural causal model is inferred via interventional data to capture object interactions and environment laws. A latent diffusion generator then synthesizes high-fidelity multimodal predictions—video frames, proprioceptive signals, and textual descriptions—conditioned on current state and action sequences. These predictions are embedded into hierarchical temporal abstractions by the transformer, enabling long-horizon reasoning. Agents perform trajectory optimization in the learned latent space using diffusion-based sampling to achieve robust, adaptive planning under uncertainty. We will evaluate on embodied manipulation and sim-to-real benchmarks, expecting gains in sample efficiency, generalization, and causal fidelity. This approach aims to deliver scalable, interpretable world models that bridge the gap between high-fidelity simulation and real-world applicability.