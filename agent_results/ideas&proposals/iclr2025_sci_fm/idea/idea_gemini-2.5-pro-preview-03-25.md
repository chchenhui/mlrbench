**Title:** Federated Distillation for Efficient Open Foundation Model Training

**Motivation:** Training large foundation models (FMs) requires immense computational resources, limiting participation to well-funded labs and hindering open science. Existing efficiency techniques like distillation often require access to the original large dataset, which may not always be feasible or distributed.

**Main Idea:** We propose a federated distillation framework enabling collaborative training of smaller, efficient open FMs without centralizing sensitive or large datasets. Participating institutions train local specialist models on their own data partitions. Concurrently, a central, smaller student FM learns distilled knowledge not by directly accessing local data, but by aggregating knowledge distilled from the participating specialist models' outputs or gradient updates on a shared, public dataset proxy. This approach enhances data privacy, reduces communication overhead compared to full federated learning, and democratizes the creation of capable, open FMs by leveraging distributed compute and data resources efficiently. The expected outcome is a methodology for creating performant, resource-friendly open FMs.