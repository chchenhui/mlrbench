Title: Adaptive Alpha-Stable Stochastic Gradient Descent for Enhanced Generalization  

Motivation: Traditional SGD injects Gaussian noise, which may be insufficient for escaping narrow, sharp minima and exploring complex loss landscapes. Empirical studies show that heavy-tailed gradient noise naturally emerges during training and correlates with better generalization. By explicitly harnessing heavy-tailed behavior, we can improve exploration, robustness, and convergence speed in large-scale models.  

Main Idea: We propose replacing Gaussian gradient perturbations with symmetric α-stable noise in SGD, resulting in an optimizer we call HT-SGD. The stability parameter α∈(1,2] is adapted online based on local gradient variance and curvature estimates: lower α increases jump sizes in flat or highly nonconvex regions to escape sharp minima, while α→2 recovers Gaussian-like behavior near well-conditioned basins. We derive expected escape times using fractional Fokker–Planck equations, providing theoretical guarantees on exploration–exploitation trade-offs. Experiments on vision and language benchmarks will compare HT-SGD to standard optimizers, measuring generalization gap, training stability, and convergence speed. This method reframes heavy tails from a training artifact to a deliberate, tunable mechanism for robust optimization.