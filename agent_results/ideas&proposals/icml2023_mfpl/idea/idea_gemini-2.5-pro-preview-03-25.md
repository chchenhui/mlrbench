**Title:** Fair Preference Learning through Adversarial Counterfactual Estimation

**Motivation:** Preference-based learning (PbL) relies heavily on human feedback, which can inadvertently encode societal biases. Models trained on such data (e.g., RLHF for LLMs) may perpetuate or amplify unfairness. Existing fairness interventions often occur post-hoc or require explicit group definitions, which might be limiting or unavailable. We need methods to imbue fairness directly into the preference learning process.

**Main Idea:** We propose an adversarial framework to learn preference models robust against specified biases. Alongside the primary preference predictor (estimating P(A > B)), we train an adversarial network to predict sensitive attributes (e.g., gender, race proxies) or undesired correlations from the learned preference representations or reward signals. The main preference model is then trained to simultaneously maximize preference prediction accuracy *and* minimize the adversary's ability to predict these sensitive factors. This encourages the model to learn preferences based on task-relevant features while becoming invariant to the targeted biases present in the latent human judgements. Expected outcomes include fairer reward models and policies, quantifiable via downstream task evaluations on fairness metrics, leading to more equitable AI systems.