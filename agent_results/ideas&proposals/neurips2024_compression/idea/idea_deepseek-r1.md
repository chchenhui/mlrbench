**Title:** Neural Distributed Compression of Correlated Sources via Mutual Information Regularization  

**Motivation:** Distributed compression of correlated data (e.g., multi-sensor systems, federated learning) traditionally relies on Slepian-Wolf theorem-based quantization, which struggles with complex, high-dimensional correlations. Emerging neural compression methods excel at modeling intricate dependencies but lack theoretical grounding and adaptation to distributed settings. This research bridges this gap, aiming to improve efficiency and enable applications like decentralized IoT systems.  

**Main Idea:** We propose a **mutual information (MI)-regularized neural framework** for distributed compression of correlated continuous sources. Each source is encoded by a neural network with a variational autoencoder (VAE) structure, trained to maximize MI between latent codes of correlated sources while minimizing reconstruction error. This replaces explicit quantization with continuous, correlation-aware latent spaces. Theoretical analysis will connect achievable rate-distortion bounds to MI regularization strength, comparing them to Slepian-Wolf limits. Experiments on multi-view imagery and wireless sensor data will validate improved compression rates over baseline neural and classical methods. Successful outcomes could enable efficient distributed systems with theoretical guarantees, impacting federated learning, edge computing, and low-bandwidth communication networks.