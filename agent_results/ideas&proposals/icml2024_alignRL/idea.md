**Title:** Reverse-Engineering Empirical Successes: A Theoretical Analysis of Practical Reinforcement Learning Heuristics  

**Motivation:** Many RL algorithms achieve practical success through heuristics (e.g., reward shaping, exploration bonuses) that lack theoretical justification. This disconnect hinders generalization and trust in RL systems. Bridging this gap requires understanding why these heuristics work, enabling the design of theoretically grounded algorithms with similar or improved performance.  

**Main Idea:** This research will systematically analyze widely used empirical heuristics by formalizing their implicit assumptions and identifying the problem structures they exploit. For each heuristic, we will derive theoretical guarantees (e.g., sample efficiency, regret bounds) under realistic conditions and propose hybrid algorithms that replace heuristics with principled components. For example, if reward shaping is found to implicitly encode domain knowledge about reward sparsity, we might develop a formal reward structure learning algorithm. Experimental validation on real-world tasks will ensure practical relevance, while theoretical analysis will provide insights into generalizability. This work aims to create a bridge where empirical practices inform theory, leading to robust, adaptable RL algorithms.