# Bridging Theory and Practice in RL through Benchmark Translations

## Motivation
The disconnect between theoretical RL algorithms with provable guarantees and practical implementations that perform well in complex environments limits progress in both domains. Theorists develop algorithms for simplified settings that experimentalists find difficult to apply, while experimentalists create solutions that work but lack theoretical understanding. This gap prevents cross-fertilization of ideas and slows overall advancement in the field.

## Main Idea
We propose creating a systematic framework for "benchmark translations" that converts theoretical problem settings into practical benchmarks and vice versa. For each major theoretical setting (e.g., tabular MDPs, linear MDPs, low-rank MDPs), we would develop corresponding practical environments that preserve the theoretical structure while incorporating real-world complexities. Conversely, for popular practical benchmarks (e.g., Atari, robotics tasks), we would identify simplified versions that maintain core challenges but allow theoretical analysis. Each translation would be accompanied by a leaderboard tracking both theoretically-grounded metrics (sample complexity, regret bounds) and practical performance measures (average reward, stability). This framework would provide a common vocabulary for both communities, helping theorists identify which assumptions break in practice and helping experimentalists understand why certain algorithms succeed or fail, ultimately leading to algorithms with both practical performance and theoretical guarantees.