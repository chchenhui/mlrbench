Title: Curvature-Aware Stochastic Flow for Explaining the Edge of Stability

Motivation:  
Modern deep networks often train with learning rates so large that classical convergence theory fails. The widely observed “Edge of Stability” (EoS)—where loss oscillates yet still decreases over time—remains poorly understood. A principled framework capturing realistic loss landscapes and gradient noise can demystify EoS and guide hyperparameter choices, saving immense compute in the large-model era.

Main Idea:  
We model discrete-time gradient methods (with or without momentum) as a continuous stochastic differential equation whose drift term includes both the gradient and a Hessian-weighted correction, and whose diffusion term stems from gradient noise. By deriving the associated Fokker–Planck equation, we characterize the threshold learning rate at which curvature-induced drift and stochastic diffusion balance—quantifying EoS. We validate on controlled toy models and mid-sized Transformers, showing accurate EoS predictions and suggesting tailored learning-rate schedules. We further extend the framework to adaptive optimizers (e.g., Adam) by embedding preconditioning into the SDE. This theory‐driven approach promises to replace costly trial-and-error tuning with predictive, curvature-aware guidelines for large-scale training.