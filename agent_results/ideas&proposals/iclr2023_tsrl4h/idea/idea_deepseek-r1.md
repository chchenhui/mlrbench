**Title:** Interpretable Self-Supervised Learning for Sparse-Labeled Multimodal Time Series in Minority Health Populations  

**Motivation:** Labeling long-term, multimodal health time series (e.g., ICU data, rare disease monitoring) is resource-intensive, especially for minority populations where expert annotations are scarce. Existing methods often fail to balance label efficiency with interpretability, limiting clinical trust and utility.  

**Main Idea:** Develop a self-supervised framework that learns robust, interpretable representations from unlabeled multimodal time series (e.g., vital signs, wearable data) and adapts to downstream tasks with minimal labels. The model will use contrastive learning across modalities and time to capture latent patterns, coupled with attention mechanisms to highlight clinically salient regions (e.g., critical time steps or cross-modal interactions). For irregular/missing data, integrate masked reconstruction and uncertainty-aware imputation. To ensure actionability, the attention weights will map to human-understandable features (e.g., abnormal lab values) and validate on pediatric ICU or rare disease datasets with clinician feedback. Expected outcomes include (1) improved performance on sparse-label tasks (e.g., early prediction of sepsis in ICU), (2) visual explanations aligning with medical reasoning, and (3) reduced annotation burden. This approach bridges the gap between automated representation learning and clinical interpretability, particularly benefiting underrepresented populations with limited labeled data.