**Title:** Schema-Aware Multimodal Pre-training for Table-Text Alignment

**Motivation:** Understanding tables often requires interpreting not just cell values but also column headers (schema) and associated free-form text (e.g., captions, surrounding paragraphs). Existing models struggle to effectively fuse these modalities, especially respecting the structured nature of schema. This research aims to improve table understanding by explicitly modeling the relationship between table structure, content, and related textual descriptions.

**Main Idea:** We propose a multimodal pre-training approach that jointly learns representations for table schema, cell values, and accompanying text. Unlike methods that flatten tables, we will use structure-aware encoders (e.g., graph neural networks or specialized transformers) for the schema and cell grid. Text will be encoded using a standard transformer. The model will be pre-trained on large-scale table-text corpora using novel objectives: 1) Schema-Text Alignment: predicting relevant text snippets for given column headers, 2) Cell-Text Grounding: linking cell values to mentions in the text, and 3) Table-Text Matching: determining if a text accurately describes a table. Expected outcomes include improved performance on text-conditional table tasks (e.g., text-to-SQL, table-based QA) and better table summarization.