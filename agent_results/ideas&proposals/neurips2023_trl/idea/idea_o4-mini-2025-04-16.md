Title: Tri-View Contrastive Pretraining for Unified Table Embeddings

Motivation:  
Current table representation models often specialize in a single format (e.g., CSV text or schema graphs) and struggle to generalize across tasks like QA, semantic parsing, and classification. Real‐world tables appear in diverse modalities—flat text, relational schemas, or embedded SQL—and capturing their shared semantics is crucial for robust, cross‐format transfer.

Main Idea:  
We propose a tri‐view pretraining framework that generates three complementary views for each table:  
1. Text View – row-major serialization of header and cells fed into a transformer.  
2. Graph View – schema and cell relationships encoded by a graph neural network.  
3. SQL View – synthetic SQL queries sampling table rows processed by a seq-to-seq model.  

During pretraining, we apply:  
- Contrastive alignment loss to pull representations of the same table across views closer.  
- Masked cell modeling in each view to capture local semantics.  

We collect millions of diverse web tables and relational DBs to train this multi‐encoder. Fine-tuning on tasks (text-to-SQL, QA, classification) leverages any single view or fused embeddings. Expected outcomes include improved cross-format transfer, fewer labeled examples for downstream tasks, and a unified embedding space for heterogeneous tables, fostering more adaptable table understanding systems.