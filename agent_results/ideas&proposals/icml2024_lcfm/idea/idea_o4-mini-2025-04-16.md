Title: Hierarchical Retrieval–Compression for Extreme-Length Contexts

Motivation:  
Tasks such as multi-document QA, large-scale codebase comprehension, and genomic analysis demand synthesizing millions of tokens—far beyond current context windows. Existing retrieval-augmented models either overload memory with raw passages or compress in a non-adaptive way, leading to degraded accuracy and inefficiencies.

Main Idea:  
We propose an end-to-end pipeline combining a trainable retriever, a multi-granularity Context Compressor, and a long-context foundation model. Given a query, the retriever fetches coarse-grained “document chunks.” The Context Compressor employs cross-chunk attention and gated summarization to produce hierarchical embeddings (sentence-, paragraph-, and chunk-level summaries). These summaries, along with selected raw passages, are then fed into a transformer with specialized hierarchical cross-attention layers that adaptively allocate compute to the most relevant context granularity.  
Training jointly fine-tunes the retriever, compressor, and transformer via a combined retrieval-and-language-modeling objective. We will benchmark on multi-document QA, code search across vast repositories, and genomic pattern discovery.  
Expected outcomes include improved answer accuracy, linear-to-sublinear scaling in compute cost, and applicability to domains with millions of tokens.