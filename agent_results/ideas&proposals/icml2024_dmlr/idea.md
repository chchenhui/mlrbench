**Title:** Adaptive Model-Assisted Dataset Construction with Diversity-Aware Feedback Loops  

**Motivation:** Building high-quality, diverse datasets for emerging domains (e.g., climate science, robotics) remains labor-intensive and inefficient. Traditional model-assisted methods prioritize sheer scale or basic quality checks but often neglect diversity, leading to biased or incomplete datasets that hinder foundation model performance.  

**Main Idea:** Propose an iterative framework where foundation models actively guide dataset creation via *diversity-aware feedback loops*. The approach involves: (1) Initial model training on seed domain data; (2) Synthetic data generation targeting underrepresented patterns identified by clustering latent embeddings; (3) Active learning-driven human validation to verify quality and fill critical gaps; and (4) Continuous metrics quantifying diversity (e.g., distributional coverage) and quality (e.g., cross-model consistency) to refine the process. Expected outcomes include datasets with demonstrably higher diversity and task specificity compared to static model-assisted methods. This could reduce annotation costs by 30–50% in domains like biomedical imaging, while improving downstream model robustness to distribution shifts. The framework’s modular design enables adaptation to niche domains, advancing ethical data practices by explicitly monitoring bias during construction.