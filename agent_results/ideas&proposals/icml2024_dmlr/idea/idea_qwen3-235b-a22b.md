1. **Title**: AutoCurate: Learning to Predict and Improve Data Quality Using Model-Assisted Analytics  

2. **Motivation**: Despite the dominance of foundation models, data curation remains ad hoc and labor-intensive, with manual cleaning and labeling being prohibitively expensive for large datasets. Existing quality signals (e.g., outlier scores, label consistency) lack holistic integration or domain adaptation, leading to suboptimal data quality that compromises model reliability. Automating the identification of noisy, biased, or redundant samples, and guiding curation would drastically enhance the scalability and fairness of foundation models, especially in under-resourced domains.  

3. **Main Idea**: Propose a self-supervised, model-assisted framework called *AutoCurate* that learns to evaluate and enhance dataset quality through meta-learning. The system trains a meta-classifier on domain-agnostic dataset features (e.g., feature importance, sample complexity, label noise) using data streams from heterogeneous domains. This classifier predicts quality scores for samples and recommends automated curation actions (e.g., relabeling, augmentation, deduplication). To ensure robustness, AutoCurate leverages feedback from a lightweight vision/language foundation model during training to penalize samples that degrade downstream task performance. The framework also incorporates active learning to prioritize ambiguous or high-risk samples for human-in-the-loop review. We expect AutoCurate to reduce curation costs by 50%+ while improving foundation model accuracy and fairness, and enabling dynamic adaptation to dataset drifts. This work bridges the gap between data-centric research and foundation model deployment, advancing reproducibility and ethics via automated bias detection and provenance-aware curation.