Title: Entropy-Regularized Differentiable Shortest-Path (SoftSP)

Motivation:  
Classical shortest-path algorithms (e.g., Dijkstra’s, Bellman-Ford) are non-differentiable at the argmin step, preventing end-to-end learning of graph edge weights or parameters in systems that incorporate path planning. Enabling differentiable path planning would allow training planners from partial or noisy trajectory data, integrate planners into larger pipelines, and improve robustness to uncertain costs.

Main Idea:  
We introduce SoftSP, a relaxation of the Bellman recursion using an entropy-regularized “softmin” instead of hard minima. Each node’s cost‐to‐go is computed via a log-sum-exp over incoming edges, yielding smooth value functions and a distribution over paths. We derive closed-form gradients of expected path length w.r.t. edge weights and implement SoftSP as a block within automatic-differentiation frameworks. During training, observed (partial or noisy) trajectories serve as weak supervision, and stochastic gradient descent recovers latent edge parameters. We will evaluate on grid mazes, real-world traffic networks, and integrate SoftSP into reinforcement learning agents. Expected outcomes include faster convergence from sparse feedback, smoother policy gradients, and seamless integration of classical planning into neural architectures.