**Title:** Differentiable Optimal Transport Top-K for Adaptive Sparse Computation

**Motivation:** Large models often employ uniform computation across all inputs, leading to inefficiency. Adaptive sparsity, selecting the top-K most relevant computational units (e.g., experts, attention heads, tokens) per input, can drastically reduce cost. However, the discrete nature of Top-K selection hinders end-to-end optimization of the selection mechanism.

**Main Idea:** We propose a differentiable Top-K operator based on Optimal Transport (OT), specifically using entropic regularization (Sinkhorn algorithm). Given scores for N items, we formulate selecting the top K as an OT problem mapping a uniform distribution over K "slots" to the N items, maximizing the total score transferred. The resulting doubly-stochastic transport plan provides soft, differentiable weights indicating selection importance. Integrating this OT-TopK layer allows networks (e.g., Mixture-of-Experts, Transformers) to learn *how* to best select the most informative K units via backpropagation. This approach offers potentially smoother gradients and more principled relaxation than alternatives like Gumbel-Softmax, enabling more effective training of adaptive, computationally efficient models.