**Title:** Meta-Learning Robust Solvers for Inverse Problems with Forward Model Uncertainty

**Motivation:** Deep learning solutions for inverse problems often achieve state-of-the-art results but rely heavily on precise knowledge of the forward operator and noise model. In many real-world applications (e.g., medical imaging, geophysics), the exact forward model is unknown or varies due to calibration errors, environmental changes, or simplified assumptions, leading to performance degradation. This research aims to develop learned solvers robust to such model uncertainties.

**Main Idea:** We propose a meta-learning framework to train inverse problem solvers that can generalize across a *distribution* of forward models. Instead of training on a single, fixed forward operator, the network will be optimized using episodes where each episode involves a slightly perturbed or different forward model sampled from a predefined uncertainty distribution (e.g., varying operator parameters, adding unmodeled physics, diverse noise types). The meta-objective will encourage the network to rapidly adapt or perform well on average across these sampled models. We expect this approach to yield reconstruction models that maintain high accuracy and stability even when the true underlying physics deviates from the nominal model assumed during standard training, enhancing practical deployment reliability.