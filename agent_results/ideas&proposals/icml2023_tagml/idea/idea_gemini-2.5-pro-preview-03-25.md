**Title:** Geometric Intrinsic Dimension for Model Interpretability and Regularization

**Motivation:** Understanding *why* deep learning models work remains challenging due to their high-dimensional parameter spaces and complex learned representations. Current interpretability methods often focus on input attribution, while regularization techniques (like L1/L2) don't capture the intrinsic geometric complexity of the function learned by the network.

**Main Idea:** We propose leveraging geometric measures, specifically the *intrinsic dimension* of data representations within neural network layers, for both interpretability and regularization. Using techniques from manifold learning (e.g., estimating dimension via nearest neighbours or topological methods), we will track how the intrinsic dimension evolves during training across different layers. We hypothesize that effective models progressively reduce intrinsic dimensionality towards the output layer. This measure can serve as a novel interpretability metric reflecting information compression. Furthermore, we will introduce a regularization term that penalizes unnecessarily high intrinsic dimensions in intermediate layers, encouraging models to learn simpler, more compressible representations. This geometrically-informed regularization aims to improve generalization and potentially robustness compared to standard techniques.