**Title:** Pluralistic Value Alignment through Participatory Moral Mapping  
**Motivation:** Current AI alignment methods, like RLHF, risk homogenizing human values by over-representing dominant cultural or demographic groups. This limits AI's ability to equitably serve diverse populations and perpetuates systemic biases. There is urgent need for frameworks that systematically integrate pluralistic moral perspectives into AI systems.  

**Main Idea:** This research proposes a participatory framework that combines *moral foundations theory* (from psychology) with co-design workshops to map culturally diverse values onto AI systems. First, diverse stakeholders collaboratively identify core moral priorities (e.g., care, fairness, autonomy) in context-specific AI applications. These are formalized as value “weights” using participatory ranking and deliberation techniques. Next, a graph-based model translates these weights into alignment constraints for AI training, enabling systems to dynamically adapt outputs to different value profiles while maintaining transparency. Expected outcomes include (1) a toolkit for auditing value diversity in existing AI systems and (2) a modular architecture for pluralistic alignment. The approach challenges the RLHF paradigm by decentralizing value specification, potentially reducing bias in AI ethics workflows.