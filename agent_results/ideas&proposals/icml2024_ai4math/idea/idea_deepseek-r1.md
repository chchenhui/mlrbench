**Title:** Dual-Process Autoformalization via Hybrid Neural-Symbolic Dependency Graphs  

**Motivation:** Autoformalization bridges human-readable mathematical reasoning and machine-verifiable formal proofs, but current methods lack precision due to ambiguities in natural language and rigid formal syntax. This limits collaboration between humans and AI in theorem proving and formal verification. A robust solution would accelerate mathematical discovery and reliable code generation.  

**Main Idea:** Develop a hybrid model combining neural networks with symbolic dependency graphs to jointly optimize natural-to-formal (autoformalization) and formal-to-natural (reverse informalization) translations. The neural component (e.g., fine-tuned LLM) generates initial mappings, while symbolic graphs explicitly model logical dependencies (e.g., theorem premises, lemmas) to refine outputs via constraint satisfaction. Training uses aligned corpora of natural proofs (e.g., arXiv) and formal proofs (e.g., Lean4 libraries), with reinforcement learning to minimize graph inconsistency and maximize human evaluatorsâ€™ clarity ratings. Metrics include *graph alignment score* (measuring structural match) and *reversibility loss* (ensuring bidirectional coherence). Expected outcomes: higher precision in formalization (reducing manual correction by 50%) and more interpretable informalization. Impact: Enables seamless human-AI co-reasoning, aiding education, formal verification, and theorem discovery.