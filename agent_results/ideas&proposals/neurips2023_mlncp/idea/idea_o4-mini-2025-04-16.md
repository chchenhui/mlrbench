Title: Noise-Resilient Deep Equilibrium Energy-Based Models for Analog Accelerators

Motivation:  
Emerging analog and (opto-)analog hardware promise massive energy and throughput gains but suffer from noise, device mismatch, and low bit‐depth. Traditional deep models falter under these constraints. Energy‐based models (EBMs) and deep equilibrium networks (DEQs) are inherently implicit and iterative—ideal for analog physics—but require new algorithms that embrace hardware imperfections to unlock sustainable, high‐performance generative AI.

Main Idea:  
We propose a co‐design of an EBM/DEQ hybrid that (1) leverages implicit fixed‐point solvers mapped onto analog crossbar arrays and (2) incorporates a noise‐adaptive regularizer during training. A differentiable hardware simulator injects measured device noise and mismatch into the forward pass; the model learns explicit stability margins via a noise‐robust loss term. Implicit differentiation is approximated through low‐precision random perturbations compatible with analog updates. On prototype analog chips, our method aims to achieve 3–5× energy reduction versus GPU baselines while maintaining generative quality, paving the way for large‐scale, sustainable EBM training and inference.