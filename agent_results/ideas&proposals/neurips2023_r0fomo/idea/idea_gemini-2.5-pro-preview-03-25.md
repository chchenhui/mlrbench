**Title:** Contextualized Adversarial Prompts for Evaluating Few-Shot Robustness

**Motivation:** Evaluating the robustness of few-shot learning in foundation models is challenging. Standard adversarial attacks may not reflect realistic failures, and manually finding vulnerabilities is slow. We need automated methods to identify specific weaknesses introduced or amplified by the few-shot examples and prompts themselves, revealing blind spots beyond general model vulnerabilities.

**Main Idea:** Develop an algorithm, Contextualized Adversarial Prompting (CAP), that generates adversarial *prompts* or *few-shot examples* optimized to cause failures for a specific task defined by initial few-shot data. Instead of perturbing inputs directly, CAP uses gradient-based or search methods to subtly modify the prompt instructions or the provided examples to maximize prediction error or elicit harmful outputs on clean target inputs. This evaluates robustness specifically tied to the few-shot context, not just the underlying model. Expected outcome: An automated tool identifying brittle prompt/example combinations and specific task formulations where the model is weak. Impact: Enables targeted robustness improvements and safer deployment by revealing context-specific vulnerabilities in few-shot learning scenarios.