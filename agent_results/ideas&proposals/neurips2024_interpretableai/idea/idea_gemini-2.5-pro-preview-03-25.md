**Title:** Adversarial Faithfulness Probing for Inherently Interpretable Models

**Motivation:** Evaluating inherently interpretable models often relies on subjective inspection or proxy metrics. This lack of rigorous, quantitative assessment hinders trust and reliable deployment, especially in critical applications. We need objective methods to verify that the model's explicit interpretable structure accurately reflects its decision-making process and is not misleading.

**Main Idea:** We propose an adversarial probing framework to quantify the faithfulness and reliability of inherently interpretable models (e.g., rule lists, sparse linear models, GAMs). The core idea involves training simple "probe" models to predict the interpretable model's output using *only* its explicit interpretable structure (e.g., weights, rules). We then generate adversarial input perturbations specifically designed to maximally change the interpretable components' activations or applicability. By comparing the probe's predictions under perturbation with the actual interpretable model's output changes, we measure alignment. High alignment indicates faithfulness; discrepancies reveal reliance on implicit factors or structural unreliability. This yields robust, quantitative faithfulness scores, enabling better model selection and building trust.