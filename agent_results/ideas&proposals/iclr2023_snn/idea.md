**Title:** Adaptive Compute Fabric for Accelerated Sparse Neural Network Training

**Motivation:** While sparse training algorithms promise significant efficiency gains, current hardware like GPUs struggles with the irregular computation and memory access patterns inherent in sparsity, limiting practical speedups and energy savings. This research aims to bridge this gap by designing hardware specifically tailored for sparse operations.

**Main Idea:** We propose an adaptive compute fabric (ACF) co-designed with sparse training algorithms. The ACF will feature specialized compute units capable of dynamically bypassing zero-operand multiplications and accumulations, coupled with dedicated memory controllers optimized for fetching non-zero weights and activations based on sparse indices (e.g., CSR/CSC formats). We will investigate reconfigurable interconnects within the fabric to adapt dataflow to varying sparsity patterns during training. The co-design involves tailoring pruning strategies (e.g., magnitude pruning with structured sparsity constraints) to maximize ACF utilization. Expected outcomes include significantly reduced training time and energy consumption for sparse models compared to GPU implementations, potentially enabling efficient training of much larger models.