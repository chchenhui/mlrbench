Title: BlockDySparsity – Hardware-Friendly Dynamic Block-Sparse Training for Sustainable Deep Learning

Motivation:  
Irregular sparsity in large neural networks delivers high theoretical savings but fails to exploit parallel hardware, leading to underutilized compute units and marginal energy gains. By aligning sparsity patterns with hardware-friendly blocks and dynamically adapting them during training, we can significantly cut energy and memory use without sacrificing accuracy.

Main Idea:  
We propose BlockDySparsity, a co-design framework that learns and updates block-structured sparsity masks during training. Each layer is partitioned into fixed-size blocks; an importance score for each block is computed via gradient-based saliency. At regular intervals, low-saliency blocks are pruned and high-saliency dormant blocks are regrown, preserving overall sparsity. We implement custom CUDA kernels to accelerate block-sparse matmuls on GPUs and integrate a lightweight reinforcement learning agent to adjust block sizes and sparsity ratios per layer on the fly. We will evaluate on ResNet-50 and BERT pretraining, targeting ≥30% reduction in training FLOPs, ≥40% energy savings, and <1% top-1 accuracy drop. This work bridges algorithmic sparsity and practical hardware support, paving the way for more sustainable large-scale model training.