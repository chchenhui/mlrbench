**Title:** Aligning Code LLMs for Performance via Execution-Guided Fine-tuning

**Motivation:** Current alignment techniques for code LLMs primarily focus on functional correctness using execution feedback (pass/fail). However, practical software development often requires optimizing for non-functional properties like runtime efficiency or memory usage. This research aims to align code LLMs to generate not only correct but also performant code by leveraging richer execution signals.

**Main Idea:** We propose fine-tuning code LLMs using reinforcement learning guided by multi-objective execution feedback. Initial code generation models produce candidate solutions. These are compiled and executed against test suites, collecting not only correctness status but also performance metrics (e.g., CPU time, peak memory). A reward function is designed to heavily penalize incorrect solutions while rewarding correct solutions inversely proportional to their resource consumption (e.g., faster execution gets higher reward). This performance-aware reward signal will be used to fine-tune the model (e.g., via PPO), steering it towards generating more efficient code implementations while maintaining correctness. The expected outcome is a model capable of generating functionally correct code that is demonstrably more performant than its baseline counterpart.