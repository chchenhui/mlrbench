**Title:** PAC-Bayesian Guided Exploration for Sample-Efficient Deep Active Learning

**Motivation:** Deep active learning aims to reduce labeling costs by selecting the most informative unlabeled data. However, existing heuristics (e.g., uncertainty sampling) lack strong theoretical grounding for sample efficiency, especially regarding generalization. PAC-Bayesian theory provides bounds on generalization error for probabilistic models, making it ideal for principled active learning.

**Main Idea:** We propose a novel deep active learning algorithm, PB-AL (PAC-Bayes Active Learner). Instead of relying purely on point-estimate uncertainty, PB-AL maintains a posterior distribution over model parameters (e.g., using MC Dropout or ensembles). It queries data points that maximally reduce a PAC-Bayesian bound on the expected generalization error of the posterior mean predictor. This involves selecting points expected to most significantly tighten the bound, either by reducing the empirical error term or the KL divergence complexity term. This approach directly optimizes for guaranteed generalization improvement, promising more sample-efficient learning and robust performance compared to standard heuristics. We will evaluate PB-AL on image classification benchmarks.