Title: Revealing Architecture-Specific Inductive Biases Through Dataset Distillation  
Motivation: Deep learning architectures excel empirically but lack principled understanding of their inductive biasesâ€”critical for model selection and robust design. Traditional approaches focus on theoretical bounds, but empirical studies can directly unveil how different architectures prioritize input patterns during training.  
Main Idea: We propose using *dataset distillation* as a probe to uncover inductive biases. Given a full dataset with structured patterns (e.g., hierarchical or geometric rules), we will synthesize minimal datasets that preserve performance, comparing CNNs, Transformers, and hybrids. By analyzing the distilled datasets, we aim to answer: (1) Which architectural types preserve high-level semantic structures (e.g., compositional features in Transformers) versus local patterns (e.g., textures in CNNs)? (2) How do these differences affect robustness to distribution shifts? We will pair this with intervention experiments: modifying the distilled datasets to introduce targeted spurious correlations and measuring how architectures adapt. Expected outcomes include a framework for systematically assessing inductive biases, empirical validation of hypotheses (e.g., Transformers prioritize global compositionality), and actionable insights for architecture design. The results will bridge theory-practice gaps, guiding applications where specific biases are required (e.g., scientific modeling vs. pixel-level consistency).