**Title:** Unmasking the Role of Attention Heads in Transformer In-Context Learning  

**Motivation:** In-context learning (ICL) enables transformers to adapt to new tasks via prompts, but the mechanisms behind this capability remain unclear. Prior work hypothesizes that specialized attention heads mediate task-specific computations. Validating or falsifying this hypothesis is critical to understand transformers’ emergent abilities and guide efficient model design.  

**Main Idea:** This work systematically ablates attention heads during ICL inference across diverse tasks (e.g., arithmetic, text editing) to test if specific heads are “task experts.” Experiments involve: (1) training transformer variants on multi-task datasets, (2) measuring post-ablation performance degradation per task, (3) analyzing attention patterns for task-specificity, and (4) scaling studies to assess stability across model sizes. Expected outcomes include identifying head subsets critical for distinct tasks, correlations between head specialization and scaling efficiency, and evidence supporting/refuting the “task expert” hypothesis. Results could inform architecture simplifications (e.g., pruning irrelevant heads) and training strategies to enhance ICL in smaller models.