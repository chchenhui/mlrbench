1. Title: Self-Interventional Distillation for Robust Large Language Models

2. Motivation:  
Large language models often latch onto spurious correlations in training data, leading to brittle behavior under distribution shifts. Injecting causal inductive biases via interventions can improve robustness, but collecting real counterfactual examples is costly. A self-interventional approach enables models to generate and learn from their own counterfactuals, reducing reliance on external data and enhancing trustworthiness in safety-critical applications.

3. Main Idea:  
We propose a two-stage self-interventional distillation framework:  
• Intervention Generation: Use prompt-based causal probing to identify latent “cause” variables (e.g., entity attributes, sentiment markers). The model then generates counterfactual text pairs by algorithmically intervening on one variable at a time (negation flips, entity swaps, stylistic shifts). A lightweight discriminator filters out implausible examples.  
• Distillation with Consistency Loss: Finetune the model on these self-generated pairs using a contrastive consistency objective that enforces invariance to non-causal features and sensitivity to causal changes.  
Expected outcomes include improved out-of-distribution generalization on benchmarks like Adversarial NLI and RobustQA, reduced spurious feature reliance, and a scalable pipeline for infusing causal robustness into large models without extra annotation.