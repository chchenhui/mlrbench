**Title:** Auto-Distill: Adaptive Model Compression via Learned Information Allocation

**Motivation:** Compressing large foundation models is crucial for deployment, but uniform compression (e.g., fixed quantization bits across layers) is suboptimal. Different parts of a model contribute differently to performance, suggesting information capacity should be allocated adaptively. Current methods often require expensive search or heuristics to determine layer-specific compression ratios.

**Main Idea:** We propose Auto-Distill, a framework that automatically learns how to allocate "compression budget" (e.g., bits for quantization, sparsity levels) across different model components during distillation. We introduce a small controller network, trained alongside the student model, that dynamically outputs compression parameters (e.g., per-layer bit-widths) based on the model state or input data characteristics. The controller is optimized using reinforcement learning or differentiable approaches to maximize student performance under a global compression constraint (e.g., total model size or FLOPs). This allows fine-grained, adaptive compression tailored to the model's information flow, potentially achieving better rate-performance trade-offs than fixed strategies, especially for complex architectures and diverse deployment constraints.