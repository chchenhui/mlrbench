**Title:** Self-Correcting MFM-Policies for Robust Low-Level Embodied Control

**Motivation:** Multi-modal Foundation Models (MFMs) excel at high-level reasoning and planning for embodied agents, but often struggle with the nuances of low-level control execution in dynamic or unpredictable environments. Plans generated by MFMs can easily fail due to minor execution errors or environmental changes not anticipated by the high-level model, requiring robust error detection and correction mechanisms.

**Main Idea:** We propose a framework where an MFM acts as a high-level planner, generating sub-goals or skill primitives for a low-level controller. Crucially, we augment the MFM to also predict the *expected visual outcome* of executing each sub-goal. During execution, the agent compares the actual visual observation post-action with the MFM's prediction. A significant mismatch triggers a self-correction module: the MFM receives the current state, the failed sub-goal, and the discrepancy description/image as input, prompting it to generate a revised, corrective sub-goal or suggest replanning. This closed-loop interaction allows the agent to leverage the MFM's reasoning capabilities not just for initial planning but also for real-time, low-level error correction, leading to more robust and adaptable embodied agents.