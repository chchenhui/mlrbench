**Title:** Self-Supervised Curriculum Learning via Emergent Task Generation in Open-Ended Environments  

**Motivation:**  
Current reinforcement learning (RL) systems often terminate learning after mastering predefined tasks, failing to sustain lifelong skill acquisition akin to human adaptability. Open-ended environments demand agents that continuously encounter novel challenges, yet designing curricula to drive such learning remains a key challenge. Existing methods rely on static task distributions or external supervision, limiting scalability and adaptability. This work addresses how to autonomously generate an endless stream of progressively complex tasks without human priors, enabling agents to develop robust, general capabilities in dynamic real-world scenarios like sim2real transfer.  

**Main Idea:**  
We propose a self-supervised curriculum learning framework where an agent’s own interactions with the environment generate tasks. A large generative model (e.g., LLM) learns to synthesize tasks by abstracting patterns from the agent’s past experiences, prioritizing those that maximally improve underrepresented skills (e.g., exploration, planning). The environment dynamically adjusts task difficulty via a population of co-evolving adversarial agents, ensuring challenges remain solvable yet cognitively demanding. Methodologically, we combine quality-diversity algorithms with self-play, using unsupervised skill discovery to identify emergent capabilities. Expected outcomes include agents that exhibit open-ended learning trajectories, with measurable gains in out-of-distribution generalization and zero-shot adaptation. This approach could revolutionize training for robotics or digital ecosystems, where sustained, autonomous skill growth is critical.