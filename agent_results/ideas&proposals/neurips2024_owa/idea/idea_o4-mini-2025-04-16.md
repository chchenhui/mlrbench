Title: RL-CoT: Reinforcement Learning with Dynamic Chain-of-Thought for Open-World Decision-Making

Motivation:  
Open-world environments demand agents that can both reason about long-term objectives and make low-level control decisions under uncertainty. While large language models excel at multi-step reasoning (chain-of-thought), they lack grounded interaction capabilities; conversely, RL agents adapt control policies but struggle with abstract planning. Bridging this gap can lead to agents that adaptively generate and refine plans in novel, dynamic settings without extensive supervision.

Main Idea:  
We propose RL-CoT, a hybrid framework that interleaves LLM-based reasoning and RL-based control in an iterative loop. At each decision point, an LLM module produces a chain-of-thought outlining subgoals and rationale. A hierarchical RL policy conditions on this reasoning trace to execute environment interactions through specialized sub-policies. Feedback from the environment (rewards and observations) is fed back to the LLM to refine subsequent chains-of-thought via self-supervised fine-tuning. We employ a dynamic replay buffer that prioritizes high-quality reasoning traces and successful trajectories. Evaluated in procedurally generated 3D simulation tasks, RL-CoT is expected to improve sample efficiency, generalize to unseen scenarios, and demonstrate interpretable, adaptive planning in open-world settings.