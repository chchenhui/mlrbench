**Title:** Function-Space Model Merging via Optimal Transport Alignment

**Motivation:** Merging independently trained neural networks can improve performance and reduce computational costs. However, simply averaging parameters often fails due to functional differences arising from symmetries in the loss landscape. Existing methods often rely on heuristic parameter matching. Aligning models based on their functional similarity *before* merging offers a more principled approach.

**Main Idea:** We propose merging models by first aligning their intermediate representations layer-wise using Optimal Transport (OT). For two models trained on similar tasks, we compute the OT plan that maps activation distributions from a layer in one model to the corresponding layer in the other, minimizing a chosen transport cost (e.g., Euclidean distance). This plan identifies the optimal neuron permutation/transformation to functionally align the layers. After applying these OT-derived transformations to the parameters (weights and biases) of one model's layers, we average the parameters of the aligned models. We expect this function-space alignment approach to yield merged models with significantly better performance than naive parameter averaging or basic permutation methods, enabling more effective model reuse and ensemble building.