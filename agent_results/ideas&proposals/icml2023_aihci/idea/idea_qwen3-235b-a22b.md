1. **Title**: Explanatory Reinforcement Learning from Human Feedback: Bridging Intent and Action through Interpretable Policies  

2. **Motivation**: Current reinforcement learning from human feedback (RLHF) methods rely on scalar rewards, which often fail to capture the richness of human intent, leading to suboptimal policies and reduced user trust. This limitation hinders applications like adaptive user interfaces, personalized robotics, and collaborative AI tools, where understanding *why* a decision is made is critical for alignment and safety.  

3. **Main Idea**: We propose a framework that extends RLHF by jointly modeling scalar rewards and natural language explanations provided by users during feedback. A dual-model architecture is introduced: (1) a policy network learns task execution via reinforcement learning, and (2) a language model maps explanations to semantic reward modifiers, dynamically refining the reward function. The system also generates justifications for its actions, enabling users to critique both decisions and reasoning. This bidirectional loop—where explanations shape policies and policies generate explainable actions—enhances learning efficiency and transparency. We expect this approach to reduce the sample complexity of RLHF, improve generalization to novel scenarios, and empower users to correct misalignments proactively. By bridging intent and action, the framework could advance ethical AI systems in healthcare, education, and human-AI collaboration, where accountability and adaptability are paramount.