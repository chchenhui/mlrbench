**Title:** Interactive Visual Correction for Personalized Generative Models

**Motivation:** Generative AI models often struggle to perfectly capture user intent, requiring tedious re-prompting or parameter tuning. Existing correction methods lack directness and intuitive control. This research aims to bridge this gap by enabling users to directly correct model outputs visually, fostering more efficient personalization and user satisfaction.

**Main Idea:** We propose a framework where users can visually manipulate generated outputs (e.g., sketching corrections on an image, rearranging elements in a UI layout). These interactions are interpreted as direct feedback signals. We will investigate methods to translate these visual edits into effective learning signals (e.g., contrastive learning objectives based on edits, gradients via differentiable rendering, or updating personalized adapter layers). This allows the model to incrementally adapt to the user's specific preferences and correction patterns without extensive retraining. The expected outcome is a more intuitive, correctable, and personalized generative system, evaluated through user studies measuring task efficiency and satisfaction.