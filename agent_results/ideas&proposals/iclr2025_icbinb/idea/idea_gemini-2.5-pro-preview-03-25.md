**Title:** Beyond Accuracy: Quantifying the Impact of Latent Distribution Shifts on Deep Learning Reliability

**Motivation:** Deep learning models excel on benchmarks but often falter in real-world deployment due to distribution shifts. While shift detection is studied, we lack a clear understanding of *how* different types of subtle, latent shifts (e.g., changing correlations between features, not just marginal distributions) specifically impact model reliability and internal representations, leading to unexpected failures even when overall accuracy seems acceptable.

**Main Idea:** This research proposes investigating the link between specific, hard-to-detect latent distribution shifts and model failure modes beyond simple accuracy drops (e.g., loss of calibration, increased vulnerability to adversarial examples, fairness violations). We will curate real-world or simulated datasets exhibiting controlled latent shifts (e.g., changes in feature dependencies conditioned on the class). We will then analyze how these shifts affect not only model predictions but also internal representations (using techniques like Centered Kernel Alignment or neuron activation analysis) and reliability metrics across various architectures. The goal is to identify characteristic "signatures" of failure induced by specific latent shifts, providing a deeper understanding of why models fail unpredictably and guiding the development of more robust training or adaptation techniques sensitive to these subtle changes.