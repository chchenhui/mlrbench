**Title:** Calibrated Uncertainty Quantification for Robust Medical Image Analysis under Domain Shift

**Motivation:** Deep learning models often provide overconfident predictions, especially when encountering data different from their training distribution (domain shift), a common scenario in clinical deployment. Reliable uncertainty estimation is crucial for safe clinical decision-support, indicating when a model's prediction should not be trusted. This research addresses the need for robust and well-calibrated uncertainty measures in medical imaging AI.

**Main Idea:** We propose developing a novel framework combining post-hoc calibration techniques (e.g., temperature scaling, histogram binning) with test-time domain adaptation strategies. The core idea is to dynamically adjust uncertainty calibration based on detected shifts between the training data statistics and the incoming clinical images/volumes during inference. This involves learning adaptation parameters online or using unsupervised domain adaptation methods to align feature distributions before calibration. Expected outcomes include significantly improved calibration (e.g., lower Expected Calibration Error) on out-of-distribution clinical data, enhancing model reliability and facilitating safer AI integration into clinical workflows for tasks like segmentation or diagnosis.