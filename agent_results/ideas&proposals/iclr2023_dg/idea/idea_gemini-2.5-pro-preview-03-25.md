**Title:** Cross-Modal Consistency as a Supervisory Signal for Domain Generalization

**Motivation:** Models trained on single modalities often latch onto spurious correlations specific to the training domains, hindering generalization. While multi-modal data offers richer information, simply fusing modalities doesn't guarantee robustness if spurious correlations exist across modalities or shift differently. We propose that inconsistencies *between* modalities under distribution shifts can serve as implicit supervision to identify and mitigate reliance on non-robust features.

**Main Idea:** We propose a framework that utilizes cross-modal prediction consistency as a signal for learning domain-invariant representations. During training on multiple source domains using paired multi-modal data (e.g., images and text), the model makes predictions based on each modality independently, as well as jointly. We introduce a consistency regularization loss that penalizes divergence between unimodal predictions and the joint prediction, especially for samples where cross-domain shifts are likely (identified via domain labels or unsupervised clustering). This encourages the model to rely on features that yield consistent interpretations across modalities, presumably capturing more core, invariant aspects of the data, rather than domain-specific artifacts within a single view. Expected outcome: Improved generalization to unseen domains by learning representations less sensitive to modality-specific spurious correlations.