**1. Title:** Cross-Modal Invariance Learning for Domain Generalization  

**2. Motivation:**  
Domain generalization (DG) struggles in real-world scenarios due to domain shifts altering input distributions. While existing DG methods underperform against baselines, multi-modal data (e.g., text, images, sensors) offers redundant, complementary views of the same latent concept. Exploiting cross-modal consistency could reveal domain-invariant features, yet few approaches explicitly align multi-modal representations to enforce invariance. This work bridges this gap, enabling models to generalize by learning features shared across both modalities and domains.  

**3. Main Idea:**  
We propose a framework that trains modality-specific encoders to produce **jointly normalized representations** (JNRs) aligned via cross-modal contrastive learning. First, each modality is encoded into a domain-invariant latent space using adversarial domain-adaptation. Second, JNRs are enforced by normalizing modalities to a shared distribution (e.g., unit Gaussian) and training encoders to align multi-modal samples from the same instance in this space. Finally, a classifier learns from fused JNRs with a domain-agnostic loss. This leverages cross-modal agreement as a supervisory signal, ensuring features are both modality-independent and domain-invariant. Experiments on medical imaging (X-ray/MRI) and autonomous driving (LiDAR/Camera) datasets will assess performance. We expect improved accuracy on unseen domains compared to ERM and existing DG methods, advancing robustness in critical multi-modal applications.