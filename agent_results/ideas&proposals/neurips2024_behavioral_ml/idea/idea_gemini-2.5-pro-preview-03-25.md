**Title:** Interpreting AI Failures through the Lens of Cognitive Biases

**Motivation:** AI systems, particularly complex ones like LLMs, can exhibit unexpected or erroneous behaviors that are difficult to understand using standard interpretability techniques. These failures often lack intuitive explanations, hindering trust and debugging. Behavioral science offers rich models of human cognitive biases, which describe systematic errors in human judgment and decision-making.

**Main Idea:** This research proposes mapping AI failure modes onto established cognitive biases from behavioral science. We hypothesize that certain AI errors might resemble human biases (e.g., confirmation bias in recommendation systems, anchoring effects in predictive models). We will develop methods to analyze model inputs, internal states, and outputs during failure cases to identify patterns consistent with specific cognitive biases. The output would be a "bias attribution" explaining the failure in human-understandable terms (e.g., "The model's prediction error resembles anchoring bias, over-relying on initial information."). This approach aims to provide more intuitive, actionable explanations for AI failures, facilitating better debugging and user understanding.