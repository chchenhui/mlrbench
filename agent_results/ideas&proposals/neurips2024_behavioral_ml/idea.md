**Title:** Cognitive Architecture-Guided Training for Human-Like Reasoning in Language Models  

**Motivation:** Large language models (LLMs) often produce outputs that lack transparent, human-like reasoning, limiting their trustworthiness and alignment with human expectations. Bridging this gap requires integrating formal cognitive models to ground LLM decisions in validated psychological processes.  

**Main Idea:** This research proposes a framework where computational cognitive architectures (e.g., ACT-R, CLARION) guide LLM training and inference. Cognitive models, which simulate human memory, attention, and problem-solving, will be used to structure latent reasoning pathways in LLMs. Methodologically, we design (1) a hybrid training objective combining language modeling loss with alignment to cognitive model "traces" (e.g., penalizing deviations in step-by-step reasoning), and (2) a constrained decoding mechanism that prioritizes token sequences matching cognitive architecture-predicted steps. For evaluation, we measure behavioral congruence with human experiments (e.g., syllogistic reasoning tasks) and user-perceived naturalness. This approach aims to produce LLMs that generate explanations, decisions, and interactions that are both performant and psychologically interpretable, enhancing collaboration in education, healthcare, and human-AI teams.