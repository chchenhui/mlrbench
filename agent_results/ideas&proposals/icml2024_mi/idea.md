**Title:** Modeling Cognitive Effort in Human Feedback for Robust AI Alignment  

**Motivation:** Current human feedback models in AI alignment (e.g., RLHF) often assume humans act rationally and consistently, ignoring the cognitive effort required to provide feedback. This leads to misalignment when humans simplify decisions due to task complexity or fatigue. Addressing this gap is critical for building AI systems that robustly infer human preferences in real-world, effort-intensive scenarios.  

**Main Idea:** This work proposes a *cognitive effort-aware feedback model* that explicitly quantifies the trade-off between human decision-making accuracy and mental effort. Drawing from bounded rationality frameworks in cognitive science, the model treats feedback as a noisy, effort-constrained approximation of true preferences. Methodologically, we integrate effort dynamics into inverse reinforcement learning by jointly inferring human preferences and effort levels via hierarchical Bayesian inference. The model will be trained and validated on behavioral datasets where humans provide feedback under varying task complexities (e.g., ranking items with time constraints). Expected outcomes include (1) a measurable improvement in preference inference accuracy under effortful conditions, and (2) identification of systematic biases introduced by cognitive shortcuts. This could enable AI systems to distinguish between intended preferences and effort-induced noise, advancing alignment in applications like healthcare or education where human feedback is inherently imperfect.