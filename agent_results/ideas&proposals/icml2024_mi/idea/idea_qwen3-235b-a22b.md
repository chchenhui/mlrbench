1. **Title**: Bounded Rationality-aware Inverse Reinforcement Learning for Human-AI Alignment  
2. **Motivation**: Current Inverse Reinforcement Learning (IRL) methods assume humans act optimally under full rationality, which contradicts empirical findings in behavioral economics and cognitive science. This mismatch leads to misaligned reward functions, causing AI systems to misinterpret noisy or suboptimal human demonstrations. Addressing this gap is critical for safe AI deployment in high-stakes domains like robotics and healthcare.  
3. **Main Idea**: We propose integrating models of *bounded rationality*—quantifying cognitive limitations and decision-making heuristics—into IRL. Our framework will explicitly model human biases (e.g., anchoring, confirmation bias) and effort constraints by incorporating neuroimaging/eye-tracking data to estimate cognitive load during decisions. Using Bayesian inference, the model will jointly recover reward functions and task-specific human "noise parameters," enabling more accurate preference extrapolation even from suboptimal demonstrations. We will benchmark against traditional IRL methods on robot motion planning and recommendation tasks, measuring improvements in alignment with reject-sampled human preferences. The resulting toolkit could enhance AI safety in human-in-the-loop systems by avoiding overtrust in flawed human feedback models.