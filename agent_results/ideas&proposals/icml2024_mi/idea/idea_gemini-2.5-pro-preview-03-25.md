**Title:** Effort-Aware RLHF: Modeling Cognitive Load in Human Feedback for Improved AI Alignment

**Motivation:** Current Reinforcement Learning from Human Feedback (RLHF) frameworks typically assume human feedback directly reflects stable underlying preferences. However, behavioral economics and cognitive science show human decision-making is influenced by cognitive effort; comparing complex AI outputs can be tiring, leading to inconsistent or heuristic-based feedback. Ignoring this "effort cost" leads to noisy reward signals and potentially misaligned AI systems.

**Main Idea:** We propose incorporating cognitive effort into the human preference model used in RLHF. We will collect proxy measures for cognitive effort alongside standard preference labels (e.g., response times, self-reported difficulty, computational estimates of comparison complexity). We will then modify the standard preference model (e.g., Bradley-Terry) to explicitly account for effort. The likelihood of a stated preference will depend not just on the latent reward difference between options, but also on the estimated effort required for the comparison, potentially modeling increased noise or bias under high effort. This Effort-Aware Reward Model will be used within the RLHF loop, aiming to learn a more robust representation of human preferences by discounting or down-weighting feedback suspected to be effort-affected. Expected outcome: Improved AI alignment robustness, especially in complex domains requiring nuanced human judgments.