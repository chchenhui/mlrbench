Title: Bounded Rationality-Aware RLHF Framework

Motivation:  
Reinforcement Learning with Human Feedback (RLHF) typically assumes that human evaluators act as fully rational, unbiased oracles when scoring model outputs. In reality, cognitive limitations, effort costs and systematic biases affect feedback quality, leading to misaligned reward models and unsafe or unhelpful behaviors in deployed systems. Accounting for these factors is crucial to build AI agents that truly align with human values and decision processes.

Main Idea:  
We propose a hierarchical Bayesian RLHF pipeline that embeds a formal bounded-rationality model into the reward inference stage. Firstly, we collect paired feedback and “effort scores” from annotators solving small calibration tasks to estimate individual noise parameters, satisficing thresholds, and bias priors (e.g., anchoring, loss aversion). Secondly, we integrate these parameters as latent variables in the reward model, correcting raw human ratings for predicted biases and uncertainty. During policy optimization, the agent maximizes the inferred “debiased” reward while maintaining robustness to residual noise through Gaussian process regularization. We will validate this framework on language summarization and dialogue tasks, expecting more stable fine-tuning, better generalization to unseen prompts, and improved interpretability of human feedback influences.