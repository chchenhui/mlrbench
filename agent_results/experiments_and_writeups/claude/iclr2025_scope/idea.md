**Title:** Dynamic Sparse Retrieval-Augmented Sub-Quadratic Models for Efficient Long Context Adaptation  

**Motivation:** Foundation models face a critical trade-off between leveraging long contextual information and maintaining inference efficiency. Current retrieval-augmented generation (RAG) methods improve relevance but inflate computational costs by appending retrieved data to inputs, exacerbating quadratic attention complexity. This research aims to enable efficient, real-time adaptation to evolving contexts (e.g., news streams) while minimizing latency and memory overhead.  

**Main Idea:** We propose a sub-quadratic architecture integrating *dynamic sparse retrieval* and *compressive KV caching*. First, a lightweight retriever module, trained via reinforcement learning, selectively fetches context tokens most relevant to the input query, minimizing redundant prefill. Second, a sparse attention mechanism processes only retrieved tokens, reducing complexity to sub-quadratic. Third, a *rotating compressive KV cache* compresses historical context into fixed-size latent states using low-rank projections, avoiding unbounded memory growth. The retriever and attention are co-optimized end-to-end with a hybrid loss balancing task accuracy and retrieval/compute costs.  

**Expected Impact:** This approach would enable foundation models to dynamically adapt to streaming data with constant memory and sub-quadratic compute, improving throughput for long-context tasks (e.g., real-time news analysis) while maintaining accuracy. It directly addresses workshop goals of scalable adaptation and inference efficiency.