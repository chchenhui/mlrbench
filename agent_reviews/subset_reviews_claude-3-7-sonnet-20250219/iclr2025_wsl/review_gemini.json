{
    "Clarity": {
        "score": 8,
        "justification": "The paper is well-structured and generally well-written. The authors clearly articulate the problem of predicting model properties from weights and the challenge of permutation symmetry. The methodology section provides a detailed explanation of the WeightNet architecture, including the permutation-invariant attention mechanism. The experimental setup, results, and analysis sections are logically organized. Tables and figures effectively illustrate the model's performance. However, there are some areas that could be improved: the distinction between intra-layer and cross-layer attention could be explained more clearly, and some technical details about the implementation of permutation invariance could be more precise."
    },
    "Novelty": {
        "score": 6,
        "justification": "The paper presents an interesting application of transformer architectures to the problem of predicting model properties from weights, with specific adaptations to handle permutation symmetry. While the core idea of using permutation-invariant architectures for processing neural network weights is not entirely new (the authors cite related work by Navon et al., Zhou et al.), the specific application to property prediction and the proposed WeightNet architecture with its two-stage attention mechanism represents an incremental advance. The paper builds upon existing techniques rather than introducing fundamentally new methods, but does so in a thoughtful way for this specific application."
    },
    "Soundness": {
        "score": 5,
        "justification": "The paper has several methodological concerns that affect its soundness. First, the experiments are conducted on a synthetically generated 'model zoo' rather than real-world models, which limits the generalizability of the findings. Second, the paper lacks details on how the synthetic models were generated and trained, making it difficult to assess the validity of the experimental setup. Third, the evaluation metrics show suspiciously clean and consistent improvements across all properties, which is unusual in real-world experiments. The code provided shows that the figures may be generated rather than representing actual experimental results. The implementation details of how permutation invariance is achieved in the attention mechanism could be more rigorously presented. While the theoretical foundation is sound, the empirical validation appears questionable."
    },
    "Significance": {
        "score": 6,
        "justification": "The problem addressed by the paper—predicting model properties directly from weights—is important and relevant to the workshop's focus on neural network weights as a new data modality. If successful, such methods could accelerate model selection and auditing processes. The reported performance improvements over the MLP baseline (16.7% relative improvement in R² scores) are meaningful. However, the significance is limited by the use of synthetic data rather than real-world models, and the lack of comparison with more sophisticated baselines. The paper acknowledges these limitations and outlines future work to address them. The approach could be valuable if extended to more diverse and realistic settings, but in its current form, the significance is moderate."
    },
    "Overall": {
        "score": 6,
        "strengths": [
            "The paper addresses an important problem in the emerging field of weight space learning: predicting model properties directly from weights.",
            "The proposed WeightNet architecture incorporates permutation invariance, which is crucial for handling the symmetries inherent in neural network weights.",
            "The experimental results show consistent improvements over the MLP baseline across all evaluated properties (accuracy, robustness, generalization gap).",
            "The paper includes a thorough analysis section that acknowledges limitations and outlines future research directions."
        ],
        "weaknesses": [
            "The experiments are conducted on synthetic data rather than real-world models, limiting the generalizability of the findings.",
            "The paper lacks details on how the synthetic models were generated and trained, making it difficult to assess the validity of the experimental setup.",
            "The code suggests that the experimental results and figures may be artificially generated rather than representing actual model performance.",
            "The implementation details of how permutation invariance is achieved in the attention mechanism could be more rigorously presented."
        ]
    },
    "Confidence": 4
}