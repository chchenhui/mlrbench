{
  "Experiment_description": "We evaluated uncertainty-based data rejuvenation on two fronts: (1) a synthetic 2D binary classification task with three logistic regression models, where we iteratively add high-entropy points to the training set and track loss, model agreement variance, and Challenge Gap Recovery (CGR); (2) an MNIST study comparing a simple MLP versus a small CNN on original versus rotated test splits, measuring per-epoch losses, accuracies, and CGR.",
  "Significance": "These experiments quantify how uncertainty-driven sample selection influences model convergence and inter-model variance over training, and demonstrate that deeper architectures (CNNs) both generalize better under distribution shift and benefit differently from rejuvenation compared to shallower MLPs. The CGR metric reveals optimal windows for injecting challenging samples, informing data augmentation and active learning strategies.",
  "Description": "In the synthetic pipeline, three PyTorch logistic regressors are trained via SGD on normalized data; after each epoch, we select a batch of high-entropy points from a pre-generated pool, label them with the true linear rule, and augment the test set to compute CGR as relative change in accuracy variance. For MNIST, we fix seeds, normalize inputs, create a rotated test set, and train two architectures (MLP, CNN) with Adam; at each epoch we log train/validation losses, original/rotated accuracies, and CGR based on inter-model variance.",
  "List_of_included_plots": [
    {
      "path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_a412d654efce49988a3041aa482166eb_proc_3702065/synthetic_linear_CGR.png",
      "description": "CGR over epochs exhibits a sharp initial decline from a positive value (~0.3) at epoch 0 to a deep trough (~\u20130.72) by epoch 3, partial recovery to ~\u20130.35, a brief rebound to ~0.15 at epochs 12\u201313, then flattening at zero.",
      "analysis": "The initial drop indicates that newly injected high-uncertainty samples greatly increase model disagreement, but as models learn these points, their variance recovers and eventually saturates, showing a diminishing benefit of further rejuvenation."
    },
    {
      "path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_cdcac0528a7c4006b9bc7b3ac3c7e8b4_proc_3702065/mnist_accuracy_curves.png",
      "description": "Original and augmented accuracy curves for both MLP and CNN: MLP climbs from ~96% to ~97.6% (original) and from ~86% to ~89.7% (rotated), while CNN rises from ~97.5% to ~98.9% (original) and ~90% to ~92.3% (rotated).",
      "analysis": "The CNN consistently outperforms the MLP on both distributions and converges faster, indicating superior capacity to learn robust features under distribution shift."
    },
    {
      "path": "experiments/2025-06-04_15-18-13_benchmark_lifecycle_attempt_0/logs/0-run/experiment_results/experiment_cdcac0528a7c4006b9bc7b3ac3c7e8b4_proc_3702065/mnist_cgr_curve.png",
      "description": "MNIST CGR fluctuates non-monotonically: dips to ~0.73 at epoch 2, peaks at ~2.02 at epoch 3, then settles around 1.45\u20131.84 by epochs 4\u20135.",
      "analysis": "The CGR peak at epoch 3 identifies an optimal point for injecting challenging rotated examples to maximize inter-model variance reduction, guiding active selection timing."
    }
  ],
  "Key_numerical_results": [
    {
      "result": 0.1787,
      "description": "Final validation loss on synthetic task",
      "analysis": "Low and stable loss indicates successful convergence and effective generalization when mixing rejuvenated samples."
    },
    {
      "result": -0.72,
      "description": "Minimum CGR on synthetic rejuvenated set (epoch 3)",
      "analysis": "Shows that early injected samples most strongly increase model disagreement, highlighting the high informativeness of initial uncertainty samples."
    },
    {
      "result": 0.9237,
      "description": "CNN final accuracy on rotated MNIST",
      "analysis": "Demonstrates that the CNN retains strong performance under distribution shift, outperforming the MLP\u2019s ~0.897, and benefits more from rejuvenation."
    },
    {
      "result": 2.02,
      "description": "Peak MNIST CGR (epoch 3)",
      "analysis": "Indicates the epoch with maximum relative variance increase, pinpointing the ideal rejuvenation injection window during training."
    }
  ]
}