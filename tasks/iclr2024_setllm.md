# Workshop on Secure and Trustworthy Large Language Models

## About

The striding advances of large language models (LLMs) are revolutionizing many long-standing natural language processing tasks ranging from machine translation to question-answering and dialog systems. However, as LLMs are often built upon massive amounts of text data and subsequently applied in a variety of downstream tasks, building, deploying and operating LLMs entails profound security and trustworthiness challenges, which have attracted intensive research efforts in recent years.

The primary aim of the proposed workshop is to identify such emerging challenges, discuss novel solutions to address them, and explore new perspectives and constructive views across the full theory/algorithm/application stack.

## Topics

The potential topics include but are not limited to:
- Reliability assurance and assessment of LLMs
- Privacy leakage issues of LLMs
- Copyright protection
- Interpretability of LLMs
- Plagiarism detection and prevention
- Security of LLM deployment
- Backdoor attacks and defenses in LLMs
- Adversarial attacks and defenses in LLMs
- Toxic speech detection and mitigation
- Challenges in new learning paradigms of LLMs (e.g., prompt engineering)
- Fact verification (e.g. hallucinated generation)
