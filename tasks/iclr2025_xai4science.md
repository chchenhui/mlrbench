# XAI4Science: From Understanding Model Behavior to Discovering New Scientific Knowledge

## About This Workshop
Machine learning (ML) models are impressive when they work but they can also show unreliable, untrustworthy, and harmful dangerous behavior. Yet, such models are widely adopted and deployed, even though we do not understand why they work so well and fail miserably at times. Such rapid dissemination encourages irresponsible use, for example, to spread misinformation or create deep fakes, while hindering the efforts to use them to solve pressing societal problems and advance human knowledge.


Ideally, we want models to help us improve our understanding of the world and, at the very least, we want them to aid human knowledge and help us to further enrich it. Our goal in this workshop is to take a step in this direction by bringing together researchers working on understanding model behavior and using it to discover new human knowledge. The workshop will include theoretical topics on understanding model behavior, namely interpretability and explainability (XAI), but also three distinct scientific application areas: weather and climate, healthcare, and material science (ML4Science).

## Topics
A-priori (i.e., ante-hoc) interpretability and self-explainable models for understanding model’s behaviour


A-posteriori (i.e., post-hoc) interpretability and attribution methods for understanding model’s behaviour, including methods for evaluating the accuracy of post-hoc interpretability and attribution


Practical use of interpretability and explainability for knowledge discovery in

- ⁠Weather and climate science,
- ⁠Material science, and
- ⁠⁠Healthcare
