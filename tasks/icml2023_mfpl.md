## The Many Facets of Preference-based Learning

Learning from human preferences, or preference-based learning, has been critical to major advances in AI and machine learning. It is based on the fact that humans are more reliable at providing relative feedback compared to numerical values. Therefore, preference feedback is usually easier to collect and less biased. A recent success story that showed the dormant potential of learning from preference feedback is fine-tuning of large language models with a reward function learned from human feedback and reinforcement learning to follow instructions in a dialogue context. There are other areas where preference-based learning yielded promising results, such as guided image generation, robotics and self-driving vehicles, games, collaborative filtering, simulated continuous control tasks, optimization and search problems, and healthcare. Despite these ground-breaking successes, the most exciting opportunities still lie ahead of us.

The broad objective of this workshop is twofold:

1. Bring together different communities where preference-based learning has played a major role.

2. Connect theory to practice by identifying real-world systems that can benefit from incorporating preference feedback.

The aim of this workshop is to create a suitable platform for sharing techniques and ideas, learning from each other, and potentially posing new and groundbreaking research questions.

## Topics

We cordially invite researchers who feel addressed by the theme of the workshop to submit their latest works to our workshop. Topics include but are not limited to:

- Collaborative filtering
- Control theory
- Convex optimization
- Dueling and preference-based bandit
- Econometrics and assortment selection
- Fairness
- Game theory, equilibria, and multiplayer games
- Marketing and revenue management
- Multi-objective optimization 
- Ranking aggregation
- Recommender systems 
- Reinforcement learning 
- Robotics
- Search engine optimization
- Social choice theory