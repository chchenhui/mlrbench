## Foundation Models for Science: Progress, Opportunities, and Challenges

The integration of artificial intelligence (AI) and machine learning (ML) into the realm of science represents a pivotal shift in the traditional methods of scientific discovery. For centuries, the systematic and logical exploration of the natural world has followed a consistent methodology. However, the emergence of AI and ML technologies promises a profound transformation in how fundamental scientific discoveries are made today. This joint effort is crucial for enhancing interdisciplinary dialogue, stimulating innovative problem-solving approaches, and ultimately, enriching the scientific communityâ€™s capacity to tackle some of the most pressing and intricate problems in modern science.

Meanwhile, foundation models, trained on vast and diverse datasets, have significantly altered the landscape of computer vision and natural language processing by demonstrating robust adaptability across a multitude of tasks. These models, including prominent examples like GPT-4 for language and CLIP for image-text processing, have revolutionized their respective fields by providing a versatile, pre-trained base that can be fine-tuned for various applications. By leveraging the extensive knowledge encoded in foundation models, researchers are addressing critical challenges such as long-term planning and multi-modal reasoning, which are essential for complex real-world applications like robotics and dialogue systems.

We see an opportunity to collaboratively pursue the integration of AI-for-Science and foundation models, which is emerging as a transformative force in scientific domains. Leveraging foundation models, trained on extensive datasets and capable of multimodal processing, offers a unique opportunity to solve scientific problems and serve as a robust base for further domain-specific adaptations. Thus, the synergy between AI-for-Science and foundation models is poised to radically improve how we model complex phenomena, making it an essential area of investment for future scientific advancements. In contrast with small-scale AI-for-science models or foundation models for traditional domains of computer vision or natural language processing, we see both opportunities and unique challenges in advancing and solving scientific problems through approaches of building and applying foundation models.

## Topics
In this workshop, we aim to bring together experts from foundation models and scientific problems, spur discussions, and foster collaborations on broad and transformative questions and challenges (include but not limited to):

**Progress.**
- Scalability: Is the scaling law and training strategy of scientific foundation models different from counterparts of NLP and vision?
- Reusability: Can scientific foundation models be trained for once and adopted in different scenarios?
- Performance: Can scientific foundation models consistently outperform domain-specific models?

**Opportunities.**
- How to make foundation models understand multi-modal scientific inputs and capable of multiple scientific problems?
- How to accelerate scientific discovery and collection/assimilation of scientific data with foundation models?
- How to make foundation model compatible and enable integration of classic scientific tools?

**Challenges.**
- How to diagnose failure cases or modes that scientific foundation models cannot perform well?
- How to align scientific foundation models with scientific facts without hallucination?
- How to quantify the scientific uncertainty of foundation models?

**Scientific Domains.** We invite paper submissions from various scientific domains, including but not limited to: Astrophysics and Space Science, Biomedicine (e.g., proteins, biosequences, virtual screening), Computational Science (e.g., PDEs, forecasting), Earth Science, Materials Science (e.g., batteries, chemical synthesis), Quantum Mechanics (e.g., nuclear fusion), Small Molecules. Applications-driven submissions focusing on AI-for-Science and Scientific Machine Learning (SciML) are also highly encouraged.
