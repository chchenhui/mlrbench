## Workshop on Safe & Trustworthy Agents

This workshop aims to clarify key questions on the safety of agentic AI systems and foster a community of researchers working in this area.


## Topics
‚ÄçThis workshop aims to clarify key questions on the trustworthiness of agentic AI systems and foster a community of researchers working in this area. We welcome papers on topics including, but not limited to, the following:

- Research into safe reasoning and memory. We are interested in work that makes LLM agent
reasoning or memory trustworthy, e.g., by preventing hallucinations or mitigating bias.
- Research into adversarial attacks, security and privacy for agents. As LLM agents interact
with more data modalities and a wider variety of input/output channels, we are interested in work
that studies or defends against possible threats and privacy leaks.
- Research into controlling agents. We are interested in novel control methods which specify goals,
constraints, and eliminate unintended consequences in LLM agents.
- Research into agent evaluation and accountability. We are interested in evaluation for LLM
agents (e.g., automated red-teaming) and interpretability + attributability of LLM agent actions.
- Research into environmental and societal impacts of agents. We are interested in research that
examines the environmental cost, fairness, social influence, and economic impacts of LLM agents.
- Research into multi-agent safety and security. We are interested in research that analyzes novel
phenomena with multiple agents: emergent functionality at a group level, collusion between agents, correlated failures, etc.
