## Workshop on Symmetry and Geometry in Neural Representations

An emerging set of findings in sensory and motor neuroscience is beginning to illuminate a new paradigm for understanding the neural code. Across sensory and motor regions of the brain, neural circuits are found to mirror the geometric and topological structure of the systems they representâ€”either in their synaptic structure, or in the implicit manifold generated by their activity. This phenomenon can be observed in the circuit of neurons representing head direction in the fly (Kim et al. (2017); Wolff et al. (2015); Chaudhuri et al. (2019)), in the activities of grid cells (Gardner et al. (2022)), and in the low-dimensional manifold structure observed in motor cortex (Gallego et al. (2017)). This suggests a general computational strategy that is employed throughout the brain to preserve the geometric structure of data throughout stages of information processing.

Independently but convergently, this very same computational strategy has emerged in the field of deep learning. The nascent sub-field of Geometric Deep Learning (Bronstein et al. (2021)) incorporates geometric priors into artificial neural networks to preserve the geometry of signals as they are passed through layers of the network. This approach provably demonstrates gains in the computational efficiency, robustness, and generalization performance of these models.

The convergence of these findings suggests deep, substrate-agnostic principles for information processing. Symmetry and geometry were instrumental in unifying the models of 20th-century physics. Likewise, they have the potential to illuminate unifying principles for how neural systems form useful representations of the world.

The NeurReps Workshop brings together researchers from applied mathematics and deep learning with neuroscientists whose work reveals the elegant implementation of mathematical structure in biological neural circuitry. The first and second editions of NeurReps were held at NeurIPS 2022 and at NeurIPS 2023. The invited and contributed talks drew exciting connections between trends in geometric deep learning and neuroscience, emphasizing parallels between equivariant structures in brains and machines. This year's workshop will feature five invited talks covering emerging topics in geometric deep learning, mechanistic interpretability,  geometric structure in the brain, world models and the role of dynamics in shaping neural representations. 

## Topics
We invite submissions contributing novel research incorporating symmetry, geometry, or topology into the design of artificial neural networks, the analysis of neural data, or theories of neural computation. We welcome contributions in the intersection of geometric and topological deep learning, computational and theoretical neuroscience, geometric statistics, and topological data analysis.

The following themes are particularly relevant:

- Theory and methods for learning invariant and equivariant representations
- Statistical learning theory in the context of topology, geometry, and symmetry
- Representational geometry in neural data
- Learning and leveraging group structure in data 
- Equivariant world models for robotics
- Dynamics of neural representations
- Topological deep learning and topological data analysis
- Geometric structure in language
- Geometric and topological analysis of generative models
- Symmetries, dynamical systems, and learning
