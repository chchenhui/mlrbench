## Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability

This FITML workshop aims to contribute to the recent radical paradigm shift for fine-tuning in modern machine learning, theoretically, computationally, and systematically.

It encourages researchers to push forward the frontiers of theoretical understanding of fine-tuning, devising expeditious and resource-efficient inference and fine-tuning methods in machine learning systems, enabling their deployment within constrained computational resources.

This FITML workshop explores theoretical and/or empirical results for understanding and advancing modern practices for efficiency in machine learning.

# Topics

Key topics include but are not limited to:
- Exploration of new methodology for fine-tuning of various strategies, architectures and systems, from low-rank representation to sparse representation, from deep neural networks to LLMs, from algorithmic design to hardware design.
- Theoretical foundations of fine-tuning, e.g. approximation, optimization, and generalization from the perspective of transfer learning, deep learning theory, RLHF. Besides, theoretical understanding of low-rank representation from sketching and signal recovery are also welcome.
- Works that propose new experimental observations that can help advance our understanding of the underlying mechanisms of fine-tuning, a discrepancy between existing theoretical analyses and practice, explainability and interpretability of fine-tuning in scientific contexts.

The topics are not limited to fine-tuning, LLMs. Any topic on theoretical and/or empricial results for understanding and advancing modern practices for efficiency in machine learning is also welcome.