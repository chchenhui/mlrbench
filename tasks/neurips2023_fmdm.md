# Foundation Models for Decision Making

Foundation models pretrained on diverse vision and language datasets have demonstrated exceptional capabilities in performing a wide range of downstream vision and language tasks. As foundation models are deployed in real-world applications such as dialogue, autonomous driving, healthcare, and robotics, they inevitably face new challenges such as learning from external feedback, adapting to different task modalities, and performing long-term reasoning and planning. Such challenges have traditionally been at the core of sequential decision making, encompassing areas such as reinforcement learning, imitation learning, planning, search, and optimal control. These research fields have traditionally focused on task-specific settings with limited prior knowledge, and yet there has been significant research progress in surpassing human performance in tasks like playing board games and Atari video games, as well as operating robots to complete navigation and manipulation tasks. However, since these methods generally learn to solve a specific task from scratch without broad knowledge from vision and language, they can struggle with generalization and sample efficiency.

Research in the intersection of foundation models and sequential decision making is gaining attention. Research in foundation models has expanded to address long-term reasoning and multiple model interactions, while researchers in sequential decision making are developing larger datasets and training larger-scale interactive agents. Further blurring the lines between the two fields, dialogue agents have been optimized by reinforcement learning with human feedback, and large pretrained vision-language models have been used as perception and reasoning components of embodied agents. Foundation models have also been adapted to interact with search engines, calculators, translators, simulators, and program interpreters. Despite these early successes, foundation models for decision making still faces many scientific questions and challenges that have not been addressed by existing work. Examples of questions that we hope to make progress towards answering through this workshop include:

- Develop language model agents that can automatically learn to interact with humans, tools, the world, and each other in a scientific and principled way.
- Derive sound, practical, and scalable algorithms similar to RLHF and MCTS for language and vision based decision making applications.
- How to structure environments and tasks so that vision language foundation models can benefit traditional decision making applications in control, planning, and reinforcement learning?
- Foundation models are trained on data without actions. How to overcome this limitation from both the dataset and modeling perspectives?

# Topics
More specific topics will include but are not limited to:
- Foundation model agents interacting with humans, computers, tools, simulators, physical world, and each other.
- Rethinking the implementation, ecosystem, and model modularity of decision making agents under emerging technologies such as ChatGPT and language model plug-ins.
- Applying foundation models to traditional decision making problems in control, planning, online / offline RL.
- Learning multi-modal, multi-task, multi-environment, and generalist policies.
- Long-horizon reasoning and planning in language models.
- New evaluation protocols, benchmarks, datasets, and applications that apply foundation models to solve decision making problems.
- Theoretical understanding of the roles foundation models play in decision making.
