# Workshop on Goal-Conditioned Reinforcement Learning

Learning goal-directed behavior is one of the classical problems in AI, one that has received renewed interest in recent years and currently sits at the crossroads of many seemingly-disparate research threads: self-supervised learning , representation learning, probabilistic inference, metric learning, and duality. Our workshop focuses on these goal-conditioned RL (GCRL) algorithms and their connections to different areas of machine learning. Goal-conditioned RL is exciting not just because of these theoretical connections with different fields, but also because it promises to lift some of the practical challenges with applying RL algorithms: users can specify desired outcomes with a single observation, rather than a mathematical reward function. As such, GCRL algorithms may be applied to problems varying from robotics to language models tuning to molecular design to instruction following. Our workshop aims to bring together researchers studying the theory, methods, and applications of GCRL, researchers who might be well posed to answer questions such as: 
- How does goal-directed behavior in animals inform better GCRL algorithmic design? 
- How can GCRL enable more precise and customizable molecular generation? 
- Do GCRL algorithms provide an effective mechanism for causal reasoning? 
- When and how should GCRL algorithms be applied to precision medicine?

# Goal
The workshop aims to foster an inclusive environment where researchers and practitioners from all backgrounds can engage in discussions and build collaborations on the theory, methods, and applications of GCR. Broadly, the workshop will focus on the following topics and problems: 
- Connections : What are the connections between GCRL and representation learning, few-shot learning, and self-supervised learning? When does (say) effective representation learning emerge from GCRL? 
- Future directions : What are limitations of existing methods, benchmarks, and assumptions? 
- Algorithms : How might we improve existing methods, and do this in a way that enables applications to broader domains (e.g., molecular discovery, instruction-following robots)?

# Topics 
We solicit submissions related to (but not limited to) the following topics:
- Algorithms. We encourage both proposals of new methods, as well as analyses and/or evaluations of existing ones.
Connections between goal-conditioned RL and other ML areas.
- Examples might include representation learning, self-supervised learning, adversarial training, probabilistic inference, metric learning, duality, etc.
- Applications of goal-conditioned decision making.
In addition to common decision making tasks (e.g., robotics and games) and goal-conditioned applications (e.g. instruction-following, molecular discovery), we especially encourage work in goal-conditioned domains where GCRL is not (yet)
the mainstream strategy
