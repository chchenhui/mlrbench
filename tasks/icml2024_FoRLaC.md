# Foundations of RL and Control: Connections and New Perspectives

## About

Despite rapid advances in machine learning, solving large-scale stochastic dynamic programming problems remains a significant challenge. The combination of neural networks with RL has opened new avenues for algorithm design, but the lack of theoretical guarantees of these approaches hinders their applicability to high-stake problems traditionally addressed using control theory, such as online supply chain optimization, industrial automation, and adaptive transportation systems. This workshop focuses on recent advances in developing a learning theory of decision (control) systems, that builds on techniques and concepts from two communities that have had limited interactions despite their shared target: reinforcement learning and control theory.

This workshop aims to reinforce the connection between reinforcement learning and control theory by bringing together researchers from both fields. In particular, we invite contributions on all fundamental and theoretical aspects, with a special emphasis on topics that connect both fields and provide new perspectives. Contributions that bridge theory and applications are also welcome. We believe that significant progress in tackling large-scale applications can only be achieved through collaborative efforts and a mutual understanding of each field’s strengths and approaches. Our workshop is dedicated to fostering dialogue and collaboration, paving the way for breakthroughs in complex dynamic programming challenges and interactive systems.

## Topics 
We invite researchers to submit papers on the topics listed below.  Technical topics include, but are not limited to, the following aspects:

- Performance measures and guarantees: Stability, robustness, regret bounds, sample-complexity, stochastic vs non-stochastic approaches, MDPs etc.
- Fundamental assumptions: Linear and non-linear systems, excitation, stability, etc.
- Fundamental limits: Results that mathematically characterize the difficulty of a given problem, statistical, information theoretic and computational lower bounds.
- Computational aspects: Efficient algorithms, computational hardness, approximations, etc.
- Topology: Continuous-state and action spaces vs discrete spaces; Discrete and continuous time analysis.
- Models: Bandits, Markov Decision Processes, Linear and nonlinear control, partial observability, POMDPs, partial monitoring, etc
- Data Acquisition & Exploration: Exploration-exploitation trade-offs, pure-exploration, experimental design.
- Offline vs. online: Open-loop and closed loop control, offline and online reinforcement learning and hybrid approaches.
- Planning and learned search: Dynamic programming, tree search and planning algorithms.
- Target applications: Formalization of applications such as autonomous vehicles, robots, industrial processes, recommender systems, internet routing, hardware optimization, hyper-parameter optimization and AutoML …
- Benchmarks: Evaluation of algorithms and theoretical results on a suitable collection of problems.
